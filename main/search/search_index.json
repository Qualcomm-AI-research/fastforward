{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcfc FastForward","text":"<p>FastForward is a Python package built on top of PyTorch for neural network quantization. It aims to serve as a foundation for research and prototyping in quantization. By leveraging PyTorch's eager mode, you can experiment with neural network quantization as easily as with any other PyTorch module. This means you can use breakpoints, print statements, and other introspection methods with quantized neural networks, just as you would with standard ones.</p>"},{"location":"#status","title":"Status","text":"<p>FastForward is currently under active development. While it has been successfully used in various projects, core parts of the library may still change.</p>"},{"location":"#main-features","title":"Main Features","text":"<ul> <li>Quantized Tensor: A versatile container for quantized data that supports   multiple quantization formats while retaining metadata.</li> <li>Range Estimation: General methods for range estimation that can be easily   extended to new quantization methods.</li> <li>Quantized Operator Dispatching: A dispatcher built on top of the PyTorch   dispatcher, specialized for different quantization schemes and methods.</li> <li>Quantization Setup and Initialization: A step-by-step process for   converting a non-quantized model into a quantized one, customizable at each   stage.</li> <li>Quantization Safety: A default mode ensuring correctly quantized models   that can be deployed to efficient hardware, with opt-out options if needed.   This helps to catch common quantization mistakes early.</li> <li>mpath: A utility to describe, access, and update multiple layers in   a module hierarchy at a higher level of abstraction.</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>More Quantization Methods: Implementations of quantization methods such   as Omniquant, GPTQ, SpinQuant, and others.</li> <li>Autoquant: Automatic conversions any non-quantized PyTorch model into an   eager-mode quantized model.</li> <li>Export: Generation of deployment artifacts for functional quantized neural   networks.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started, explore these tutorials:</p> <ul> <li>Getting Started: Quantizing a LLM from Scratch</li> <li>Quick Start: Quantization of Llama-v3</li> </ul> <p>For more tutorials and the API reference, visit the general documentation.</p>"},{"location":"#how-to-get-it","title":"How to Get It","text":"<ol> <li> <p>Ensure you have a working installation of PyTorch.</p> </li> <li> <p>Install FastForward:</p> </li> </ol> <pre><code>pip install git+https://github.com/Qualcomm-AI-research/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxRangeEstimator.split_module","title":"<code>split_module(module)</code>","text":"<p>Split the module into individual quantizers.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to split.</p> required <p>Yields:</p> Type Description <code>Quantizer</code> <p>Iterator[Quantizer]: An iterator over the quantizers.</p> Source code in <code>fastforward/range_setting/minmax.py</code> <pre><code>def split_module(self, module: torch.nn.Module) -&gt; Iterator[Quantizer]:\n    \"\"\"Split the module into individual quantizers.\n\n    Args:\n        module: The module to split.\n\n    Yields:\n        Iterator[Quantizer]: An iterator over the quantizers.\n    \"\"\"\n    for _, quantizer in named_quantizers(module, recurse=True):\n        if isinstance(quantizer, RangeSettable) or not self.skip_unsupported_quantizers:\n            yield quantizer\n        else:\n            logger.warning(\n                f\"{type(quantizer).__name__} does not implement RangeSettable. Therefore \"\n                f\"it is not included in {type(self).__name__} range setting.\"\n            )\n</code></pre>"}]}