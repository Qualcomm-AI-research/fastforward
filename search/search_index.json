{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcfc FastForward","text":"<p>FastForward is a Python package built on top of PyTorch for neural network quantization. It aims to serve as a foundation for research and prototyping in quantization. By leveraging PyTorch's eager mode, you can experiment with neural network quantization as easily as with any other PyTorch module. This means you can use breakpoints, print statements, and other introspection methods with quantized neural networks, just as you would with standard ones.</p>"},{"location":"#status","title":"Status","text":"<p>FastForward is currently under active development. While it has been successfully used in various projects, core parts of the library may still change.</p>"},{"location":"#main-features","title":"Main Features","text":"<ul> <li>Quantized Tensor: A versatile container for quantized data that supports   multiple quantization formats while retaining metadata.</li> <li>Range Estimation: General methods for range estimation that can be easily   extended to new quantization methods.</li> <li>Quantized Operator Dispatching: A dispatcher built on top of the PyTorch   dispatcher, specialized for different quantization schemes and methods.</li> <li>Quantization Setup and Initialization: A step-by-step process for   converting a non-quantized model into a quantized one, customizable at each   stage.</li> <li>Quantization Safety: A default mode ensuring correctly quantized models   that can be deployed to efficient hardware, with opt-out options if needed.   This helps to catch common quantization mistakes early.</li> <li>mpath: A utility to describe, access, and update multiple layers in   a module hierarchy at a higher level of abstraction.</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>More Quantization Methods: Implementations of quantization methods such   as Omniquant, GPTQ, SpinQuant, and others.</li> <li>Autoquant: Automatic conversions any non-quantized PyTorch model into an   eager-mode quantized model.</li> <li>Export: Generation of deployment artifacts for functional quantized neural   networks.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started, explore these tutorials:</p> <ul> <li>Getting Started: Quantizing a LLM from Scratch</li> <li>Quick Start: Quantization of Llama-v3</li> </ul> <p>For more tutorials and the API reference, visit the general documentation.</p>"},{"location":"#how-to-get-it","title":"How to Get It","text":"<ol> <li> <p>Ensure you have a working installation of PyTorch.</p> </li> <li> <p>Install FastForward:</p> <pre><code>```\npip install git+https://github.com/Qualcomm-AI-research/fastforward@main\n```\n</code></pre> </li> </ol>"},{"location":"examples/","title":"Tutorial","text":"<p>Here is a list of examples which you could check to learn how to use FastForward:</p> <ul> <li>Getting Started: Quantizing a LLM from scratch</li> <li>Quantizing Llama-v3</li> <li>MPath: a utility for submodule selection</li> <li>Implementing a custom Quantizer</li> </ul>"},{"location":"examples/#internal-documentation","title":"Internal Documentation","text":"<ul> <li>Neural Network Execution Orchestration</li> </ul>"},{"location":"examples/implement_custom_quantizer.nb/","title":"Implementing a custom Quantizer","text":"In\u00a0[1]: Copied! <pre>from typing import Callable\n\nimport torch\n\nfrom torch import Tensor\n\nfrom fastforward import QuantizedTensor, estimate_ranges\nfrom fastforward.common import ensure_tensor\nfrom fastforward.nn.quantizer import Quantizer\nfrom fastforward.quantization import granularity\nfrom fastforward.quantization.affine import parameters_for_range\nfrom fastforward.quantization.function import QuantizationFunction\nfrom fastforward.quantization.ste import round_ste\nfrom fastforward.range_setting.minmax import RunningMinMaxRangeEstimator\n\n\nclass MyQuantizationFunction(QuantizationFunction):\n    \"\"\"My custom quantization function class\"\"\"\n\n    @staticmethod\n    def quantize(  # type: ignore[override]\n        data: Tensor, num_bits: int, scale: float, offset: float | None = None\n    ) -&gt; Tensor:\n        if offset is None:\n            offset = 0.0\n        min_int = -(2 ** (num_bits - 1))\n        max_int = -min_int - 1\n        return torch.clamp(round_ste(data / scale - offset), min_int, max_int)\n\n    @staticmethod\n    def dequantize(  # type: ignore[override]\n        quant_data: Tensor, num_bits, scale: float, offset: float | None = None\n    ) -&gt; Tensor:\n        if offset is not None:\n            _offset = torch.round(torch.tensor(offset, device=quant_data.device))\n            return (quant_data + _offset) * scale\n        else:\n            return quant_data * scale\n</pre> from typing import Callable  import torch  from torch import Tensor  from fastforward import QuantizedTensor, estimate_ranges from fastforward.common import ensure_tensor from fastforward.nn.quantizer import Quantizer from fastforward.quantization import granularity from fastforward.quantization.affine import parameters_for_range from fastforward.quantization.function import QuantizationFunction from fastforward.quantization.ste import round_ste from fastforward.range_setting.minmax import RunningMinMaxRangeEstimator   class MyQuantizationFunction(QuantizationFunction):     \"\"\"My custom quantization function class\"\"\"      @staticmethod     def quantize(  # type: ignore[override]         data: Tensor, num_bits: int, scale: float, offset: float | None = None     ) -&gt; Tensor:         if offset is None:             offset = 0.0         min_int = -(2 ** (num_bits - 1))         max_int = -min_int - 1         return torch.clamp(round_ste(data / scale - offset), min_int, max_int)      @staticmethod     def dequantize(  # type: ignore[override]         quant_data: Tensor, num_bits, scale: float, offset: float | None = None     ) -&gt; Tensor:         if offset is not None:             _offset = torch.round(torch.tensor(offset, device=quant_data.device))             return (quant_data + _offset) * scale         else:             return quant_data * scale <p>With this, can already use our <code>MyQuantizationFunction</code> to quantize a floating point tensor into a <code>QuantizedTensor</code> storing integer data:</p> In\u00a0[2]: Copied! <pre># Create new random data tensor\ndata = torch.rand(1024, 1024, requires_grad=True)\n\n# Quantize and dequantize data\nbits = 8\nqf_data = MyQuantizationFunction.apply(data, num_bits=bits, scale=2 ** (-bits), offset=128)\ndqf_data = qf_data.dequantize()\n\n# Compute quantization error\nqf_error = torch.abs(data - dqf_data)\nprint(f\"The maximum quantization error found on input data is: {qf_error.max()} \")\n</pre> # Create new random data tensor data = torch.rand(1024, 1024, requires_grad=True)  # Quantize and dequantize data bits = 8 qf_data = MyQuantizationFunction.apply(data, num_bits=bits, scale=2 ** (-bits), offset=128) dqf_data = qf_data.dequantize()  # Compute quantization error qf_error = torch.abs(data - dqf_data) print(f\"The maximum quantization error found on input data is: {qf_error.max()} \") <pre>The maximum quantization error found on input data is: 0.0039054155349731445 \n</pre> <p>Quantizing tensors like this, is not very convenient. But we can write unit tests for <code>MyQuantizationFunction</code> following this approach.</p> <p>A much better way to use the quantization function, is to implement our custom <code>MyQuantizer</code> that will use our custom quantization function internally. Instead of passing <code>scale</code> and <code>offset</code> to the constructor of the quantizer, we will let the user of the object to set the quantization range, delegating to the class internals how to modify <code>scale</code> and <code>offset</code> accordingly.</p> <p>To do so, we will implement the <code>fastforward.range_setting.RangeSettable</code> protocol, i.e. the following methods:</p> <ul> <li><code>quantization_range</code> getter: return to the caller the quantization range on which our quantizer can operate without incurring in clipping.</li> <li><code>quantization_range</code> setter: given a min-max range, this method will set our quantization parameters accordingly.</li> </ul> In\u00a0[3]: Copied! <pre>class MyQuantizer(Quantizer):\n    \"\"\"My custom quantizer class\"\"\"\n\n    def __init__(self, num_bits: int, symmetric: bool = False, device: torch.device | str = \"cpu\"):\n        super().__init__()\n        self.num_bits = num_bits\n        self.scale = torch.nn.Parameter(torch.tensor([0.0], device=device))\n        self.offset = None if symmetric else torch.nn.Parameter(torch.tensor([0.0], device=device))\n\n    def extra_repr(self) -&gt; str:\n        extra = f\"num_bits={self.num_bits}, symmetric={self.symmetric}, scale={self.scale}, offset={self.offset}\"\n        return super().extra_repr() + extra\n\n    def quantize(self, data: Tensor) -&gt; Tensor:\n        return MyQuantizationFunction.apply(\n            data, num_bits=self.num_bits, scale=self.scale, offset=self.offset\n        )\n\n    @property\n    def granularity(self) -&gt; granularity.Granularity:\n        return granularity.PerTensor()\n\n    @property\n    def symmetric(self) -&gt; bool:\n        return \"offset\" in self._buffers or self.offset is None\n\n    @property\n    def quantization_range(self) -&gt; tuple[Tensor | float | None, Tensor | float | None]:\n        offset = self.offset or 0.0\n        range_min = (integer_minimum(self.num_bits) + offset) * self.scale\n        range_max = (integer_maximum(self.num_bits) + offset) * self.scale\n        return range_min, range_max\n\n    @quantization_range.setter\n    def quantization_range(self, quant_range: tuple[Tensor | float, Tensor | float]) -&gt; None:\n        if not isinstance(quant_range, (tuple, list)):\n            raise ValueError(\n                \"Tried to set quantization range with a single value. A 2-tuple is expected\"\n            )\n        if not len(quant_range) == 2:\n            raise ValueError(\n                f\"Tried to set quantization range with {len(quant_range)}-tuple. A 2-tuple is expected\"\n            )\n\n        min, max = (ensure_tensor(t, device=self.scale.device) for t in quant_range)\n        with torch.no_grad():\n            scale, offset = parameters_for_range(\n                min, max, self.num_bits, self.symmetric, allow_one_sided=False\n            )\n            self.scale.copy_(scale)\n            if self.offset is not None and offset is not None:\n                self.offset.copy_(offset)\n\n\ndef integer_minimum(num_bits: float) -&gt; float:\n    return -(2 ** (num_bits - 1))\n\n\ndef integer_maximum(num_bits: float) -&gt; float:\n    return -integer_minimum(num_bits) - 1\n</pre>   class MyQuantizer(Quantizer):     \"\"\"My custom quantizer class\"\"\"      def __init__(self, num_bits: int, symmetric: bool = False, device: torch.device | str = \"cpu\"):         super().__init__()         self.num_bits = num_bits         self.scale = torch.nn.Parameter(torch.tensor([0.0], device=device))         self.offset = None if symmetric else torch.nn.Parameter(torch.tensor([0.0], device=device))      def extra_repr(self) -&gt; str:         extra = f\"num_bits={self.num_bits}, symmetric={self.symmetric}, scale={self.scale}, offset={self.offset}\"         return super().extra_repr() + extra      def quantize(self, data: Tensor) -&gt; Tensor:         return MyQuantizationFunction.apply(             data, num_bits=self.num_bits, scale=self.scale, offset=self.offset         )      @property     def granularity(self) -&gt; granularity.Granularity:         return granularity.PerTensor()      @property     def symmetric(self) -&gt; bool:         return \"offset\" in self._buffers or self.offset is None      @property     def quantization_range(self) -&gt; tuple[Tensor | float | None, Tensor | float | None]:         offset = self.offset or 0.0         range_min = (integer_minimum(self.num_bits) + offset) * self.scale         range_max = (integer_maximum(self.num_bits) + offset) * self.scale         return range_min, range_max      @quantization_range.setter     def quantization_range(self, quant_range: tuple[Tensor | float, Tensor | float]) -&gt; None:         if not isinstance(quant_range, (tuple, list)):             raise ValueError(                 \"Tried to set quantization range with a single value. A 2-tuple is expected\"             )         if not len(quant_range) == 2:             raise ValueError(                 f\"Tried to set quantization range with {len(quant_range)}-tuple. A 2-tuple is expected\"             )          min, max = (ensure_tensor(t, device=self.scale.device) for t in quant_range)         with torch.no_grad():             scale, offset = parameters_for_range(                 min, max, self.num_bits, self.symmetric, allow_one_sided=False             )             self.scale.copy_(scale)             if self.offset is not None and offset is not None:                 self.offset.copy_(offset)   def integer_minimum(num_bits: float) -&gt; float:     return -(2 ** (num_bits - 1))   def integer_maximum(num_bits: float) -&gt; float:     return -integer_minimum(num_bits) - 1 <p>Now we have a more user-friendly interface:</p> In\u00a0[4]: Copied! <pre># Create new random data tensor\ndata = torch.randn(128, 64, 4, requires_grad=True)\n\n# Instantiate quantizer:\nquantizer = MyQuantizer(num_bits=8, symmetric=False)\n\n# Set quantization range manually:\nquantizer.quantization_range = data.min(), data.max()\n\n# Quantize and dequantize data\nq_data = quantizer(data)\ndq_data = q_data.dequantize()\n\n# Compute quantization error\nq_error = torch.abs(data - dq_data)\nprint(f\"The maximum quantization error found on input tensor is: {q_error.max()} \")\n</pre>  # Create new random data tensor data = torch.randn(128, 64, 4, requires_grad=True)  # Instantiate quantizer: quantizer = MyQuantizer(num_bits=8, symmetric=False)  # Set quantization range manually: quantizer.quantization_range = data.min(), data.max()  # Quantize and dequantize data q_data = quantizer(data) dq_data = q_data.dequantize()  # Compute quantization error q_error = torch.abs(data - dq_data) print(f\"The maximum quantization error found on input tensor is: {q_error.max()} \")  <pre>The maximum quantization error found on input tensor is: 0.02473825216293335 \n</pre> <pre>/tmp/ipykernel_145204/439193663.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  _offset = torch.round(torch.tensor(offset, device=quant_data.device))\n</pre> <p>Moreover, we are now able to use standard <code>estimate_range</code> functionality over our custom quantizer:</p> In\u00a0[5]: Copied! <pre># Create new random data tensor\ndata = torch.randn(10, 10, requires_grad=True)\n\n# Instantiate quantizer:\nquantizer = MyQuantizer(num_bits=8, symmetric=False)\n\n# Estimate range\nwith estimate_ranges(quantizer, RunningMinMaxRangeEstimator):\n    q_data = quantizer(data)\n\n# Compute quantization error over the calibration data\ndq_data = q_data.dequantize()\nq_error = torch.abs(data - dq_data)\nprint(f\"The maximum quantization error found on calibraiton data is: {q_error.max()} \")\n\n# Create new random data tensor and quantize with range fixed previously\ndata = torch.randn(10, 10, requires_grad=True)\nq_data = quantizer(data)\ndq_data = q_data.dequantize()\nq_error = torch.abs(data - dq_data)\nprint(f\"The maximum quantization error found on new data is: {q_error.max()} \")\n</pre>  # Create new random data tensor data = torch.randn(10, 10, requires_grad=True)  # Instantiate quantizer: quantizer = MyQuantizer(num_bits=8, symmetric=False)  # Estimate range with estimate_ranges(quantizer, RunningMinMaxRangeEstimator):     q_data = quantizer(data)  # Compute quantization error over the calibration data dq_data = q_data.dequantize() q_error = torch.abs(data - dq_data) print(f\"The maximum quantization error found on calibraiton data is: {q_error.max()} \")  # Create new random data tensor and quantize with range fixed previously data = torch.randn(10, 10, requires_grad=True) q_data = quantizer(data) dq_data = q_data.dequantize() q_error = torch.abs(data - dq_data) print(f\"The maximum quantization error found on new data is: {q_error.max()} \") <pre>The maximum quantization error found on calibraiton data is: 0.016443923115730286 \nThe maximum quantization error found on new data is: 0.6475285291671753 \n</pre> <pre>/tmp/ipykernel_145204/439193663.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  _offset = torch.round(torch.tensor(offset, device=quant_data.device))\n</pre> <p>Certain range estimator will also require you to implement the <code>fastforward.range_setting.SupportsRangeBasedOperator</code> protocol. In this case, we just need to add the following method to <code>MyQuantizer</code>:</p> In\u00a0[6]: Copied! <pre># class MyQuantizer:\n#    ...\ndef operator_for_range(\n    self, min_range: Tensor, max_range: Tensor, data_shape: torch.Size\n) -&gt; Callable[[torch.Tensor], QuantizedTensor]:\n    scale, offset = parameters_for_range(\n        min_range, max_range, self.num_bits, self.symmetric, allow_one_sided=False\n    )\n    return MyQuantizationFunction.bind(num_bits=self.num_bits, scale=scale, offset=offset)\n\n\n# %% [markdown]\n# Copyright (c) 2024 Qualcomm Technologies, Inc.\n# All Rights Reserved.\n</pre>   # class MyQuantizer: #    ... def operator_for_range(     self, min_range: Tensor, max_range: Tensor, data_shape: torch.Size ) -&gt; Callable[[torch.Tensor], QuantizedTensor]:     scale, offset = parameters_for_range(         min_range, max_range, self.num_bits, self.symmetric, allow_one_sided=False     )     return MyQuantizationFunction.bind(num_bits=self.num_bits, scale=scale, offset=offset)   # %% [markdown] # Copyright (c) 2024 Qualcomm Technologies, Inc. # All Rights Reserved."},{"location":"examples/implement_custom_quantizer.nb/#implementing-a-custom-quantizer","title":"Implementing a custom Quantizer\u00b6","text":"<p>When implementing a new quantizer, we can identify two main components in the FastForward stack: quantizers (torch modules) and quantization functions (static classes).</p>"},{"location":"examples/implement_custom_quantizer.nb/#quantizer-module","title":"Quantizer Module\u00b6","text":"<p><code>Quantizer</code> classes should implement the <code>quantize</code> method, taking a <code>Tensor</code> as input and returning a <code>QuantizedTensor</code>.</p> <p>To correctly quantize data, the <code>Quantizer</code> should delegate to an implementation of a class in the <code>QuantizationFunction</code> hierarchy.</p> <p>Quantizers modules normally lives in the <code>fastforward.nn</code> package.</p>"},{"location":"examples/implement_custom_quantizer.nb/#quantization-function","title":"Quantization Function\u00b6","text":"<p>Each <code>QuantizationFunction</code> class represents a class of quantization functions. QuantizationFunctions normally lives in the <code>fastforward.quantization</code> package.</p> <p>These classes should implement <code>quantize</code> and <code>dequantize</code> methods and do the actual computation that transforms a float tensor to a <code>QuantizeTensor</code> and back to the float format.</p> <p>QuantizationFunctions are static: they are containers of the functions that operate on the data. For this reason you can't create objects of type QuantizationFunction. On the other hand, calling <code>QuantizationFunction.bind(...)</code> one can bind parameters to a <code>QuantizationFunction</code> class, storing these information in an object of type <code>BoundQuantizationFunction</code>.</p> <p>More importantly, you can quantize an input tensor simply using the method <code>QuantizationFunction.apply(...)</code>.</p> <p>Once a tensor is quantized, a <code>BoundQuantizationFunction</code> containing all the quantization parameters is attached to it so that one can easily interpret the data or dequantize back to well-known data representation (floating point) simply calling <code>dequantize(...)</code> method on the tensor.</p>"},{"location":"examples/implement_custom_quantizer.nb/#quantization-autograd-function","title":"Quantization Autograd Function\u00b6","text":"<p><code>QuantizationAutogradFunction</code> is a variant of <code>QuantizationFunction</code> with the extra functionality of a custom backward pass implementation for the whole <code>quant&amp;dequant</code> operation. This enables one to optimize code for the backward pass and avoid unneeded computation.</p> <p>Instead, <code>QuantizationFunction</code> backward rely on the standard torch autograd, applied to the <code>quantize</code> and <code>dequantize</code> functions, doing some extra computation.</p> <p></p> <p>To implement the quantizer, one should at least implement a custom Quantizer class inheriting from <code>fastforward.nn.Quantizer</code> and a custom <code>QuantizationFunction</code> class inheriting from <code>fastforward.quantization.function.QuantizationFunction</code> or <code>fastforward.quantization.function.QuantizationAutogradFunction</code>.</p>"},{"location":"examples/implement_custom_quantizer.nb/#minimal-example-implementing-a-basic-linear-quantizer","title":"Minimal Example: implementing a basic linear quantizer\u00b6","text":"<p>In this example we show how to create a new quantizer from scratch with minimum effort. To do so, we first implement a custom <code>QuantizationFunction</code> in its simplest version (i.e. without custom backward pass), and then a custom <code>Quantizer</code> class.</p> <p>We can implement our custom <code>QuantizationFunction</code> with the following code:</p>"},{"location":"examples/mpath.nb/","title":"MPath: A Query-Based Method for Selecting PyTorch Submodules","text":"In\u00a0[1]: Copied! <pre>import torch\n\nimport fastforward as ff\n</pre> import torch  import fastforward as ff In\u00a0[2]: Copied! <pre>def module(**kwargs: torch.nn.Module) -&gt; torch.nn.ModuleDict:\n    return torch.nn.ModuleDict(kwargs)\n\n\ndef linear() -&gt; torch.nn.Linear:\n    return torch.nn.Linear(10, 10)\n\n\ndef conv() -&gt; torch.nn.Conv2d:\n    return torch.nn.Conv2d(3, 3, 3)\n\n\nmy_module = module(\n    layer1=module(sublayer1=linear(), sublayer2=conv()),\n    layer2=module(sublayer1=linear(), activation=torch.nn.ReLU(), sublayer2=conv()),\n    layer3=module(sublayer1=linear(), sublayer2=conv()),\n    layer4=module(\n        sublayer1=linear(),\n        sublayer2=module(first=linear(), second=conv(), last=module(only=linear())),\n    ),\n)\nmy_module\n</pre> def module(**kwargs: torch.nn.Module) -&gt; torch.nn.ModuleDict:     return torch.nn.ModuleDict(kwargs)   def linear() -&gt; torch.nn.Linear:     return torch.nn.Linear(10, 10)   def conv() -&gt; torch.nn.Conv2d:     return torch.nn.Conv2d(3, 3, 3)   my_module = module(     layer1=module(sublayer1=linear(), sublayer2=conv()),     layer2=module(sublayer1=linear(), activation=torch.nn.ReLU(), sublayer2=conv()),     layer3=module(sublayer1=linear(), sublayer2=conv()),     layer4=module(         sublayer1=linear(),         sublayer2=module(first=linear(), second=conv(), last=module(only=linear())),     ), ) my_module Out[2]: <pre>ModuleDict(\n  (layer1): ModuleDict(\n    (sublayer1): Linear(in_features=10, out_features=10, bias=True)\n    (sublayer2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  )\n  (layer2): ModuleDict(\n    (sublayer1): Linear(in_features=10, out_features=10, bias=True)\n    (activation): ReLU()\n    (sublayer2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  )\n  (layer3): ModuleDict(\n    (sublayer1): Linear(in_features=10, out_features=10, bias=True)\n    (sublayer2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  )\n  (layer4): ModuleDict(\n    (sublayer1): Linear(in_features=10, out_features=10, bias=True)\n    (sublayer2): ModuleDict(\n      (first): Linear(in_features=10, out_features=10, bias=True)\n      (second): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n      (last): ModuleDict(\n        (only): Linear(in_features=10, out_features=10, bias=True)\n      )\n    )\n  )\n)</pre> In\u00a0[3]: Copied! <pre>ff.mpath.search(\"**/[cls:torch.nn.Linear]\", my_module)\n</pre> ff.mpath.search(\"**/[cls:torch.nn.Linear]\", my_module) Out[3]: <pre>MPathCollection([\n    &lt;0: layer4.sublayer1: Linear&gt;,\n    &lt;1: layer4.sublayer2.first: Linear&gt;,\n    &lt;2: layer4.sublayer2.last.only: Linear&gt;,\n    &lt;3: layer3.sublayer1: Linear&gt;,\n    &lt;4: layer2.sublayer1: Linear&gt;,\n    &lt;5: layer1.sublayer1: Linear&gt;,\n])</pre> <p>When we use <code>mpath.search</code>, it returns an <code>MPathCollection</code>. This collection contains submodules in <code>my_module</code> that match our query string.</p> <p>In the query string:</p> <ul> <li><code>**</code> means \"match zero or more modules\"</li> <li><code>[cls:torch.nn.Linear]</code> matches exactly one module of type <code>torch.nn.Linear</code></li> </ul> <p>Altarnatively, we could also search for all <code>Linear</code> layers specifically within the <code>layer4</code> submodule:</p> In\u00a0[4]: Copied! <pre>ff.mpath.search(\"layer4/**/[cls:torch.nn.Linear]\", my_module)\n</pre> ff.mpath.search(\"layer4/**/[cls:torch.nn.Linear]\", my_module) Out[4]: <pre>MPathCollection([\n    &lt;0: layer4.sublayer2.first: Linear&gt;,\n    &lt;1: layer4.sublayer2.last.only: Linear&gt;,\n    &lt;2: layer4.sublayer1: Linear&gt;,\n])</pre> <p>In this example, we included the double wildcard <code>**</code> to match any module within the <code>layer4</code> submodule.</p> <p>Alternatively, we could use a single wildcard <code>*</code>, which means \"match exactly one module\". This would result in finding only <code>layer4.sublayer2.first</code> in our collection:</p> In\u00a0[5]: Copied! <pre>ff.mpath.search(\"layer4/*/[cls:torch.nn.Linear]\", my_module)\n</pre> ff.mpath.search(\"layer4/*/[cls:torch.nn.Linear]\", my_module) Out[5]: <pre>MPathCollection([\n    &lt;0: layer4.sublayer2.first: Linear&gt;,\n])</pre> <p>Lastly, if we don't use a wildcard at all, we will only match the <code>Linear</code> layers that are direct children of <code>layer4</code>:</p> In\u00a0[6]: Copied! <pre>ff.mpath.search(\"layer4/[cls:torch.nn.Linear]\", my_module)\n</pre> ff.mpath.search(\"layer4/[cls:torch.nn.Linear]\", my_module) Out[6]: <pre>MPathCollection([\n    &lt;0: layer4.sublayer1: Linear&gt;,\n])</pre> In\u00a0[7]: Copied! <pre>ff.mpath.search(\"layer4/*/[cls:torch.nn.Linear]\", my_module)\n</pre> ff.mpath.search(\"layer4/*/[cls:torch.nn.Linear]\", my_module) Out[7]: <pre>MPathCollection([\n    &lt;0: layer4.sublayer2.first: Linear&gt;,\n])</pre> In\u00a0[8]: Copied! <pre>ff.mpath.search(\n    r\"[re:layer[12\\]]/sublayer1\", my_module\n)  # we have to escape ']' in the regex because the regex pattern is '[' and ']' delimited\n</pre> ff.mpath.search(     r\"[re:layer[12\\]]/sublayer1\", my_module )  # we have to escape ']' in the regex because the regex pattern is '[' and ']' delimited Out[8]: <pre>MPathCollection([\n    &lt;0: layer2.sublayer1: Linear&gt;,\n    &lt;1: layer1.sublayer1: Linear&gt;,\n])</pre> In\u00a0[9]: Copied! <pre>ff.mpath.search(\"layer2/~[cls:torch.nn.Linear]\", my_module)\n</pre> ff.mpath.search(\"layer2/~[cls:torch.nn.Linear]\", my_module) Out[9]: <pre>MPathCollection([\n    &lt;0: layer2.activation: ReLU&gt;,\n    &lt;1: layer2.sublayer2: Conv2d&gt;,\n])</pre> In\u00a0[10]: Copied! <pre>ff.quantize_model(my_module)\n</pre> ff.quantize_model(my_module) Out[10]: <pre>QuantizedModuleDict(\n  (layer1): QuantizedModuleDict(\n    (sublayer1): QuantizedLinear(\n      in_features=10, out_features=10, bias=True\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n    (sublayer2): QuantizedConv2d(\n      3, 3, kernel_size=(3, 3), stride=(1, 1)\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n  )\n  (layer2): QuantizedModuleDict(\n    (sublayer1): QuantizedLinear(\n      in_features=10, out_features=10, bias=True\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n    (activation): QuantizedRelu(\n      (input_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n    (sublayer2): QuantizedConv2d(\n      3, 3, kernel_size=(3, 3), stride=(1, 1)\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n  )\n  (layer3): QuantizedModuleDict(\n    (sublayer1): QuantizedLinear(\n      in_features=10, out_features=10, bias=True\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n    (sublayer2): QuantizedConv2d(\n      3, 3, kernel_size=(3, 3), stride=(1, 1)\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n  )\n  (layer4): QuantizedModuleDict(\n    (sublayer1): QuantizedLinear(\n      in_features=10, out_features=10, bias=True\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n    (sublayer2): QuantizedModuleDict(\n      (first): QuantizedLinear(\n        in_features=10, out_features=10, bias=True\n        (input_quantizer): QuantizerStub()\n        (weight_quantizer): QuantizerStub()\n        (bias_quantizer): QuantizerStub()\n        (output_quantizer): QuantizerStub()\n      )\n      (second): QuantizedConv2d(\n        3, 3, kernel_size=(3, 3), stride=(1, 1)\n        (input_quantizer): QuantizerStub()\n        (weight_quantizer): QuantizerStub()\n        (bias_quantizer): QuantizerStub()\n        (output_quantizer): QuantizerStub()\n      )\n      (last): QuantizedModuleDict(\n        (only): QuantizedLinear(\n          in_features=10, out_features=10, bias=True\n          (input_quantizer): QuantizerStub()\n          (weight_quantizer): QuantizerStub()\n          (bias_quantizer): QuantizerStub()\n          (output_quantizer): QuantizerStub()\n        )\n      )\n    )\n  )\n)</pre> <p>Let's say we want to initialize all output quantizers for linear layers to 4-bit per-tensor linear quantizers. First, let's find all the relevant quantizers:</p> In\u00a0[11]: Copied! <pre>quantizers = ff.find_quantizers(my_module, \"**/[cls:torch.nn.Linear]/[quantizer:activation/output]\")\nquantizers\n</pre> quantizers = ff.find_quantizers(my_module, \"**/[cls:torch.nn.Linear]/[quantizer:activation/output]\") quantizers Out[11]: <pre>QuantizerCollection([\n    &lt;0: layer4.sublayer1.output_quantizer: QuantizerStub&gt;,\n    &lt;1: layer4.sublayer2.first.output_quantizer: QuantizerStub&gt;,\n    &lt;2: layer4.sublayer2.last.only.output_quantizer: QuantizerStub&gt;,\n    &lt;3: layer3.sublayer1.output_quantizer: QuantizerStub&gt;,\n    &lt;4: layer2.sublayer1.output_quantizer: QuantizerStub&gt;,\n    &lt;5: layer1.sublayer1.output_quantizer: QuantizerStub&gt;,\n])</pre> <p>Note that instead of using <code>fastforward.mpath.search</code>, we are using <code>ff.find_quantizers</code>. This returns a <code>QuantizerCollection</code> instead of an <code>MPathCollection</code>. Members of this collection are always quantizers. It also supports a convenient method for initializing quantizers. Let's do that now:</p> In\u00a0[12]: Copied! <pre>quantizers.initialize(ff.nn.LinearQuantizer, num_bits=4, granularity=ff.PerTensor())\nquantizers\n</pre> quantizers.initialize(ff.nn.LinearQuantizer, num_bits=4, granularity=ff.PerTensor()) quantizers Out[12]: <pre>QuantizerCollection([\n    &lt;0: layer4.sublayer1.output_quantizer: LinearQuantizer&gt;,\n    &lt;1: layer4.sublayer2.first.output_quantizer: LinearQuantizer&gt;,\n    &lt;2: layer4.sublayer2.last.only.output_quantizer: LinearQuantizer&gt;,\n    &lt;3: layer3.sublayer1.output_quantizer: LinearQuantizer&gt;,\n    &lt;4: layer2.sublayer1.output_quantizer: LinearQuantizer&gt;,\n    &lt;5: layer1.sublayer1.output_quantizer: LinearQuantizer&gt;,\n])</pre> <p>In the example above, we created a <code>fastforward.nn.LinearQuantizer</code> for each element in the <code>QuantizerCollection</code> using the provided keyword arguments. All the <code>QuantizerStub</code>s from the initial <code>QuantizerCollection</code> have now been replaced by the newly defined <code>Quantizer</code> type.</p> <p>This change is reflected in the module representation below, where the output layers are now <code>LinearQuantizer</code>s.</p> In\u00a0[13]: Copied! <pre>my_module\n</pre> my_module Out[13]: <pre>QuantizedModuleDict(\n  (layer1): QuantizedModuleDict(\n    (sublayer1): QuantizedLinear(\n      in_features=10, out_features=10, bias=True\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): LinearQuantizer(num_bits=4, symmetric=True, granularity=PerTensor())\n    )\n    (sublayer2): QuantizedConv2d(\n      3, 3, kernel_size=(3, 3), stride=(1, 1)\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n  )\n  (layer2): QuantizedModuleDict(\n    (sublayer1): QuantizedLinear(\n      in_features=10, out_features=10, bias=True\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): LinearQuantizer(num_bits=4, symmetric=True, granularity=PerTensor())\n    )\n    (activation): QuantizedRelu(\n      (input_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n    (sublayer2): QuantizedConv2d(\n      3, 3, kernel_size=(3, 3), stride=(1, 1)\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n  )\n  (layer3): QuantizedModuleDict(\n    (sublayer1): QuantizedLinear(\n      in_features=10, out_features=10, bias=True\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): LinearQuantizer(num_bits=4, symmetric=True, granularity=PerTensor())\n    )\n    (sublayer2): QuantizedConv2d(\n      3, 3, kernel_size=(3, 3), stride=(1, 1)\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n  )\n  (layer4): QuantizedModuleDict(\n    (sublayer1): QuantizedLinear(\n      in_features=10, out_features=10, bias=True\n      (input_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n      (bias_quantizer): QuantizerStub()\n      (output_quantizer): LinearQuantizer(num_bits=4, symmetric=True, granularity=PerTensor())\n    )\n    (sublayer2): QuantizedModuleDict(\n      (first): QuantizedLinear(\n        in_features=10, out_features=10, bias=True\n        (input_quantizer): QuantizerStub()\n        (weight_quantizer): QuantizerStub()\n        (bias_quantizer): QuantizerStub()\n        (output_quantizer): LinearQuantizer(num_bits=4, symmetric=True, granularity=PerTensor())\n      )\n      (second): QuantizedConv2d(\n        3, 3, kernel_size=(3, 3), stride=(1, 1)\n        (input_quantizer): QuantizerStub()\n        (weight_quantizer): QuantizerStub()\n        (bias_quantizer): QuantizerStub()\n        (output_quantizer): QuantizerStub()\n      )\n      (last): QuantizedModuleDict(\n        (only): QuantizedLinear(\n          in_features=10, out_features=10, bias=True\n          (input_quantizer): QuantizerStub()\n          (weight_quantizer): QuantizerStub()\n          (bias_quantizer): QuantizerStub()\n          (output_quantizer): LinearQuantizer(num_bits=4, symmetric=True, granularity=PerTensor())\n        )\n      )\n    )\n  )\n)</pre> In\u00a0[14]: Copied! <pre>class MyModule(torch.nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def forward(self, data: torch.Tensor) -&gt; torch.Tensor:\n        return data * 2\n\n\nclass QuantizedMyModule(ff.nn.QuantizedModule, MyModule):\n    def __init_quantization__(self) -&gt; None:\n        super().__init_quantization__()\n        self.input_quantizer = ff.nn.QuantizerStub(\"my_tag_hierarchy/my_tag/input\")\n        self.output_quantizer = ff.nn.QuantizerStub(\"my_tag_hierarchy/my_tag/output\")\n\n    def forward(self, data: torch.Tensor) -&gt; torch.Tensor:\n        data = self.input_quantizer(data)\n        return self.output_quantizer(data * 2)\n</pre> class MyModule(torch.nn.Module):     def __init__(self) -&gt; None:         super().__init__()      def forward(self, data: torch.Tensor) -&gt; torch.Tensor:         return data * 2   class QuantizedMyModule(ff.nn.QuantizedModule, MyModule):     def __init_quantization__(self) -&gt; None:         super().__init_quantization__()         self.input_quantizer = ff.nn.QuantizerStub(\"my_tag_hierarchy/my_tag/input\")         self.output_quantizer = ff.nn.QuantizerStub(\"my_tag_hierarchy/my_tag/output\")      def forward(self, data: torch.Tensor) -&gt; torch.Tensor:         data = self.input_quantizer(data)         return self.output_quantizer(data * 2) In\u00a0[15]: Copied! <pre>my_quantized_module = QuantizedMyModule()\n\nff.find_quantizers(my_quantized_module, \"**/[quantizer:my_tag_hierarchy/my_tag]\")\n</pre> my_quantized_module = QuantizedMyModule()  ff.find_quantizers(my_quantized_module, \"**/[quantizer:my_tag_hierarchy/my_tag]\") Out[15]: <pre>QuantizerCollection([\n    &lt;0: input_quantizer: QuantizerStub&gt;,\n    &lt;1: output_quantizer: QuantizerStub&gt;,\n])</pre> <p>Alternatively, we can only match the input quantizer:</p> In\u00a0[16]: Copied! <pre>ff.find_quantizers(my_quantized_module, \"**/[quantizer:my_tag_hierarchy/my_tag/input]\")\n</pre> ff.find_quantizers(my_quantized_module, \"**/[quantizer:my_tag_hierarchy/my_tag/input]\") Out[16]: <pre>QuantizerCollection([\n    &lt;0: input_quantizer: QuantizerStub&gt;,\n])</pre> <p>To emphasize our earlier point about hierarchy matching, note that using <code>input</code> as a tag alone will not match any quantizer. This will not raise an error; it will simply result in an empty <code>QuantizerCollection</code>.</p> In\u00a0[17]: Copied! <pre>ff.find_quantizers(my_quantized_module, \"**/[quantizer:input]\")\n</pre> ff.find_quantizers(my_quantized_module, \"**/[quantizer:input]\") Out[17]: <pre>QuantizerCollection([])</pre> <p>Lastly, note that there is a difference between the module hierarchy and the tag hierarchy in a query string. We can mix tag and module hierarchy queries.</p> <p>For example, <code>top_module/sub_module/**/[quantizer:parameter/weight]</code> will match all quantizers in <code>top_module.sub_module</code> that have the <code>parameter/weight</code> tag.</p> <p>Copyright (c) 2024 Qualcomm Technologies, Inc. All Rights Reserved.</p>"},{"location":"examples/mpath.nb/#mpath-a-query-based-method-for-selecting-pytorch-submodules","title":"MPath: A Query-Based Method for Selecting PyTorch Submodules\u00b6","text":"<p>MPath is a utility library that is part of the <code>fastforward</code> package (<code>fastforward.mpath</code>). It simplifies the process of selecting PyTorch submodules using queries. These queries can be either query strings or manually constructed queries composed of several query fragments.</p> <p>For example, MPath can help you find all <code>Linear</code> modules that are part of a <code>decoder</code> module.</p>"},{"location":"examples/mpath.nb/#examples","title":"Examples\u00b6","text":"<p>Let's look at a few examples. First, we'll create a PyTorch module (with submodules) that we'll refer to as <code>my_module</code>:</p>"},{"location":"examples/mpath.nb/#finding-linear-modules","title":"Finding <code>Linear</code> Modules\u00b6","text":"<p>Let's try to find all <code>Linear</code> modules in <code>my_module</code>:</p>"},{"location":"examples/mpath.nb/#query-strings","title":"Query Strings\u00b6","text":"<p>By default, a query string is composed of one or multiple module names separated by <code>/</code> to indicate hierarchy. For example: <code>decoder/attention/q_mapping</code>.</p> <p>However, MPath queries are more powerful and come with the following three options out of the box. Here are examples of each:</p> <ul> <li><code>[cls:quantified name]</code> or <code>[class:quantified name]</code>: Matches a module if it is an instance of the class identified by the quantified name.</li> <li><code>[re:regex pattern]</code> or <code>[regex:regex pattern]</code>: Matches a module if its attribute name on the parent module fully matches the regex pattern.</li> <li><code>~</code>: Matches a module that does not match the specified criteria.</li> </ul>"},{"location":"examples/mpath.nb/#class-or-instance-based-matching","title":"Class or Instance-Based Matching\u00b6","text":""},{"location":"examples/mpath.nb/#regex-based-matching","title":"Regex based matching\u00b6","text":""},{"location":"examples/mpath.nb/#inverted-matching","title":"Inverted matching\u00b6","text":""},{"location":"examples/mpath.nb/#query-string-extension","title":"Query String Extension\u00b6","text":"<p>You can extend query strings and register your own extensions. A good starting point is the implementation of <code>fastforward.mpath.fragments.RegexPathSelectorFragment</code> or <code>fastforward.mpath.fragments.ClassSelectorFragments</code>. These examples are registered in <code>fastforward.mpath</code>.</p>"},{"location":"examples/mpath.nb/#mpath-for-quantization-initialization","title":"MPath for Quantization Initialization\u00b6","text":""},{"location":"examples/mpath.nb/#quantizer-initialization","title":"Quantizer Initialization\u00b6","text":"<p>Quantizer initialization, which is the process of introducing concrete quantizers to the model, can be achieved using MPath.</p> <p>First, let's turn <code>my_module</code> into a quantization-ready module. This means converting all modules in <code>my_module</code> to ones that can operate in a quantization setting. We use <code>fastforward.quantize_model</code> for this purpose.</p>"},{"location":"examples/mpath.nb/#quantizer-tags","title":"Quantizer Tags\u00b6","text":"<p>In the example above, we used the quantizer tag system to match specific types of quantizers. This system uses the format <code>[quantizer:&lt;tag&gt;(, &lt;tag&gt;)*]</code> or <code>[qtag:&lt;tag&gt;(, &lt;tag&gt;)*]</code>.</p>"},{"location":"examples/mpath.nb/#how-it-works","title":"How It Works:\u00b6","text":"<ul> <li>Tag Assignment: Tags are added to <code>QuantizerStub</code> when they are created and are assigned to any quantizer that replaces them.</li> <li>Easy Matching: This allows us to easily find quantizers that match a certain tag.</li> </ul> <p>For example, we create the following non-quantized module and its quantized counterpart:</p>"},{"location":"examples/mpath.nb/#hierarchical-tags","title":"Hierarchical Tags\u00b6","text":"<p>The tags we create can be hierarchical. For example, consider the hierarchy <code>my_tag_hierarchy -&gt; my_tag -&gt; {input, output}</code>.</p>"},{"location":"examples/mpath.nb/#how-tag-matching-works","title":"How Tag Matching Works:\u00b6","text":"<ul> <li>Root Element Matching: A tag like <code>my_tag_hierarchy</code> will match any tag that has <code>my_tag_hierarchy</code> as its root element.</li> <li>Full Path Matching: A tag like <code>my_tag_hierarchy/my_tag</code> requires both the first and second elements to match.</li> </ul> <p>Continuing our example, we construct a module and use our newly created tags to obtain a <code>QuantizerCollection</code> that contains both quantizers.</p>"},{"location":"examples/orchestrator.nb/","title":"Neural Network Execution Orchestration","text":"In\u00a0[1]: Copied! <pre>from typing import Any, Optional\n\nfrom IPython.display import Markdown, display\n\nfrom fastforward._orchestration.concurrent_execution import ConcurrentExecOrchestrator\n\n\ndef example_function(name: str) -&gt; None:\n    print(f\"#1 Start 'example_function' for {name}.\")\n    print(f\"#2 Do something for {name}.\")\n    print(f\"#3 Clean up for {name}.\")\n    print(f\"#4 Finish 'example_function' for {name}.\")\n</pre> from typing import Any, Optional  from IPython.display import Markdown, display  from fastforward._orchestration.concurrent_execution import ConcurrentExecOrchestrator   def example_function(name: str) -&gt; None:     print(f\"#1 Start 'example_function' for {name}.\")     print(f\"#2 Do something for {name}.\")     print(f\"#3 Clean up for {name}.\")     print(f\"#4 Finish 'example_function' for {name}.\") <p>Let's say we have two input to this function: <code>alice = \"Alice\"</code> and <code>bob = \"Bob\"</code> and we call the function sequantially, the output will be as expected:</p> In\u00a0[2]: Copied! <pre>people = [\"Alice\", \"Bob\"]\nalice, bob = people\n\nexample_function(alice)\nexample_function(bob)\n</pre>  people = [\"Alice\", \"Bob\"] alice, bob = people  example_function(alice) example_function(bob) <pre>#1 Start 'example_function' for Alice.\n#2 Do something for Alice.\n#3 Clean up for Alice.\n#4 Finish 'example_function' for Alice.\n#1 Start 'example_function' for Bob.\n#2 Do something for Bob.\n#3 Clean up for Bob.\n#4 Finish 'example_function' for Bob.\n</pre> <p>Now, let's say we want to run step #1 and #2 for all people before we run any step #3 or #4. We can rewrite our function as follows:</p> In\u00a0[3]: Copied! <pre>def example_function_multi(names: list[str]) -&gt; None:\n    for name in names:\n        print(f\"#1 Start 'example_function' for {name}.\")\n        print(f\"#2 Do something for {name}.\")\n    for name in names:\n        print(f\"#3 Clean up for {name}.\")\n        print(f\"#4 Finish 'example_function' for {name}.\")\n\n\nexample_function_multi(people)\n</pre> def example_function_multi(names: list[str]) -&gt; None:     for name in names:         print(f\"#1 Start 'example_function' for {name}.\")         print(f\"#2 Do something for {name}.\")     for name in names:         print(f\"#3 Clean up for {name}.\")         print(f\"#4 Finish 'example_function' for {name}.\")   example_function_multi(people) <pre>#1 Start 'example_function' for Alice.\n#2 Do something for Alice.\n#1 Start 'example_function' for Bob.\n#2 Do something for Bob.\n#3 Clean up for Alice.\n#4 Finish 'example_function' for Alice.\n#3 Clean up for Bob.\n#4 Finish 'example_function' for Bob.\n</pre> <p>However, this requires us to rewrite our function for a single person to a function that accepts multiple persons. In the case of neural networks, this may be very cumbersome. Ideally, we would rewrite our function to something like the following (and also remove some of the more verbose labeling):</p> In\u00a0[4]: Copied! <pre>def example_function_wait(name: str) -&gt; None:\n    print(f\"#1 {name}.\")\n    print(f\"#2 {name}.\")\n\n    # This function does not exist yet.\n    wait_for_all_other_people()\n\n    print(f\"#3 {name}.\")\n    print(f\"#4 {name}.\")\n</pre> def example_function_wait(name: str) -&gt; None:     print(f\"#1 {name}.\")     print(f\"#2 {name}.\")      # This function does not exist yet.     wait_for_all_other_people()      print(f\"#3 {name}.\")     print(f\"#4 {name}.\") <p>The <code>wait_for_other_people()</code> function does not exist yet, but we can implement it using the help of <code>ConcurrentExecOrchestrator</code>.</p> In\u00a0[5]: Copied! <pre>orchestrator = ConcurrentExecOrchestrator(\n    target=example_function_wait,\n    num_stages=1,\n    execution_order=[(0,)],\n)\n\n\n# Here we implement wait_for_other_people that is used in example_function_wait.\ndef wait_for_all_other_people():\n    orchestrator.synchronize()\n\n\n# Instead of calling our target function (example_function_wait) directly, we call add_batch()\n# on the orchestrator. This will register the arguments and call the target function for each\n# registered set of of arguments once the orchestrator starts.\nfor name in people:\n    orchestrator.add_batch(name)\n\norchestrator.start()\n</pre>  orchestrator = ConcurrentExecOrchestrator(     target=example_function_wait,     num_stages=1,     execution_order=[(0,)], )   # Here we implement wait_for_other_people that is used in example_function_wait. def wait_for_all_other_people():     orchestrator.synchronize()   # Instead of calling our target function (example_function_wait) directly, we call add_batch() # on the orchestrator. This will register the arguments and call the target function for each # registered set of of arguments once the orchestrator starts. for name in people:     orchestrator.add_batch(name)  orchestrator.start() <pre>#1 Alice.\n#2 Alice.\n#1 Bob.\n#2 Bob.\n#3 Alice.\n#4 Alice.\n#3 Bob.\n#4 Bob.\n</pre> <p>Note that the code above first ran <code>example_function_wait</code> up to <code>wait_for_all_other_people()</code> for <code>alice</code>, then for <code>bob</code> and then the remainder for <code>alice</code> and then for <code>bob</code> -- in order. The code above requires some 'ceremony' to set up. For this reason, this API is considered internal and should be abstracted in user code for more specific use cases.</p> In\u00a0[6]: Copied! <pre># Helper Functions\n\n\ndef tabulate_exec_order_outputs(labels: list[str], outputs: list[list[tuple[Any, ...]]]) -&gt; None:\n    max_length = max([len(str(label)) for output in outputs for label in output])\n    raw_markdown_lines = [\n        \"|\" + \"\".join(f\"{label}|\" for label in labels),\n        \"|\" + \"\".join(\"---|\" for _ in outputs),\n    ]\n    fmt = \"__{0}__: part {1}, stage {2}\"\n    for lines in zip(*outputs):\n        data = \" | \".join(fmt.format(*line).ljust(max_length) for line in lines)\n        raw_markdown_lines.append(f\"|{data}|\")\n    display(Markdown(\"\\n\".join(raw_markdown_lines)))\n</pre> # Helper Functions   def tabulate_exec_order_outputs(labels: list[str], outputs: list[list[tuple[Any, ...]]]) -&gt; None:     max_length = max([len(str(label)) for output in outputs for label in output])     raw_markdown_lines = [         \"|\" + \"\".join(f\"{label}|\" for label in labels),         \"|\" + \"\".join(\"---|\" for _ in outputs),     ]     fmt = \"__{0}__: part {1}, stage {2}\"     for lines in zip(*outputs):         data = \" | \".join(fmt.format(*line).ljust(max_length) for line in lines)         raw_markdown_lines.append(f\"|{data}|\")     display(Markdown(\"\\n\".join(raw_markdown_lines))) In\u00a0[7]: Copied! <pre>class TargetFunction:\n    def __init__(self, orchestrator: Optional[ConcurrentExecOrchestrator] = None):\n        self.orchestrator: Optional[ConcurrentExecOrchestrator] = orchestrator\n        self.outputs: list[tuple[str, int, int]] = []\n\n    def __call__(self, name: str) -&gt; None:\n        stage = self.orchestrator.stage if self.orchestrator is not None else -1\n        self.outputs.append((name, 1, stage))\n        if self.orchestrator is not None:\n            self.orchestrator.synchronize()\n        self.outputs.append((name, 2, stage))\n\n\ndef execution_order_example(\n    num_stages: int, execution_order: list[tuple[int, ...]]\n) -&gt; list[tuple[str, int, int]]:\n    target_function = TargetFunction()\n    orchestrator = ConcurrentExecOrchestrator(\n        target_function, num_stages=num_stages, execution_order=execution_order\n    )\n    target_function.orchestrator = orchestrator\n\n    for batch in people:\n        orchestrator.add_batch(batch)\n    orchestrator.start()\n    return target_function.outputs\n\n\nexec_orders: list[list[tuple[int, ...]]] = [\n    [(0,), (1,), (2,)],\n    [(0, 1), (2,)],\n    [(0, 1, 2)],\n    [(2, 0), (1,)],\n]\nexec_results = [execution_order_example(3, exec_order) for exec_order in exec_orders]\n\ntabulate_exec_order_outputs(list(map(str, exec_orders)), exec_results)\n</pre>   class TargetFunction:     def __init__(self, orchestrator: Optional[ConcurrentExecOrchestrator] = None):         self.orchestrator: Optional[ConcurrentExecOrchestrator] = orchestrator         self.outputs: list[tuple[str, int, int]] = []      def __call__(self, name: str) -&gt; None:         stage = self.orchestrator.stage if self.orchestrator is not None else -1         self.outputs.append((name, 1, stage))         if self.orchestrator is not None:             self.orchestrator.synchronize()         self.outputs.append((name, 2, stage))   def execution_order_example(     num_stages: int, execution_order: list[tuple[int, ...]] ) -&gt; list[tuple[str, int, int]]:     target_function = TargetFunction()     orchestrator = ConcurrentExecOrchestrator(         target_function, num_stages=num_stages, execution_order=execution_order     )     target_function.orchestrator = orchestrator      for batch in people:         orchestrator.add_batch(batch)     orchestrator.start()     return target_function.outputs   exec_orders: list[list[tuple[int, ...]]] = [     [(0,), (1,), (2,)],     [(0, 1), (2,)],     [(0, 1, 2)],     [(2, 0), (1,)], ] exec_results = [execution_order_example(3, exec_order) for exec_order in exec_orders]  tabulate_exec_order_outputs(list(map(str, exec_orders)), exec_results) [(0,), (1,), (2,)] [(0, 1), (2,)] [(0, 1, 2)] [(2, 0), (1,)] Alice: part 1, stage 0 Alice: part 1, stage 0 Alice: part 1, stage 0 Alice: part 1, stage 2 Bob: part 1, stage 0 Alice: part 1, stage 1 Alice: part 1, stage 1 Alice: part 1, stage 0 Alice: part 1, stage 1 Bob: part 1, stage 0 Alice: part 1, stage 2 Bob: part 1, stage 2 Bob: part 1, stage 1 Bob: part 1, stage 1 Bob: part 1, stage 0 Bob: part 1, stage 0 Alice: part 1, stage 2 Alice: part 1, stage 2 Bob: part 1, stage 1 Alice: part 1, stage 1 Bob: part 1, stage 2 Bob: part 1, stage 2 Bob: part 1, stage 2 Bob: part 1, stage 1 Alice: part 2, stage 0 Alice: part 2, stage 0 Alice: part 2, stage 0 Alice: part 2, stage 2 Bob: part 2, stage 0 Alice: part 2, stage 1 Alice: part 2, stage 1 Alice: part 2, stage 0 Alice: part 2, stage 1 Bob: part 2, stage 0 Alice: part 2, stage 2 Bob: part 2, stage 2 Bob: part 2, stage 1 Bob: part 2, stage 1 Bob: part 2, stage 0 Bob: part 2, stage 0 Alice: part 2, stage 2 Alice: part 2, stage 2 Bob: part 2, stage 1 Alice: part 2, stage 1 Bob: part 2, stage 2 Bob: part 2, stage 2 Bob: part 2, stage 2 Bob: part 2, stage 1 <p>It is still the case that for each execution a seperate thread is created. For example, when using 3 batches and 4 stages, 12 threads are created.</p> In\u00a0[8]: Copied! <pre># Hook Example\n\n# The following example shows a 3-stage orchestrator and we will be applying\n# global and single stage hooks on the last stage\n\n\ndef hook_target(idx):\n    print(f\"Partition=1 batch={idx} stage={orchestrator.stage=}\")\n    orchestrator.synchronize(f\"{idx=}\")\n    print(f\"Partition=2 batch={idx} stage={orchestrator.stage}\")\n\n\norchestrator = ConcurrentExecOrchestrator(hook_target, 3, [(0, 1), (2,)])\norchestrator.add_batch(1)\norchestrator.add_batch(2)\n\n\ndef pre_stage_hook(orchestrator: ConcurrentExecOrchestrator, stage_data: Any) -&gt; None:\n    print(f\"    &gt;&gt; pre_stage_hook stage={orchestrator.stage} batch={orchestrator.batch}\")\n\n\ndef post_stage_hook(orchestrator: ConcurrentExecOrchestrator, stage_data: Any) -&gt; None:\n    print(f\"    &gt;&gt; post_stage_hook stage={orchestrator.stage} batch={orchestrator.batch}\")\n\n\ndef global_pre_stage_hook(orchestrator: ConcurrentExecOrchestrator) -&gt; None:\n    print(f\"  &gt;&gt;&gt;&gt; global_pre_stage_hook stage={orchestrator.stage}\")\n\n\ndef global_post_stage_hook(orchestrator: ConcurrentExecOrchestrator) -&gt; None:\n    print(f\"  &gt;&gt;&gt;&gt; global_post_stage_hook stage={orchestrator.stage}\")\n\n\nhooked_stage = 2\norchestrator.register_pre_stage_hook(hooked_stage, pre_stage_hook)\norchestrator.register_post_stage_hook(hooked_stage, post_stage_hook)\norchestrator.register_global_pre_stage_hook(hooked_stage, global_pre_stage_hook)\norchestrator.register_global_post_stage_hook(hooked_stage, global_post_stage_hook)\n\n\norchestrator.start()\n</pre> # Hook Example  # The following example shows a 3-stage orchestrator and we will be applying # global and single stage hooks on the last stage   def hook_target(idx):     print(f\"Partition=1 batch={idx} stage={orchestrator.stage=}\")     orchestrator.synchronize(f\"{idx=}\")     print(f\"Partition=2 batch={idx} stage={orchestrator.stage}\")   orchestrator = ConcurrentExecOrchestrator(hook_target, 3, [(0, 1), (2,)]) orchestrator.add_batch(1) orchestrator.add_batch(2)   def pre_stage_hook(orchestrator: ConcurrentExecOrchestrator, stage_data: Any) -&gt; None:     print(f\"    &gt;&gt; pre_stage_hook stage={orchestrator.stage} batch={orchestrator.batch}\")   def post_stage_hook(orchestrator: ConcurrentExecOrchestrator, stage_data: Any) -&gt; None:     print(f\"    &gt;&gt; post_stage_hook stage={orchestrator.stage} batch={orchestrator.batch}\")   def global_pre_stage_hook(orchestrator: ConcurrentExecOrchestrator) -&gt; None:     print(f\"  &gt;&gt;&gt;&gt; global_pre_stage_hook stage={orchestrator.stage}\")   def global_post_stage_hook(orchestrator: ConcurrentExecOrchestrator) -&gt; None:     print(f\"  &gt;&gt;&gt;&gt; global_post_stage_hook stage={orchestrator.stage}\")   hooked_stage = 2 orchestrator.register_pre_stage_hook(hooked_stage, pre_stage_hook) orchestrator.register_post_stage_hook(hooked_stage, post_stage_hook) orchestrator.register_global_pre_stage_hook(hooked_stage, global_pre_stage_hook) orchestrator.register_global_post_stage_hook(hooked_stage, global_post_stage_hook)   orchestrator.start() <pre>Partition=1 batch=1 stage=orchestrator.stage=0\nPartition=1 batch=1 stage=orchestrator.stage=1\nPartition=1 batch=2 stage=orchestrator.stage=0\nPartition=1 batch=2 stage=orchestrator.stage=1\n  &gt;&gt;&gt;&gt; global_pre_stage_hook stage=2\n    &gt;&gt; pre_stage_hook stage=2 batch=0\nPartition=1 batch=1 stage=orchestrator.stage=2\n    &gt;&gt; post_stage_hook stage=2 batch=0\n    &gt;&gt; pre_stage_hook stage=2 batch=1\nPartition=1 batch=2 stage=orchestrator.stage=2\n    &gt;&gt; post_stage_hook stage=2 batch=1\n  &gt;&gt;&gt;&gt; global_post_stage_hook stage=2\nPartition=2 batch=1 stage=0\nPartition=2 batch=1 stage=1\nPartition=2 batch=2 stage=0\nPartition=2 batch=2 stage=1\n  &gt;&gt;&gt;&gt; global_pre_stage_hook stage=2\n    &gt;&gt; pre_stage_hook stage=2 batch=0\nPartition=2 batch=1 stage=2\n    &gt;&gt; post_stage_hook stage=2 batch=0\n    &gt;&gt; pre_stage_hook stage=2 batch=1\nPartition=2 batch=2 stage=2\n    &gt;&gt; post_stage_hook stage=2 batch=1\n  &gt;&gt;&gt;&gt; global_post_stage_hook stage=2\n</pre> In\u00a0[9]: Copied! <pre># Repeated stage example\n\n\ndef repeated_target(idx):\n    print(f\"partition 1 stage={orchestrator.stage} batch={orchestrator.batch}\")\n    orchestrator.synchronize()\n    num_steps = 3 if orchestrator.stage == 1 else 1\n    for i in range(num_steps):\n        print(f\"partition 2 stage={orchestrator.stage} batch={orchestrator.batch} iteration={i}\")\n        repeat_stage = orchestrator.stage == 1 and i &lt; 2\n        orchestrator.synchronize(repeat_stage=repeat_stage)\n    print(f\"partition 3 stage={orchestrator.stage} batch={orchestrator.batch}\")\n\n\norchestrator = ConcurrentExecOrchestrator(repeated_target, 3, [(0,), (1,), (2,)])\norchestrator.add_batch(1)\norchestrator.add_batch(2)\n\n\norchestrator.start()\n</pre> # Repeated stage example   def repeated_target(idx):     print(f\"partition 1 stage={orchestrator.stage} batch={orchestrator.batch}\")     orchestrator.synchronize()     num_steps = 3 if orchestrator.stage == 1 else 1     for i in range(num_steps):         print(f\"partition 2 stage={orchestrator.stage} batch={orchestrator.batch} iteration={i}\")         repeat_stage = orchestrator.stage == 1 and i &lt; 2         orchestrator.synchronize(repeat_stage=repeat_stage)     print(f\"partition 3 stage={orchestrator.stage} batch={orchestrator.batch}\")   orchestrator = ConcurrentExecOrchestrator(repeated_target, 3, [(0,), (1,), (2,)]) orchestrator.add_batch(1) orchestrator.add_batch(2)   orchestrator.start() <pre>partition 1 stage=0 batch=0\npartition 1 stage=0 batch=1\npartition 1 stage=1 batch=0\npartition 1 stage=1 batch=1\npartition 1 stage=2 batch=0\npartition 1 stage=2 batch=1\npartition 2 stage=0 batch=0 iteration=0\npartition 2 stage=0 batch=1 iteration=0\npartition 2 stage=1 batch=0 iteration=0\npartition 2 stage=1 batch=1 iteration=0\npartition 2 stage=1 batch=0 iteration=1\npartition 2 stage=1 batch=1 iteration=1\npartition 2 stage=1 batch=0 iteration=2\npartition 2 stage=1 batch=1 iteration=2\npartition 2 stage=2 batch=0 iteration=0\npartition 2 stage=2 batch=1 iteration=0\npartition 3 stage=0 batch=0\npartition 3 stage=0 batch=1\npartition 3 stage=1 batch=0\npartition 3 stage=1 batch=1\npartition 3 stage=2 batch=0\npartition 3 stage=2 batch=1\n</pre> <p>Copyright (c) 2024 Qualcomm Technologies, Inc. All Rights Reserved.</p>"},{"location":"examples/orchestrator.nb/#neural-network-execution-orchestration","title":"Neural Network Execution Orchestration\u00b6","text":"Notes on <code>fastforward._orchestration</code> <p> <code>fastforward._orchestration</code> is considered internal API. As       such, the API may change. As a user, we suggest to use a more high level       interface to members of <code>fastforward._orchestration</code>. These       docs are primarily included for FastForward developers.     </p>"},{"location":"examples/orchestrator.nb/#introduction","title":"Introduction\u00b6","text":"<p>For various training methods of neural networks, especially for quantized neural networks, algorithms may require to run network in a non-sequantial manner. The <code>ConcurrentExecOrchestrator</code> is a utility to help implement these types of algorithms. To illustrate this, let's consider the following function:</p>"},{"location":"examples/orchestrator.nb/#terminology","title":"## Terminology\u00b6","text":"<p>In our example above, we used a simple function that waited for other executions before completing. This is a use-case of the <code>ConcurrentExecOrchestrator</code>, but it is more versatile. To keep things organized, let's first introduce some termonology. For this we use the following example function:</p> <pre>def example_target(data):\n    print(\"partition_1\")\n    orchestrator.synchronize()\n    print(\"partition_2\")\n    orchestrator.synchronize()\n    print(\"partition_3\")\n</pre> <p>A target function is the function that is executed by the orchestrator. The function acts as an entry point, and for each invocation a seperate thread is created.</p> <p>The target function above is separated in three partition. We consider a partition any code that runs between two calls to <code>synchonize()</code> or between <code>synchronize()</code> and the start/end of the target function</p> <p>As shown in the introduction, the input to each invocation of the target function is registered using <code>ConcurrentExecOrchestrator.add_batch()</code>. As the name of this method suggests, we refer to each separate 'data' registration as a batch.</p> <p>Finally, the orchestrator has a notion of stages and execution order. In our examples so far we have used just one stage and the implied execution order <code>[(0,)]</code>. Stages refer to the number of times the target function is executed per batch. Although partitions always run sequentially, i.e., the $n+1$th partition only starts once the $n$th partition finishes for all executions (batches $\\times$ <code>num_stages</code>), the order of stages can be manipulated. For example, consider our two batches <code>alice</code> and <code>bob</code>, a target function with two partitions <code>p1</code> and <code>p2</code> and we'll use two stages <code>s1</code> and <code>s2</code>. The order of execution could then be any of the following (assuming <code>alice</code> denotes the first batch and <code>bob</code> the second:</p> <ul> <li><code>alice_s1_p1</code>, <code>alice_s2_p2</code>, <code>bob_s1_p1</code>, <code>bob_s2_p2</code></li> <li><code>alice_s1_p1</code>, <code>bob_s1_p2</code>, <code>alice_s2_p1</code>, <code>bob_s2_p2</code></li> <li><code>alice_s2_p1</code>, <code>alice_s1_p2</code>, <code>bob_s2_p1</code>, <code>bob_s1_p2</code></li> <li><code>alice_s2_p1</code>, <code>bob_s2_p2</code>, <code>alice_s1_p1</code>, <code>bob_s1_p2</code></li> </ul> <p>Below we show a concrete example using two batches, two partitions, and three stages.</p>"},{"location":"examples/orchestrator.nb/#error-handling","title":"Error Handling\u00b6","text":"<p>If an error occurs during one of the executions, all other executions are stopped. Once all executions have terminated a RuntimeError is raised on the main thread that references the caught execption in the execution.</p>"},{"location":"examples/orchestrator.nb/#hooks","title":"Hooks\u00b6","text":"<p>Hooks can be used to change the behaviour of <code>ConcurrentExecOrchestrator</code>. In particular there are pre and post 'stage' hooks and pre and post 'global stage' hooks. A stage hook is called either before (pre) or after (post) the execution of a single stage. I.e., this hook is called <code>num_stages * #batches * #partitions</code> times. In contrast, the 'global stage' hooks the global_pre_stage hook is invoked once per stage between two synchronize calls. Consider the same target function as above and the following orchestrator setup.</p> <p>Below we show an example execution using hooks. Since hooks have access to the orchestrator, they can manipulate the batch data through the <code>batch_data</code> attribute of the orchestrator. This allows for further communication and/or updates between stages.</p>"},{"location":"examples/orchestrator.nb/#repeated-stages","title":"Repeated Stages\u00b6","text":"<p>In some cases, for example when performing an optimization loop, we may want to repeat a single stage multiple times before moving to the next stage. This can be achieved by passing <code>repeat_stage=True</code> to the <code>synchronize()</code> method on the orchestrator. If the current batch is the last of the stage and <code>repeat_stage</code> is True, then instead of going to the next stage, another execution of all batches in the stage will follow. Please be aware of the following:</p> <ul> <li>if <code>repeat_stage=True</code> is passed for an execution corresponding to a batch that is not the last batch, it will not have an affect.</li> <li>If <code>repeat_stage=False</code> is passed to the penultimate (or any non-last batch) and <code>repeat_stage=True</code> is passed for the last batch, the stage will be repeated for all batches.</li> <li>The target function is responsible for executing a (sub-)partition multiple times. If <code>synchronize()</code> is called with <code>repeat_stage=True</code> without taking care of this, the orchestration fails and may even loop forever.</li> </ul> <p>See below for an example that has a loop in partition 2, but only for stage 1.</p>"},{"location":"examples/quantizing_networks.nb/","title":"Overview","text":"In\u00a0[1]: Copied! <pre>import copy\n\nfrom pprint import pprint\n\nimport torch\n\nimport fastforward as ff\n</pre> import copy  from pprint import pprint  import torch  import fastforward as ff <p>\u23e9 Let's start by creating some floating point data</p> In\u00a0[2]: Copied! <pre>in_features = 4\n\ndata = torch.rand(1, in_features) - 0.5\ndata\n</pre> in_features = 4  data = torch.rand(1, in_features) - 0.5 data Out[2]: <pre>tensor([[0.1579, 0.1376, 0.3846, 0.4860]])</pre> <p>\u23e9 Now, we quantize the data using 8-bit per-tensor quantization.</p> In\u00a0[3]: Copied! <pre>scale = torch.tensor([0.1])\nnum_bits = 8\nquantized_data = ff.quantization.affine.quantize_per_tensor(data, num_bits=num_bits, scale=scale)\n\nquantized_data\n</pre> scale = torch.tensor([0.1]) num_bits = 8 quantized_data = ff.quantization.affine.quantize_per_tensor(data, num_bits=num_bits, scale=scale)  quantized_data Out[3]: <pre>QuantizedTensor([[2., 1., 4., 5.]],\n                quantizer=TiledAffineQuantizationFunction, scale=tensor([0.1000]), tile_size=torch.Size([1, 4]), num_bits=8, output_dtype=torch.float32, offset=None)</pre> <p>\u2705 We can see that <code>quantized_data</code> is now a <code>QuantizedTensor</code>. This makes it very easy to see in FastForward if your data is actually quantized or not.</p> <p>\u2705 This tensor both holds the actual data (of same shape as <code>data</code>) as well as the hyperparameters of the quantizer. For this specific case the only hyperparameter is the quantization scale, which we have set manually.</p> <p>\u274c Because this data is transformed to a new coordinate system, it is not easy to see what floating point values they represent.</p> <p>\u23e9 For this purpose, we can dequantize the tensor, which we do below.</p> In\u00a0[4]: Copied! <pre>quantized_data.dequantize()\n</pre> quantized_data.dequantize() Out[4]: <pre>tensor([[0.2000, 0.1000, 0.4000, 0.5000]])</pre> In\u00a0[5]: Copied! <pre>quantizer = ff.nn.linear_quantizer.LinearQuantizer(num_bits=2)\nquantizer\n</pre> quantizer = ff.nn.linear_quantizer.LinearQuantizer(num_bits=2) quantizer Out[5]: <pre>LinearQuantizer(num_bits=2, symmetric=True, granularity=PerTensor())</pre> <p>\u23e9 Next, we try to quantize our data with our quantizer.</p> In\u00a0[6]: Copied! <pre>try:\n    quantizer(data)\nexcept ValueError as e:\n    print(\"[ERROR]\", e, \"\\n\")\n\nprint(f\"{quantizer.has_uninitialized_params=}\")\nprint(f\"{quantizer.quantization_range=}\")  # min, max values that quantizer can represent.\n</pre> try:     quantizer(data) except ValueError as e:     print(\"[ERROR]\", e, \"\\n\")  print(f\"{quantizer.has_uninitialized_params=}\") print(f\"{quantizer.quantization_range=}\")  # min, max values that quantizer can represent. <pre>[ERROR] Tried to quantize a tensor using an uninitialized quantizer (of type LinearQuantizer). This quantizer is initialized after its quantization_range is specified. This can be done explicitly by using the LinearQuantizer.quantization_range setter or using a range setting method. \n\nquantizer.has_uninitialized_params=True\nquantizer.quantization_range=(None, None)\n</pre> <p>\u274c We can see that our quantizer will not quantize any data just yet. The reason for this is that this specific quantizer has hyperparameters that need to be fitted before we can quantize any data. As a result, the quantization range is not yet set.</p> <p>\u23e9 We could set <code>quantizer.quantization_range</code> directly, but we would need to know the desired quantization range <code>(min, max)</code>.</p> <p>\u23e9 A more common approach is to use range estimation to find the optimal range based on data. We do this below.</p> In\u00a0[7]: Copied! <pre>with ff.range_setting.estimate_ranges(quantizer, ff.range_setting.smoothed_minmax):\n    quantizer(data)\n\nprint(f\"{quantizer.has_uninitialized_params=}\")\nprint(f\"{quantizer.quantization_range=}\")\nprint(f\"{data.min()=} {data.max()=}\")\n</pre> with ff.range_setting.estimate_ranges(quantizer, ff.range_setting.smoothed_minmax):     quantizer(data)  print(f\"{quantizer.has_uninitialized_params=}\") print(f\"{quantizer.quantization_range=}\") print(f\"{data.min()=} {data.max()=}\") <pre>quantizer.has_uninitialized_params=False\nquantizer.quantization_range=(tensor([0.], grad_fn=&lt;MulBackward0&gt;), tensor([0.4860], grad_fn=&lt;MulBackward0&gt;))\ndata.min()=tensor(0.1376) data.max()=tensor(0.4860)\n</pre> <p>\u2705 We have now set the quantization range and the quantizer is initialized.</p> <p>\u2705 We can see that the quantization range is the same as the range in the data batch.</p> <p>\u23e9 We will now use our quantizer to quantize the data</p> In\u00a0[8]: Copied! <pre>quantized_data = quantizer(data)  # type: ignore[assignment]\n\nquantized_data\n</pre> quantized_data = quantizer(data)  # type: ignore[assignment]  quantized_data Out[8]: <pre>QuantizedTensor([[-1., -1.,  0.,  1.]], grad_fn=&lt;AliasBackward0&gt;,\n                quantizer=TiledAffineQuantizationFunction, scale=tensor([0.1620], grad_fn=&lt;TiledAffineQuantizationFunctionFunctionBackward&gt;), tile_size=data_shape, num_bits=2, output_dtype=torch.float32, offset=tensor([2.], grad_fn=&lt;TiledAffineQuantizationFunctionFunctionBackward&gt;))</pre> In\u00a0[9]: Copied! <pre>out_features = 8\n\nunquantized_linear = torch.nn.Linear(in_features, out_features)\nprint(unquantized_linear)\n</pre> out_features = 8  unquantized_linear = torch.nn.Linear(in_features, out_features) print(unquantized_linear) <pre>Linear(in_features=4, out_features=8, bias=True)\n</pre> <p>\u23e9 In FastForward we use <code>ff.nn.QuantizedModule</code>s, they are drop-in replacements of <code>torch.nn.Module</code>s but additionally take care of quantization.</p> <p>\u23e9 Most modules in <code>torch.nn</code> are mirrored with their quantized counterpart in <code>ff.nn</code></p> <ul> <li>The goal of these quantized modules is that they behave exactly the same as their floating point counterparts, they have the same methods which have the same function signatures.</li> <li>The only difference is that we add quantizer children to the modules and we change the forward pass s.t. it performs quantized operations instead of floating point operations.</li> <li>\u26a0\ufe0f If you do not find your desired module in <code>ff.nn</code>, you can either open an issue with us, or implement the layer yourself.</li> </ul> <p>\u23e9 Let a closer look at the <code>ff.nn.QuantizedLinear</code> below.</p> <ul> <li>For now, we manually copied the weight data s.t. the <code>quantized_linear</code> matches the <code>unquantized_linear</code>. In the next section we will show convenience methods to automatically quantize modules in-place.</li> </ul> In\u00a0[10]: Copied! <pre>quantized_linear = ff.nn.QuantizedLinear(in_features, out_features)\nquantized_linear.weight.data = unquantized_linear.weight.data.clone()\nquantized_linear.bias.data = unquantized_linear.bias.data.clone()\n\nprint(quantized_linear)\n</pre> quantized_linear = ff.nn.QuantizedLinear(in_features, out_features) quantized_linear.weight.data = unquantized_linear.weight.data.clone() quantized_linear.bias.data = unquantized_linear.bias.data.clone()  print(quantized_linear) <pre>QuantizedLinear(\n  in_features=4, out_features=8, bias=True\n  (input_quantizer): QuantizerStub()\n  (weight_quantizer): QuantizerStub()\n  (bias_quantizer): QuantizerStub()\n  (output_quantizer): QuantizerStub()\n)\n</pre> <p>\u23e9 We can see that our QuantizedLinear has the same representation as the Linear, but instead there are four quantizer children added.</p> <ul> <li>In this case the bias_quantizer is <code>None</code> since this layer does not have a bias.</li> </ul> <p>\u23e9 Observe that all quantizers are set to be <code>QuantizerStub</code>s. These are no-op placeholders that can be repaced with quantizers if desired.</p> <p>\u23e9 Let's try to push data trough our <code>QuantizedLinear</code>.</p> In\u00a0[11]: Copied! <pre>try:\n    quantized_output = quantized_linear(data)\nexcept ff.exceptions.QuantizationError as e:\n    print(\"[ERROR]\", e, \"\\n\")\n</pre> try:     quantized_output = quantized_linear(data) except ff.exceptions.QuantizationError as e:     print(\"[ERROR]\", e, \"\\n\") <pre>[ERROR] Expected 'input' to be an instance of 'QuantizedTensor' because strict_quantization=True. \n\n</pre> <p>\u274c We see we cannot push data trough our QuantizedLinear because <code>strict_quantization=True</code>.</p> <ul> <li>This flag tries to catch a common error-case in simulated quantization where no quantization is taking place, and the user is not aware of this.</li> <li>In our case, we have not assigned any quantizers, so the layer will behave as a floating point layer, which is not allowed if <code>strict_quantization=True</code>.</li> </ul> <p>\u23e9 Let's temporarily disable the <code>strict_quantization</code> setting and see what happens when we call the <code>quantized_linear</code>.</p> In\u00a0[12]: Copied! <pre>with ff.strict_quantization(False):\n    quantized_output = quantized_linear(data)\n\nunquantized_output = unquantized_linear(data)\n\nprint(f\"{unquantized_output=}\")\nprint(f\"{quantized_output=}\")\n</pre> with ff.strict_quantization(False):     quantized_output = quantized_linear(data)  unquantized_output = unquantized_linear(data)  print(f\"{unquantized_output=}\") print(f\"{quantized_output=}\") <pre>unquantized_output=tensor([[-0.1949, -0.5414, -0.5109,  0.0402, -0.1622,  0.2810,  0.3713, -0.3943]],\n       grad_fn=&lt;AddmmBackward0&gt;)\nquantized_output=tensor([[-0.1949, -0.5414, -0.5109,  0.0402, -0.1622,  0.2810,  0.3713, -0.3943]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n</pre> <p>\u2705 Indeed, the <code>quantized_linear</code> is behaving exactly as the <code>unquantized_linear</code> as we have not specified any quantizers.</p> <p>\u23e9 We will now assign quantizers to all of the quantizer fields</p> In\u00a0[13]: Copied! <pre>quantized_linear.input_quantizer = ff.nn.linear_quantizer.LinearQuantizer(num_bits=2)\nquantized_linear.weight_quantizer = ff.nn.linear_quantizer.LinearQuantizer(num_bits=2)\nquantized_linear.output_quantizer = ff.nn.linear_quantizer.LinearQuantizer(num_bits=2)\nprint(quantized_linear)\n</pre> quantized_linear.input_quantizer = ff.nn.linear_quantizer.LinearQuantizer(num_bits=2) quantized_linear.weight_quantizer = ff.nn.linear_quantizer.LinearQuantizer(num_bits=2) quantized_linear.output_quantizer = ff.nn.linear_quantizer.LinearQuantizer(num_bits=2) print(quantized_linear) <pre>QuantizedLinear(\n  in_features=4, out_features=8, bias=True\n  (input_quantizer): LinearQuantizer(num_bits=2, symmetric=True, granularity=PerTensor())\n  (weight_quantizer): LinearQuantizer(num_bits=2, symmetric=True, granularity=PerTensor())\n  (bias_quantizer): QuantizerStub()\n  (output_quantizer): LinearQuantizer(num_bits=2, symmetric=True, granularity=PerTensor())\n)\n</pre> <p>\u23e9 As we know from the example above, we first need to initialize the quantizers by passing data trough. Let's do so.</p> In\u00a0[14]: Copied! <pre>print(\"Before range estimation\")\nprint(f\"{quantized_linear.input_quantizer.quantization_range=}\")\nprint(f\"{quantized_linear.weight_quantizer.quantization_range=}\")\nprint(f\"{quantized_linear.output_quantizer.quantization_range=}\")\nprint()\n\n\nwith ff.range_setting.estimate_ranges(quantized_linear, ff.range_setting.smoothed_minmax):\n    quantized_linear(data)\n\nprint(\"After range estimation\")\nprint(f\"{quantized_linear.input_quantizer.quantization_range=}\")\nprint(f\"{quantized_linear.weight_quantizer.quantization_range=}\")\nprint(f\"{quantized_linear.output_quantizer.quantization_range=}\")\nprint()\n</pre> print(\"Before range estimation\") print(f\"{quantized_linear.input_quantizer.quantization_range=}\") print(f\"{quantized_linear.weight_quantizer.quantization_range=}\") print(f\"{quantized_linear.output_quantizer.quantization_range=}\") print()   with ff.range_setting.estimate_ranges(quantized_linear, ff.range_setting.smoothed_minmax):     quantized_linear(data)  print(\"After range estimation\") print(f\"{quantized_linear.input_quantizer.quantization_range=}\") print(f\"{quantized_linear.weight_quantizer.quantization_range=}\") print(f\"{quantized_linear.output_quantizer.quantization_range=}\") print() <pre>Before range estimation\nquantized_linear.input_quantizer.quantization_range=(None, None)\nquantized_linear.weight_quantizer.quantization_range=(None, None)\nquantized_linear.output_quantizer.quantization_range=(None, None)\n\nAfter range estimation\nquantized_linear.input_quantizer.quantization_range=(tensor([0.], grad_fn=&lt;MulBackward0&gt;), tensor([0.4860], grad_fn=&lt;MulBackward0&gt;))\nquantized_linear.weight_quantizer.quantization_range=(tensor([-0.8725], grad_fn=&lt;MulBackward0&gt;), tensor([0.4362], grad_fn=&lt;MulBackward0&gt;))\nquantized_linear.output_quantizer.quantization_range=(tensor([-0.8864], grad_fn=&lt;MulBackward0&gt;), tensor([0.4432], grad_fn=&lt;MulBackward0&gt;))\n\n</pre> <p>\u2705 We can see that all the quantizers in our layer are initialized now.</p> <p>\u23e9 We should now be able to call our <code>quantized_linear</code>. Let's do that!</p> In\u00a0[15]: Copied! <pre>unquantized_output = unquantized_linear(data)\nquantized_output = quantized_linear(data)\n\nprint(f\"{unquantized_output=}\")\nprint()\nprint(f\"{quantized_output=}\")\nprint()\nprint(f\"{quantized_output.dequantize()=}\")\n</pre> unquantized_output = unquantized_linear(data) quantized_output = quantized_linear(data)  print(f\"{unquantized_output=}\") print() print(f\"{quantized_output=}\") print() print(f\"{quantized_output.dequantize()=}\") <pre>unquantized_output=tensor([[-0.1949, -0.5414, -0.5109,  0.0402, -0.1622,  0.2810,  0.3713, -0.3943]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nquantized_output=QuantizedTensor([[-1., -1., -1.,  0., -0.,  1.,  1., -1.]],\n                grad_fn=&lt;AliasBackward0&gt;,\n                quantizer=TiledAffineQuantizationFunction, scale=tensor([0.4432], grad_fn=&lt;TiledAffineQuantizationFunctionFunctionBackward&gt;), tile_size=data_shape, num_bits=2, output_dtype=torch.float32, offset=tensor([0.], grad_fn=&lt;TiledAffineQuantizationFunctionFunctionBackward&gt;))\n\nquantized_output.dequantize()=tensor([[-0.4432, -0.4432, -0.4432,  0.0000,  0.0000,  0.4432,  0.4432, -0.4432]],\n       grad_fn=&lt;STEAutogradFuncBackward&gt;)\n</pre> <p>\u2705 We can now see that our <code>quantized_linear</code> is behaving as expected:</p> <ul> <li>The output is a <code>QuantizedTensor</code></li> <li>The dequantized output is close to the floating point output, but it is not identical due to quantization error.</li> </ul> <p>\u23e9 We start by making a simple unquantized MLP model.</p> In\u00a0[16]: Copied! <pre>hidden_features = 3\n\nunquantized_model = torch.nn.Sequential(\n    torch.nn.Linear(in_features, hidden_features),\n    torch.nn.ReLU(),\n    torch.nn.Linear(hidden_features, hidden_features),\n    torch.nn.ReLU(),\n    torch.nn.Linear(hidden_features, out_features),\n    torch.nn.ReLU(),\n)\n\nunquantized_model\n</pre> hidden_features = 3  unquantized_model = torch.nn.Sequential(     torch.nn.Linear(in_features, hidden_features),     torch.nn.ReLU(),     torch.nn.Linear(hidden_features, hidden_features),     torch.nn.ReLU(),     torch.nn.Linear(hidden_features, out_features),     torch.nn.ReLU(), )  unquantized_model Out[16]: <pre>Sequential(\n  (0): Linear(in_features=4, out_features=3, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=3, out_features=3, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=3, out_features=8, bias=True)\n  (5): ReLU()\n)</pre> In\u00a0[17]: Copied! <pre>ff.quantized_module_map()\n</pre> ff.quantized_module_map() Out[17]: <pre>{torch.nn.modules.activation.ReLU: fastforward.nn.activations.QuantizedRelu,\n torch.nn.modules.activation.SiLU: fastforward.nn.activations.QuantizedSilu,\n torch.nn.modules.container.Sequential: fastforward.nn.container.QuantizedSequential,\n torch.nn.modules.container.ModuleList: fastforward.nn.container.QuantizedModuleList,\n torch.nn.modules.container.ModuleDict: fastforward.nn.container.QuantizedModuleDict,\n torch.nn.modules.container.ParameterList: fastforward.nn.container.QuantizedParameterList,\n torch.nn.modules.container.ParameterDict: fastforward.nn.container.QuantizedParameterDict,\n torch.nn.modules.conv.Conv2d: fastforward.nn.conv.QuantizedConv2d,\n torch.nn.modules.conv.Conv1d: fastforward.nn.conv.QuantizedConv1d,\n torch.nn.modules.sparse.Embedding: fastforward.nn.embedding.QuantizedEmbedding,\n torch.nn.modules.linear.Linear: fastforward.nn.linear.QuantizedLinear,\n torch.nn.modules.normalization.LayerNorm: fastforward.nn.normalization.QuantizedLayerNorm}</pre> <p>\u23e9 We will now run the <code>quantize_model</code> function.</p> <p>\u26a0\ufe0f Note that the <code>ff.nn.quantize_model</code> changes the module classes in-place!</p> <ul> <li>Because we want to keep access to the full precision network for our comparison we first deepcopy the floating point model.</li> </ul> In\u00a0[18]: Copied! <pre>quantized_model = copy.deepcopy(unquantized_model)\nff.quantize_model(quantized_model)\nquantized_model\n</pre> quantized_model = copy.deepcopy(unquantized_model) ff.quantize_model(quantized_model) quantized_model Out[18]: <pre>QuantizedSequential(\n  (0): QuantizedLinear(\n    in_features=4, out_features=3, bias=True\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): QuantizerStub()\n    (bias_quantizer): QuantizerStub()\n    (output_quantizer): QuantizerStub()\n  )\n  (1): QuantizedRelu(\n    (input_quantizer): QuantizerStub()\n    (output_quantizer): QuantizerStub()\n  )\n  (2): QuantizedLinear(\n    in_features=3, out_features=3, bias=True\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): QuantizerStub()\n    (bias_quantizer): QuantizerStub()\n    (output_quantizer): QuantizerStub()\n  )\n  (3): QuantizedRelu(\n    (input_quantizer): QuantizerStub()\n    (output_quantizer): QuantizerStub()\n  )\n  (4): QuantizedLinear(\n    in_features=3, out_features=8, bias=True\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): QuantizerStub()\n    (bias_quantizer): QuantizerStub()\n    (output_quantizer): QuantizerStub()\n  )\n  (5): QuantizedRelu(\n    (input_quantizer): QuantizerStub()\n    (output_quantizer): QuantizerStub()\n  )\n)</pre> <p>\u2705 We see that all modules in the model are now replaced with their quantized counterpart.</p> <p>\u23e9 Since no quantizers are inserted yet, let's confirm that the <code>quantized_model</code> still behaves the same as the <code>unquantized_model</code>.</p> In\u00a0[19]: Copied! <pre>with ff.strict_quantization(False):\n    quantized_output = quantized_model(data)\n\nunquantized_output = unquantized_model(data)\n\nprint(f\"{unquantized_output=}\")\nprint(f\"{quantized_output=}\")\n</pre> with ff.strict_quantization(False):     quantized_output = quantized_model(data)  unquantized_output = unquantized_model(data)  print(f\"{unquantized_output=}\") print(f\"{quantized_output=}\") <pre>unquantized_output=tensor([[0.0795, 0.0000, 0.4139, 0.0000, 0.4434, 0.0000, 0.0000, 0.0938]],\n       grad_fn=&lt;ReluBackward0&gt;)\nquantized_output=tensor([[0.0795, 0.0000, 0.4139, 0.0000, 0.4434, 0.0000, 0.0000, 0.0938]],\n       grad_fn=&lt;ReluBackward0&gt;)\n</pre> In\u00a0[20]: Copied! <pre>config = ff.QuantizationConfig()\n\n# We want to quantize all weights in the model.\nconfig.add_rule(\n    \"**/[quantizer:parameter/weight]\",\n    ff.nn.LinearQuantizer,\n    num_bits=8,\n    symmetric=True,\n    granularity=ff.PerChannel(),\n)\n\n# We want to quantize all the outputs in the model, too.\nconfig.add_rule(\n    \"**/[quantizer:activation/output]\",\n    ff.nn.LinearQuantizer,\n    num_bits=8,\n    symmetric=False,\n    granularity=ff.PerTensor(),\n)\n\n\n# We only want to enable the input quantizer of the first layer, so that we can turn a floating point input into a quantized input.\n# For the subsequent layers, the input will already be quantized because there will be an output quantizer in the layer before that.\ndef input_factory(name: str, current_quantizer: ff.nn.Quantizer) -&gt; ff.nn.Quantizer:\n    return ff.nn.LinearQuantizer(num_bits=8, symmetric=False, granularity=ff.PerTensor())\n\n\nconfig.add_rule(\"0/[quantizer:activation/input]\", input_factory)\n\nconfig\n</pre> config = ff.QuantizationConfig()  # We want to quantize all weights in the model. config.add_rule(     \"**/[quantizer:parameter/weight]\",     ff.nn.LinearQuantizer,     num_bits=8,     symmetric=True,     granularity=ff.PerChannel(), )  # We want to quantize all the outputs in the model, too. config.add_rule(     \"**/[quantizer:activation/output]\",     ff.nn.LinearQuantizer,     num_bits=8,     symmetric=False,     granularity=ff.PerTensor(), )   # We only want to enable the input quantizer of the first layer, so that we can turn a floating point input into a quantized input. # For the subsequent layers, the input will already be quantized because there will be an output quantizer in the layer before that. def input_factory(name: str, current_quantizer: ff.nn.Quantizer) -&gt; ff.nn.Quantizer:     return ff.nn.LinearQuantizer(num_bits=8, symmetric=False, granularity=ff.PerTensor())   config.add_rule(\"0/[quantizer:activation/input]\", input_factory)  config Out[20]: <pre>QuantizationConfig(\n  **/[quantizer:parameter/weight] =&gt; LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0)),\n  **/[quantizer:activation/output] =&gt; LinearQuantizer(num_bits=8, symmetric=False, granularity=PerTensor()),\n  0/[quantizer:activation/input] =&gt; input_factory(&lt;module_name&gt;, &lt;current_quantizer&gt;)\n)</pre> <p>\u2705 Note that in the rule for input quantizers we could have directly specified the quantizer class, but we instead show an example of using a factory function. This function receives both the name and the current quantizer at the given location. This can be an initialized quantizer or a QuantizerStub.</p> <p>\u23e9 Applying the QuantizationConfig to the model is very simple:</p> In\u00a0[21]: Copied! <pre>config.initialize(quantized_model)\nquantized_model\n</pre> config.initialize(quantized_model) quantized_model Out[21]: <pre>QuantizedSequential(\n  (0): QuantizedLinear(\n    in_features=4, out_features=3, bias=True\n    (input_quantizer): LinearQuantizer(num_bits=8, symmetric=False, granularity=PerTensor())\n    (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n    (bias_quantizer): QuantizerStub()\n    (output_quantizer): LinearQuantizer(num_bits=8, symmetric=False, granularity=PerTensor())\n  )\n  (1): QuantizedRelu(\n    (input_quantizer): QuantizerStub()\n    (output_quantizer): LinearQuantizer(num_bits=8, symmetric=False, granularity=PerTensor())\n  )\n  (2): QuantizedLinear(\n    in_features=3, out_features=3, bias=True\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n    (bias_quantizer): QuantizerStub()\n    (output_quantizer): LinearQuantizer(num_bits=8, symmetric=False, granularity=PerTensor())\n  )\n  (3): QuantizedRelu(\n    (input_quantizer): QuantizerStub()\n    (output_quantizer): LinearQuantizer(num_bits=8, symmetric=False, granularity=PerTensor())\n  )\n  (4): QuantizedLinear(\n    in_features=3, out_features=8, bias=True\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n    (bias_quantizer): QuantizerStub()\n    (output_quantizer): LinearQuantizer(num_bits=8, symmetric=False, granularity=PerTensor())\n  )\n  (5): QuantizedRelu(\n    (input_quantizer): QuantizerStub()\n    (output_quantizer): LinearQuantizer(num_bits=8, symmetric=False, granularity=PerTensor())\n  )\n)</pre> <p>\u2705 Observe that the quantizers in our quantized model are now setup up as expected.</p> <p>\u23e9 All we have to do now is estimate the ranges for the quantizers, and we can use the quantized model!</p> In\u00a0[22]: Copied! <pre>with ff.range_setting.estimate_ranges(quantized_model, ff.range_setting.smoothed_minmax):\n    quantized_model(data)\n\nquantized_model(data)\n</pre> with ff.range_setting.estimate_ranges(quantized_model, ff.range_setting.smoothed_minmax):     quantized_model(data)  quantized_model(data) Out[22]: <pre>QuantizedTensor([[ -83., -128.,  110., -128.,  127., -128., -128.,  -75.]],\n                grad_fn=&lt;AliasBackward0&gt;,\n                quantizer=TiledAffineQuantizationFunction, scale=tensor([0.0017], grad_fn=&lt;TiledAffineQuantizationFunctionFunctionBackward&gt;), tile_size=data_shape, num_bits=8, output_dtype=torch.float32, offset=tensor([128.], grad_fn=&lt;TiledAffineQuantizationFunctionFunctionBackward&gt;))</pre> In\u00a0[23]: Copied! <pre>class MySelfAttentionLayer(torch.nn.Module):\n    def __init__(self, feature_size):\n        print(\"Calling MySelfAttentionLayer.__init__\")\n        super().__init__()\n        self.feature_size = feature_size\n\n        # Linear transformations for Q, K, V from the same source\n        self.key = torch.nn.Linear(feature_size, feature_size)\n        self.query = torch.nn.Linear(feature_size, feature_size)\n        self.value = torch.nn.Linear(feature_size, feature_size)\n\n    def forward(self, x):\n        print(\"Calling MySelfAttentionLayer.forward\")\n        # Apply linear transformations\n        keys = self.key(x)\n        queries = self.query(x)\n        values = self.value(x)\n\n        # Scaled dot-product attention\n        scores = torch.matmul(queries, keys.transpose(-2, -1))\n        scores = scores / torch.sqrt(torch.tensor(self.feature_size, dtype=torch.float32))\n\n        # Apply softmax\n        attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n\n        # Multiply weights with values\n        output = torch.matmul(attention_weights, values)\n\n        return output, attention_weights\n</pre> class MySelfAttentionLayer(torch.nn.Module):     def __init__(self, feature_size):         print(\"Calling MySelfAttentionLayer.__init__\")         super().__init__()         self.feature_size = feature_size          # Linear transformations for Q, K, V from the same source         self.key = torch.nn.Linear(feature_size, feature_size)         self.query = torch.nn.Linear(feature_size, feature_size)         self.value = torch.nn.Linear(feature_size, feature_size)      def forward(self, x):         print(\"Calling MySelfAttentionLayer.forward\")         # Apply linear transformations         keys = self.key(x)         queries = self.query(x)         values = self.value(x)          # Scaled dot-product attention         scores = torch.matmul(queries, keys.transpose(-2, -1))         scores = scores / torch.sqrt(torch.tensor(self.feature_size, dtype=torch.float32))          # Apply softmax         attention_weights = torch.nn.functional.softmax(scores, dim=-1)          # Multiply weights with values         output = torch.matmul(attention_weights, values)          return output, attention_weights In\u00a0[24]: Copied! <pre>num_features = 8\n\nmy_unquantized_layer = MySelfAttentionLayer(num_features)\n\nmy_unquantized_layer\n</pre> num_features = 8  my_unquantized_layer = MySelfAttentionLayer(num_features)  my_unquantized_layer <pre>Calling MySelfAttentionLayer.__init__\n</pre> Out[24]: <pre>MySelfAttentionLayer(\n  (key): Linear(in_features=8, out_features=8, bias=True)\n  (query): Linear(in_features=8, out_features=8, bias=True)\n  (value): Linear(in_features=8, out_features=8, bias=True)\n)</pre> In\u00a0[25]: Copied! <pre>my_quantized_layer = copy.deepcopy(my_unquantized_layer)\ntry:\n    ff.quantize_model(my_quantized_layer)\nexcept ff.exceptions.QuantizationError as e:\n    print(\"[ERROR]\", e, \"\\n\")\n\nprint(\"ff.quantized_module_map():\")\npprint(ff.quantized_module_map())\n</pre> my_quantized_layer = copy.deepcopy(my_unquantized_layer) try:     ff.quantize_model(my_quantized_layer) except ff.exceptions.QuantizationError as e:     print(\"[ERROR]\", e, \"\\n\")  print(\"ff.quantized_module_map():\") pprint(ff.quantized_module_map()) <pre>[ERROR] Cannot quantize model because no quantized version of the following modules is known:\n  - __main__.MySelfAttentionLayer\nIt is possible that quantized definitions of one or more of these models\nexists, but have not been imported.\" \n\nff.quantized_module_map():\n{&lt;class 'torch.nn.modules.conv.Conv1d'&gt;: &lt;class 'fastforward.nn.conv.QuantizedConv1d'&gt;,\n &lt;class 'torch.nn.modules.conv.Conv2d'&gt;: &lt;class 'fastforward.nn.conv.QuantizedConv2d'&gt;,\n &lt;class 'torch.nn.modules.linear.Linear'&gt;: &lt;class 'fastforward.nn.linear.QuantizedLinear'&gt;,\n &lt;class 'torch.nn.modules.activation.ReLU'&gt;: &lt;class 'fastforward.nn.activations.QuantizedRelu'&gt;,\n &lt;class 'torch.nn.modules.activation.SiLU'&gt;: &lt;class 'fastforward.nn.activations.QuantizedSilu'&gt;,\n &lt;class 'torch.nn.modules.container.Sequential'&gt;: &lt;class 'fastforward.nn.container.QuantizedSequential'&gt;,\n &lt;class 'torch.nn.modules.container.ModuleList'&gt;: &lt;class 'fastforward.nn.container.QuantizedModuleList'&gt;,\n &lt;class 'torch.nn.modules.container.ModuleDict'&gt;: &lt;class 'fastforward.nn.container.QuantizedModuleDict'&gt;,\n &lt;class 'torch.nn.modules.container.ParameterList'&gt;: &lt;class 'fastforward.nn.container.QuantizedParameterList'&gt;,\n &lt;class 'torch.nn.modules.container.ParameterDict'&gt;: &lt;class 'fastforward.nn.container.QuantizedParameterDict'&gt;,\n &lt;class 'torch.nn.modules.normalization.LayerNorm'&gt;: &lt;class 'fastforward.nn.normalization.QuantizedLayerNorm'&gt;,\n &lt;class 'torch.nn.modules.sparse.Embedding'&gt;: &lt;class 'fastforward.nn.embedding.QuantizedEmbedding'&gt;}\n</pre> <p>\u274c Observe that <code>ff.quantize_model</code> does not work, because it does not know which class <code>MySelfAttentionLayer</code> should be mapped to.</p> <p>\u23e9 For now, we have to manually define the quantized equivalent of <code>MySelfAttentionLayer</code>, we will show how to do so in the next section:</p> In\u00a0[26]: Copied! <pre>class MyQuantizedSelfAttentionLayer(MySelfAttentionLayer, ff.nn.quantized_module.QuantizedModule):\n    def __init_quantization__(self) -&gt; None:\n        print(\"Calling MyQuantizedSelfAttentionLayer.__init_quantization__\")\n        super().__init_quantization__()\n\n        self.attention_scores_output_quantizer = ff.nn.QuantizerStub(output_quantizer=True)\n        self.attention_weights_output_quantizer = ff.nn.QuantizerStub(output_quantizer=True)\n        self.attention_features_output_quantizer = ff.nn.QuantizerStub(output_quantizer=True)\n\n    # This function is only wrapped for demonstration purposes\n    def quantize_children(self, *args, **kwargs) -&gt; None:\n        print(\"Calling MyQuantizedSelfAttentionLayer.quantize_children\")\n        super().quantize_children(*args, **kwargs)\n\n    def forward(self, x):\n        print(\"Calling MyQuantizedSelfAttentionLayer.forward\")\n        # Apply linear transformations\n        keys = self.key(x)\n        queries = self.query(x)\n        values = self.value(x)\n\n        # Scaled dot-product attention\n        scores = ff.nn.functional.matmul(\n            queries,\n            keys.transpose(-2, -1),\n            output_quantizer=self.attention_scores_output_quantizer,\n        )\n        scores = scores / torch.sqrt(torch.tensor(self.feature_size, dtype=torch.float32))\n\n        # Apply softmax\n        attention_weights = ff.nn.functional.softmax(\n            scores, dim=-1, output_quantizer=self.attention_weights_output_quantizer\n        )\n\n        # Multiply weights with values\n        output = ff.nn.functional.matmul(\n            attention_weights,\n            values,\n            output_quantizer=self.attention_features_output_quantizer,\n        )\n\n        return output, attention_weights\n</pre> class MyQuantizedSelfAttentionLayer(MySelfAttentionLayer, ff.nn.quantized_module.QuantizedModule):     def __init_quantization__(self) -&gt; None:         print(\"Calling MyQuantizedSelfAttentionLayer.__init_quantization__\")         super().__init_quantization__()          self.attention_scores_output_quantizer = ff.nn.QuantizerStub(output_quantizer=True)         self.attention_weights_output_quantizer = ff.nn.QuantizerStub(output_quantizer=True)         self.attention_features_output_quantizer = ff.nn.QuantizerStub(output_quantizer=True)      # This function is only wrapped for demonstration purposes     def quantize_children(self, *args, **kwargs) -&gt; None:         print(\"Calling MyQuantizedSelfAttentionLayer.quantize_children\")         super().quantize_children(*args, **kwargs)      def forward(self, x):         print(\"Calling MyQuantizedSelfAttentionLayer.forward\")         # Apply linear transformations         keys = self.key(x)         queries = self.query(x)         values = self.value(x)          # Scaled dot-product attention         scores = ff.nn.functional.matmul(             queries,             keys.transpose(-2, -1),             output_quantizer=self.attention_scores_output_quantizer,         )         scores = scores / torch.sqrt(torch.tensor(self.feature_size, dtype=torch.float32))          # Apply softmax         attention_weights = ff.nn.functional.softmax(             scores, dim=-1, output_quantizer=self.attention_weights_output_quantizer         )          # Multiply weights with values         output = ff.nn.functional.matmul(             attention_weights,             values,             output_quantizer=self.attention_features_output_quantizer,         )          return output, attention_weights <p>\u23e9 Notice that we made two changes to the model:</p> <ol> <li><p>We have re-implemented the forward pass, replacing all operations from torch.nn.functional with their FastForward quantized equivalent.</p> <ol> <li>\u274c Untill autoquant is implemented in FastForward, this means we manually need to duplicate the code from the forward pass.</li> <li>\u26a0\ufe0f NOTE: Some of the functionals might be hidden inside a function that is called in your forward pass, make sure to also rewrite those cases.</li> <li>\u26a0\ufe0f If you are adopting a 3rd party class, you will need to copy-paste the code from the forward pass. Make sure to also freeze the dependency so that your rewritten module will not diverge once the package is updated!</li> <li>In order to use the quantized functionals, we have added Quantizers to the model:</li> </ol> </li> <li><p>We added an <code>__init_quantization__</code> method that adds the <code>QuantizerStubs</code> which could be used later for quantization.</p> <ol> <li>\u2705 We do not have to copy-paste any code from the <code>__init__</code> function</li> <li>\u2705 As we will see below, <code>__init_quantization__</code> can be used both for initializing a <code>QuantizedModule</code> from scratch, or to convert a <code>Module</code> to a <code>QuantizedModule</code></li> </ol> </li> </ol> <p>\u23e9 Let's have a look to see how our <code>MyQuantizedSelfAttentionLayer</code> behaves when initialized from scratch:</p> In\u00a0[27]: Copied! <pre>new_quantized_layer = MyQuantizedSelfAttentionLayer(num_features)\nnew_quantized_layer\n</pre> new_quantized_layer = MyQuantizedSelfAttentionLayer(num_features) new_quantized_layer <pre>Calling MySelfAttentionLayer.__init__\nCalling MyQuantizedSelfAttentionLayer.__init_quantization__\n</pre> Out[27]: <pre>MyQuantizedSelfAttentionLayer(\n  (key): Linear(in_features=8, out_features=8, bias=True)\n  (query): Linear(in_features=8, out_features=8, bias=True)\n  (value): Linear(in_features=8, out_features=8, bias=True)\n  (attention_scores_output_quantizer): QuantizerStub()\n  (attention_weights_output_quantizer): QuantizerStub()\n  (attention_features_output_quantizer): QuantizerStub()\n)</pre> <p>\u23e9 Observe that:</p> <ol> <li><code>MySelfAttentionLayer.__init__</code> is first called, initializing the layer using the logic of the unquantized base layer.</li> <li><code>MyQuantizedSelfAttentionLayer.__init_quantization__</code> is then called, inserting the quantizer stubs.</li> <li>The children modules are not converted to their quantized counterparts when initializing from scratch.</li> </ol> <p>\u23e9 In practice, we will typically not initialize quantized modules from scratch, but we will rather take a floating point model and recursively convert all it's submodules.</p> <p>\u23e9 We will now look how <code>MyQuantizedSelfAttentionLayer</code> behaves when converted using the <code>quantize_model</code> function. First, let's look at the <code>quantized_module_map</code>:</p> In\u00a0[28]: Copied! <pre>print(\"ff.quantized_module_map():\")\npprint(ff.quantized_module_map()[MySelfAttentionLayer])\n</pre> print(\"ff.quantized_module_map():\") pprint(ff.quantized_module_map()[MySelfAttentionLayer]) <pre>ff.quantized_module_map():\n&lt;class '__main__.MyQuantizedSelfAttentionLayer'&gt;\n</pre> <p>\u2705 Note that <code>MySelfAttentionLayer</code> automatically appeared in the <code>quantized_module_map</code>!</p> <p>\u26a0\ufe0f All subclasses of <code>QuantizedModel</code> are automatically found in <code>fastforward.nn.quantized_module.quantized_module_map()</code>, but this requires the classes to be imported. If your class does not show up, make sure to import it, or use the <code>extra_conversion</code> argument if you want to override any mappings in the <code>quantized_module_map</code>.</p> <p>\u23e9 We will now look how <code>MyQuantizedSelfAttentionLayer</code> behaves when converted using the <code>quantize_model</code> function.</p> In\u00a0[29]: Copied! <pre>my_quantized_layer = copy.deepcopy(my_unquantized_layer)\nff.quantize_model(my_quantized_layer)\n\nmy_quantized_layer\n</pre> my_quantized_layer = copy.deepcopy(my_unquantized_layer) ff.quantize_model(my_quantized_layer)  my_quantized_layer <pre>Calling MyQuantizedSelfAttentionLayer.__init_quantization__\nCalling MyQuantizedSelfAttentionLayer.quantize_children\n</pre> Out[29]: <pre>MyQuantizedSelfAttentionLayer(\n  (key): QuantizedLinear(\n    in_features=8, out_features=8, bias=True\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): QuantizerStub()\n    (bias_quantizer): QuantizerStub()\n    (output_quantizer): QuantizerStub()\n  )\n  (query): QuantizedLinear(\n    in_features=8, out_features=8, bias=True\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): QuantizerStub()\n    (bias_quantizer): QuantizerStub()\n    (output_quantizer): QuantizerStub()\n  )\n  (value): QuantizedLinear(\n    in_features=8, out_features=8, bias=True\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): QuantizerStub()\n    (bias_quantizer): QuantizerStub()\n    (output_quantizer): QuantizerStub()\n  )\n  (attention_scores_output_quantizer): QuantizerStub()\n  (attention_weights_output_quantizer): QuantizerStub()\n  (attention_features_output_quantizer): QuantizerStub()\n)</pre> <p>\u23e9 Observe that:</p> <ol> <li>Since we convert an existing layer, <code>MySelfAttentionLayer.__init__</code> is not called again.</li> <li>The class of our module is changed from <code>MySelfAttentionLayer</code> to <code>MyQuantizedSelfAttentionLayer</code>.</li> <li><code>MyQuantizedSelfAttentionLayer.__init_quantization__</code> is still called, inserting the quantizer stubs into the previously unquantized layer.</li> <li>The children modules are also converted to their quantized counterparts by calling <code>MyQuantizedSelfAttentionLayer.quantize_children</code>.</li> </ol> <p>Copyright (c) 2024 Qualcomm Technologies, Inc. All Rights Reserved.</p>"},{"location":"examples/quantizing_networks.nb/#overview","title":"Overview\u00b6","text":"<p>In this notebook, we will go over the most important objects and classes in <code>fastforward</code>. At the end of the notebook, we will have covered how to quantize simple modules like a multi-layer perceptron as well as the large language model OPT. This is a great start if you want to familiarize yourself with fastforward.</p> <p>The notebook consists of five sections:</p> <ol> <li>Quantized Tensors: <code>QuantizedTensors</code>, a subclass of <code>torch.Tensors</code> which are the fundamental datatype in FastForward.</li> <li>Quantizers: <code>Quantizers</code> are <code>torch.nn.Modules</code> that turn floating point tensors into <code>QuantizedTensors</code> and can learn from data.</li> <li>Quantized Modules: Quantizing a module consists of three steps: 1) Changing the module to a <code>QuantizedModule</code>, 2. inserting desired quantizers, and 3. estimating the ranges for each quantizers.</li> <li>Quantized Models: How to automate the first steps described above using 1) the <code>quantize_model</code> function and 2) the <code>QuantizationConfig</code>. This section also shows how to manually quantize custom and 3rd party modules.</li> <li>Quantizing 3rd Party models: We show how we applied all of the above to quantize the huggingface OPT model.</li> </ol>"},{"location":"examples/quantizing_networks.nb/#1-quantized-tensors","title":"1. Quantized Tensors\u00b6","text":"<p>We start our tutorial with the <code>ff.quantized_tensor.QuantizedTensor</code>. This datatype is a subclass of a <code>torch.Tensor</code> designed to hold quantized data. A <code>QuantizedTensor</code> can be quantized using any type of quantization (uniform quantization, dynamic quantization, vector quantization, ...) but we will focus on linear / uniform quantization.</p> <p>There are many kinds of quantization, but in this notebook we focus on integer quantization on a fixed per-tensor or per-channel grid.</p> <p>It is not required that you know the details of this (very common) quantization scheme, but if you want to know more please refer to A White Paper on Neural Network Quantization (Nagel et al. 2021)</p>"},{"location":"examples/quantizing_networks.nb/#2-quantizers","title":"2. Quantizers\u00b6","text":"<p>In the previous paragraph, we saw above how a floating point tensor can be quantized.</p> <p>\u274c Quantization often involves hyperparameters which we do not know in advance.</p> <p>\u2705 For this purpose, we can use <code>ff.nn.Quantizers</code>. These are <code>nn.Modules</code> that can quantize a data in their forward pass and can also be used to estimate or learn the quantization hyperparameters.</p> <p>\u23e9 We create a <code>LinearQuantizer</code> now</p>"},{"location":"examples/quantizing_networks.nb/#3-quantized-modules","title":"3. Quantized Modules\u00b6","text":"<p><code>Quantizers</code> typically don't exist in isolation. Instead, we would often like to quantize an entire model. In this section we show how to turn a module into a quantized module and what is happening under the hood. In the next section we show how to use convience methods for easier quantization of bigger models.</p> <p>\u23e9 We start with a simple unquantized linear layer</p>"},{"location":"examples/quantizing_networks.nb/#4-quantized-models","title":"4. Quantized Models\u00b6","text":"<p>In the previous section we showed how to quantize a module:</p> <ol> <li>Turn an unquantized module into an unquantized module</li> <li>Replace the desired QuantizerStubs with the desired Quantizers</li> <li>Estimate the quantizer ranges by passing data trough the model.</li> </ol> <p>Performing step 1. and 2. were quite laborious in our above example. Since we have to repeat these steps for every layer in the model, we have created helper tools to automatate these tasks. In the next section we will show how to use <code>autoquant</code> tool to automatically replace all layers with their Quantized counterparts (step 1.) and how to use the <code>QuantizationConfig</code> to automatically insert quantizers into the model (step 2.).</p>"},{"location":"examples/quantizing_networks.nb/#41-automatically-replace-torchnnmodules-with-their-ffnnquantizedmodule-counterparts","title":"4.1 Automatically replace <code>torch.nn.Modules</code> with their <code>ff.nn.QuantizedModule</code> counterparts\u00b6","text":"<p>\u23e9 The <code>quantize_model</code> function can change a model in-place, recursively replacing all modules with their <code>QuantizedModule</code> counterparts.</p> <ul> <li>The way this function works is pretty simple. It just uses a dict that maps <code>torch.nn.Module</code> types to <code>ff.nn.QuantizedModule</code> types.</li> </ul> <p>\u23e9 Let's have a look at this dictionary</p>"},{"location":"examples/quantizing_networks.nb/#42-automatically-inserting-ffnnquantizers-in-the-right-place-in-the-model","title":"4.2 Automatically inserting <code>ff.nn.Quantizer</code>s in the right place in the model.\u00b6","text":"<p>The <code>ff.QuantizationConfig</code> is a tool to automatically replace <code>QuantizerStubs</code> with <code>Quantizers</code>. It works by adding quantization rules.</p> <p>A quantization rule consists of two components:</p> <ol> <li>A <code>query</code> determines to which layers the rule should be applied. Filtering is done using the <code>ff.mpath</code> library. Please see the tutorial on MPath for more information.</li> <li>A quantizer class or factory. This determines how the quantizer is created. In the case of a Quantizer class, for each match a quantizer of that class is initialized using the provided kwargs. In the case of a factory function, the function receives the full name of the quantizer and the current quantizer at that location. This function is expected to return an intiailized quantizer.</li> </ol> <p>If multiple rules match a single quantizer, the rule that was added last takes priority.</p> <p>\u23e9 We create our <code>QuantizationConfig</code> below, see if you can understand all the rules!</p>"},{"location":"examples/quantizing_networks.nb/#43-quantizing-custom-modules-manual-quantization","title":"4.3 Quantizing Custom Modules: Manual Quantization\u00b6","text":"<p>Your model might not only consist of <code>torch.nn.Modules</code>, but also contain 3rd party or custom modules. Because FastForward does not have fully automated quantization yet, trying to convert these modules using <code>quantize_model</code> does not work. Let us build such a custom module:</p>"},{"location":"examples/quantizing_networks.nb/#5-quantizing-3rd-party-models-huggingface-opt","title":"5. Quantizing 3rd party models (Huggingface OPT)\u00b6","text":"<p>Based on the tutorial above you should be able to manually quantize any model. We will now show how we quantized the OPT model in our fast-models benchmark repository</p> <p>The process of adopting the model consists of the following steps (which are explained in the notebook above):</p> <ol> <li><p>Downloading the existing model code from the huggingface library.</p> <ol> <li>\u26a0\ufe0f Because we will both be using huggingface as a library, but also copy-paste huggingface code we need to freeze our huggingface dependency so that it matches the version we copied the code from.</li> <li>\u23e9 Have a look at this commit to see how we conducted this step.</li> </ol> </li> <li><p>Cleaning the existing model code</p> <ol> <li>We remove everything except the modules that are used in the (OPT) model we aim to quantize.</li> <li>Of those modules, we only keep the forward pass.</li> <li>\u23e9 Have a look at this commit to see how we conducted this step.</li> </ol> </li> <li><p>Modifying the existing model code</p> <ol> <li>We change all the functionals in the forward pass to their quantized counterparts.</li> </ol> <p>\u26a0\ufe0f NOTE: Sometimes the functionals might be hidden inside a function (such as the <code>quantized_masked_attention</code> function in the OPT example), take care to also detect and convert those.</p> <ol> <li>We add an <code>__init_quantization__</code> method that adds the required quantizers which are used in the quantized functionals.</li> <li>\u23e9 Have a look at this commit to see how we conducted this step.</li> </ol> </li> <li><p>Adding code to insert the QuantizerStubs by adding a <code>QuantizationConfig</code></p> <ol> <li>We make a <code>QuantizationConfig</code> that determines where to insert quantizers based on our experiment settings.</li> <li>\u23e9 Have a look at this commit to see how we conducted this step.</li> </ol> </li> <li><p>Running the full benchmark experiments</p> <ol> <li>\u23e9 Have a look at this commit to see how we conducted this step.</li> </ol> </li> </ol>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/","title":"Quick Start: Quantization of Llama","text":"In\u00a0[1]: Copied! <pre>import os\n\nimport torch\n\nfrom datasets import load_dataset\nfrom quick_start_utils import tokenize_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoTokenizer, LlamaForCausalLM, default_data_collator\n\nimport fastforward as ff\n\nmodel_dtype = torch.float16\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsequence_length = 1024\nbatch_size = 1\n\nvalid_percent = 20\ntrain_percent = 5\n\nmodel_name_or_path = os.environ.get(\"FF_QUICKSTART_MODEL\", \"meta-llama/Llama-3.2-1B-Instruct\")\n\n# Load Model\nfrom_tf = bool(\".ckpt\" in model_name_or_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    model_name_or_path, from_tf=from_tf, torch_dtype=model_dtype\n)\nmodel.to(device)\n\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, legacy=False, use_fast=True)\n\n# Load Dataset\n_valid_split = \"validation\" if valid_percent is None else f\"validation[:{valid_percent}%]\"\n_train_split = \"train\" if train_percent is None else f\"train[:{train_percent}%]\"\nvalidset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=_valid_split)\ntrainset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=_train_split)\n\n# Tokenize Dataset\ntokenized_validset = tokenize_dataset(validset, tokenizer, sequence_length)\ntokenized_trainset = tokenize_dataset(trainset, tokenizer, sequence_length)\n\n# Create Dataloader\nvalid_loader = DataLoader(\n    tokenized_validset, batch_size, collate_fn=default_data_collator, shuffle=False\n)\ntrain_loader = DataLoader(\n    tokenized_trainset, batch_size, collate_fn=default_data_collator, shuffle=True\n)\n</pre> import os  import torch  from datasets import load_dataset from quick_start_utils import tokenize_dataset from torch.utils.data import DataLoader from tqdm.notebook import tqdm from transformers import AutoTokenizer, LlamaForCausalLM, default_data_collator  import fastforward as ff  model_dtype = torch.float16 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") sequence_length = 1024 batch_size = 1  valid_percent = 20 train_percent = 5  model_name_or_path = os.environ.get(\"FF_QUICKSTART_MODEL\", \"meta-llama/Llama-3.2-1B-Instruct\")  # Load Model from_tf = bool(\".ckpt\" in model_name_or_path) model = LlamaForCausalLM.from_pretrained(     model_name_or_path, from_tf=from_tf, torch_dtype=model_dtype ) model.to(device)  # Load Tokenizer tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, legacy=False, use_fast=True)  # Load Dataset _valid_split = \"validation\" if valid_percent is None else f\"validation[:{valid_percent}%]\" _train_split = \"train\" if train_percent is None else f\"train[:{train_percent}%]\" validset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=_valid_split) trainset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=_train_split)  # Tokenize Dataset tokenized_validset = tokenize_dataset(validset, tokenizer, sequence_length) tokenized_trainset = tokenize_dataset(trainset, tokenizer, sequence_length)  # Create Dataloader valid_loader = DataLoader(     tokenized_validset, batch_size, collate_fn=default_data_collator, shuffle=False ) train_loader = DataLoader(     tokenized_trainset, batch_size, collate_fn=default_data_collator, shuffle=True )  In\u00a0[2]: Copied! <pre>def prepare_batch(batch: dict, device: torch.device):\n    return {\n        \"input_ids\": batch[\"input_ids\"].to(device),\n        \"attention_mask\": batch[\"attention_mask\"].to(device),\n        \"labels\": batch[\"labels\"].to(torch.long).to(device),\n    }\n\n\ndef evaluate_model(model, valid_loader, device, max_num_batches=None):\n    model.eval()\n    losses = []\n    for batch_idx, batch in enumerate(tqdm(valid_loader)):\n        if max_num_batches is not None and batch_idx &gt;= max_num_batches:\n            break\n\n        batch = prepare_batch(batch, device)\n\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        losses.append(outputs.loss)\n\n    eval_loss = torch.stack(losses).mean()\n    perplexity = torch.exp(eval_loss)\n    return float(perplexity)\n\n\norig_perplexity = evaluate_model(model, valid_loader, device=device, max_num_batches=None)\nprint(\n    f\"Perplexity over wikitext-validation using full-precision model ({model_dtype}): {orig_perplexity:.4}\"\n)\n</pre> def prepare_batch(batch: dict, device: torch.device):     return {         \"input_ids\": batch[\"input_ids\"].to(device),         \"attention_mask\": batch[\"attention_mask\"].to(device),         \"labels\": batch[\"labels\"].to(torch.long).to(device),     }   def evaluate_model(model, valid_loader, device, max_num_batches=None):     model.eval()     losses = []     for batch_idx, batch in enumerate(tqdm(valid_loader)):         if max_num_batches is not None and batch_idx &gt;= max_num_batches:             break          batch = prepare_batch(batch, device)          with torch.no_grad():             outputs = model(**batch)          losses.append(outputs.loss)      eval_loss = torch.stack(losses).mean()     perplexity = torch.exp(eval_loss)     return float(perplexity)   orig_perplexity = evaluate_model(model, valid_loader, device=device, max_num_batches=None) print(     f\"Perplexity over wikitext-validation using full-precision model ({model_dtype}): {orig_perplexity:.4}\" )  <pre>Perplexity over wikitext-validation using full-precision model (torch.float16): 14.09\n</pre> In\u00a0[3]: Copied! <pre># Import all the custom QuantizedModules required to quantize our llama model.\n# We just need to import those modules so that `ff.quantize_model` will be able to find them.\nfrom quantized_models import quantized_llama as quantized_llama  # noqa: E402\n\n# Convert the model into a QuantizedModel  (inplace operation)\nff.quantize_model(model)\n</pre>  # Import all the custom QuantizedModules required to quantize our llama model. # We just need to import those modules so that `ff.quantize_model` will be able to find them. from quantized_models import quantized_llama as quantized_llama  # noqa: E402  # Convert the model into a QuantizedModel  (inplace operation) ff.quantize_model(model)  Out[3]: <pre>QuantizedLlamaForCausalLM(\n  (model): QuantizedLlamaModel(\n    (embed_tokens): QuantizedEmbedding(\n      128256, 2048\n      (weight_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n    (layers): QuantizedModuleList(\n      (0-15): 16 x QuantizedLlamaDecoderLayer(\n        (self_attn): QuantizedLlamaSdpaAttention(\n          (q_proj): QuantizedLinear(\n            in_features=2048, out_features=2048, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): QuantizerStub()\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (k_proj): QuantizedLinear(\n            in_features=2048, out_features=512, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): QuantizerStub()\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (v_proj): QuantizedLinear(\n            in_features=2048, out_features=512, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): QuantizerStub()\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (o_proj): QuantizedLinear(\n            in_features=2048, out_features=2048, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): QuantizerStub()\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (rotary_emb): QuantizedLlamaRotaryEmbedding()\n          (input_quantizer): QuantizerStub()\n          (rope_q_quantizer): QuantizerStub()\n          (rope_k_quantizer): QuantizerStub()\n          (sdpa_matmul_quantizer): QuantizerStub()\n          (sdpa_softmax_quantizer): QuantizerStub()\n          (sdpa_dropout_quantizer): QuantizerStub()\n          (sdpa_output_quantizer): QuantizerStub()\n        )\n        (mlp): QuantizedLlamaMLP(\n          (gate_proj): QuantizedLinear(\n            in_features=2048, out_features=8192, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): QuantizerStub()\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (up_proj): QuantizedLinear(\n            in_features=2048, out_features=8192, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): QuantizerStub()\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (down_proj): QuantizedLinear(\n            in_features=8192, out_features=2048, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): QuantizerStub()\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (act_fn): QuantizedSilu(\n            (input_quantizer): QuantizerStub()\n            (output_quantizer): QuantizerStub()\n          )\n          (input_quantizer): QuantizerStub()\n          (gate_act_quantizer): QuantizerStub()\n          (gated_up_proj_output_quantizer): QuantizerStub()\n        )\n        (input_layernorm): QuantizedLlamaRMSNorm(\n          (2048,), eps=1e-05\n          (input_quantizer): QuantizerStub()\n          (output_quantizer): QuantizerStub()\n          (weight_quantizer): QuantizerStub()\n        )\n        (post_attention_layernorm): QuantizedLlamaRMSNorm(\n          (2048,), eps=1e-05\n          (input_quantizer): QuantizerStub()\n          (output_quantizer): QuantizerStub()\n          (weight_quantizer): QuantizerStub()\n        )\n        (input_quantizer): QuantizerStub()\n        (attn_res_act_quantizer): QuantizerStub()\n        (mlp_res_act_quantizer): QuantizerStub()\n      )\n    )\n    (norm): QuantizedLlamaRMSNorm(\n      (2048,), eps=1e-05\n      (input_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n    )\n    (rotary_emb): QuantizedLlamaRotaryEmbedding()\n  )\n  (lm_head): QuantizedLinear(\n    in_features=2048, out_features=128256, bias=False\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): QuantizerStub()\n    (bias_quantizer): None\n    (output_quantizer): QuantizerStub()\n  )\n)</pre> <p>By default, FastForward operates in \"strict quantization\" mode. In this mode, many quantization errors, such as calling a quantized function without quantized tensors, are treated as errors. This is beneficial for quantizing models that require strict adherence to quantization rules. However, strict quantization is not always necessary. In this tutorial, we only partially quantize a model, so we disable strict quantization.</p> In\u00a0[4]: Copied! <pre>ff.set_strict_quantization(False)\n</pre> ff.set_strict_quantization(False) Out[4]: <pre>&lt;fastforward.flags.strict_quantization_ResetToTrue at 0x7fc3e80aace0&gt;</pre> In\u00a0[5]: Copied! <pre>w_bits = 8\n</pre> w_bits = 8 In\u00a0[6]: Copied! <pre># Set Weight Quantization\nw_quantizers = ff.find_quantizers(model, \"**/layers/*/self_attn/*/[quantizer:parameter/weight]\")\nw_quantizers |= ff.find_quantizers(model, \"**/layers/*/mlp/*/[quantizer:parameter/weight]\")\nw_quantizers.initialize(ff.nn.LinearQuantizer, num_bits=w_bits, granularity=ff.PerChannel())\nprint(f\"Found {len(w_quantizers)} weight quantizers.\")\n</pre> # Set Weight Quantization w_quantizers = ff.find_quantizers(model, \"**/layers/*/self_attn/*/[quantizer:parameter/weight]\") w_quantizers |= ff.find_quantizers(model, \"**/layers/*/mlp/*/[quantizer:parameter/weight]\") w_quantizers.initialize(ff.nn.LinearQuantizer, num_bits=w_bits, granularity=ff.PerChannel()) print(f\"Found {len(w_quantizers)} weight quantizers.\") <pre>Found 112 weight quantizers.\n</pre> In\u00a0[7]: Copied! <pre># Move model to target device: quantizer parameters (scale/offsets) should be placed on the target device too.\nmodel.to(device)\n</pre> # Move model to target device: quantizer parameters (scale/offsets) should be placed on the target device too. model.to(device) Out[7]: <pre>QuantizedLlamaForCausalLM(\n  (model): QuantizedLlamaModel(\n    (embed_tokens): QuantizedEmbedding(\n      128256, 2048\n      (weight_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n    (layers): QuantizedModuleList(\n      (0-15): 16 x QuantizedLlamaDecoderLayer(\n        (self_attn): QuantizedLlamaSdpaAttention(\n          (q_proj): QuantizedLinear(\n            in_features=2048, out_features=2048, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (k_proj): QuantizedLinear(\n            in_features=2048, out_features=512, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (v_proj): QuantizedLinear(\n            in_features=2048, out_features=512, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (o_proj): QuantizedLinear(\n            in_features=2048, out_features=2048, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (rotary_emb): QuantizedLlamaRotaryEmbedding()\n          (input_quantizer): QuantizerStub()\n          (rope_q_quantizer): QuantizerStub()\n          (rope_k_quantizer): QuantizerStub()\n          (sdpa_matmul_quantizer): QuantizerStub()\n          (sdpa_softmax_quantizer): QuantizerStub()\n          (sdpa_dropout_quantizer): QuantizerStub()\n          (sdpa_output_quantizer): QuantizerStub()\n        )\n        (mlp): QuantizedLlamaMLP(\n          (gate_proj): QuantizedLinear(\n            in_features=2048, out_features=8192, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (up_proj): QuantizedLinear(\n            in_features=2048, out_features=8192, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (down_proj): QuantizedLinear(\n            in_features=8192, out_features=2048, bias=False\n            (input_quantizer): QuantizerStub()\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (act_fn): QuantizedSilu(\n            (input_quantizer): QuantizerStub()\n            (output_quantizer): QuantizerStub()\n          )\n          (input_quantizer): QuantizerStub()\n          (gate_act_quantizer): QuantizerStub()\n          (gated_up_proj_output_quantizer): QuantizerStub()\n        )\n        (input_layernorm): QuantizedLlamaRMSNorm(\n          (2048,), eps=1e-05\n          (input_quantizer): QuantizerStub()\n          (output_quantizer): QuantizerStub()\n          (weight_quantizer): QuantizerStub()\n        )\n        (post_attention_layernorm): QuantizedLlamaRMSNorm(\n          (2048,), eps=1e-05\n          (input_quantizer): QuantizerStub()\n          (output_quantizer): QuantizerStub()\n          (weight_quantizer): QuantizerStub()\n        )\n        (input_quantizer): QuantizerStub()\n        (attn_res_act_quantizer): QuantizerStub()\n        (mlp_res_act_quantizer): QuantizerStub()\n      )\n    )\n    (norm): QuantizedLlamaRMSNorm(\n      (2048,), eps=1e-05\n      (input_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n    )\n    (rotary_emb): QuantizedLlamaRotaryEmbedding()\n  )\n  (lm_head): QuantizedLinear(\n    in_features=2048, out_features=128256, bias=False\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): QuantizerStub()\n    (bias_quantizer): None\n    (output_quantizer): QuantizerStub()\n  )\n)</pre> In\u00a0[8]: Copied! <pre>print(\"Calibrating weight-quantizers\")\n\nwith torch.no_grad(), ff.estimate_ranges(model, ff.range_setting.running_minmax):\n    x = next(iter(train_loader))\n    x = prepare_batch(x, device)\n    model(**x)\n\nprint(\"Calibrated!\")\n</pre>  print(\"Calibrating weight-quantizers\")  with torch.no_grad(), ff.estimate_ranges(model, ff.range_setting.running_minmax):     x = next(iter(train_loader))     x = prepare_batch(x, device)     model(**x)  print(\"Calibrated!\") <pre>Calibrating weight-quantizers\n</pre> <pre>Calibrated!\n</pre> In\u00a0[9]: Copied! <pre>w_quant_perplexity = evaluate_model(model, valid_loader, device=device, max_num_batches=None)\nprint(\"Perplexity over wikitext-validation:\")\nprint(f\" - Original model:       {orig_perplexity:.4f}  ({model_dtype}) \")\nprint(f\" - W-Quantized model:    {w_quant_perplexity:.4f}  (W{w_bits})\")\n</pre> w_quant_perplexity = evaluate_model(model, valid_loader, device=device, max_num_batches=None) print(\"Perplexity over wikitext-validation:\") print(f\" - Original model:       {orig_perplexity:.4f}  ({model_dtype}) \") print(f\" - W-Quantized model:    {w_quant_perplexity:.4f}  (W{w_bits})\") <pre>Perplexity over wikitext-validation:\n - Original model:       14.0874  (torch.float16) \n - W-Quantized model:    14.1063  (W8)\n</pre> In\u00a0[10]: Copied! <pre># We must import QuantizedLinear to enable the fragment [cls:QuantizedLinear] in find_quantizer.\n\na_bits = 16\n\n# Select all linear layers output quantizers:\na_quantizers = ff.find_quantizers(\n    model, \"**/layers/**/[cls:torch.nn.Linear]/[quantizer:activation/input]\"\n)\na_quantizers.initialize(\n    ff.nn.LinearQuantizer, num_bits=a_bits, symmetric=False, granularity=ff.PerTensor()\n)\nprint(f\"Found {len(a_quantizers)} activation quantizers.\")\n\n# Move model to target device: quantizer parameters (scale/offsets) should be placed on the target device too.\nmodel.to(device)\n</pre> # We must import QuantizedLinear to enable the fragment [cls:QuantizedLinear] in find_quantizer.  a_bits = 16  # Select all linear layers output quantizers: a_quantizers = ff.find_quantizers(     model, \"**/layers/**/[cls:torch.nn.Linear]/[quantizer:activation/input]\" ) a_quantizers.initialize(     ff.nn.LinearQuantizer, num_bits=a_bits, symmetric=False, granularity=ff.PerTensor() ) print(f\"Found {len(a_quantizers)} activation quantizers.\")  # Move model to target device: quantizer parameters (scale/offsets) should be placed on the target device too. model.to(device)  <pre>Found 112 activation quantizers.\n</pre> Out[10]: <pre>QuantizedLlamaForCausalLM(\n  (model): QuantizedLlamaModel(\n    (embed_tokens): QuantizedEmbedding(\n      128256, 2048\n      (weight_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n    )\n    (layers): QuantizedModuleList(\n      (0-15): 16 x QuantizedLlamaDecoderLayer(\n        (self_attn): QuantizedLlamaSdpaAttention(\n          (q_proj): QuantizedLinear(\n            in_features=2048, out_features=2048, bias=False\n            (input_quantizer): LinearQuantizer(num_bits=16, symmetric=False, granularity=PerTensor())\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (k_proj): QuantizedLinear(\n            in_features=2048, out_features=512, bias=False\n            (input_quantizer): LinearQuantizer(num_bits=16, symmetric=False, granularity=PerTensor())\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (v_proj): QuantizedLinear(\n            in_features=2048, out_features=512, bias=False\n            (input_quantizer): LinearQuantizer(num_bits=16, symmetric=False, granularity=PerTensor())\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (o_proj): QuantizedLinear(\n            in_features=2048, out_features=2048, bias=False\n            (input_quantizer): LinearQuantizer(num_bits=16, symmetric=False, granularity=PerTensor())\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (rotary_emb): QuantizedLlamaRotaryEmbedding()\n          (input_quantizer): QuantizerStub()\n          (rope_q_quantizer): QuantizerStub()\n          (rope_k_quantizer): QuantizerStub()\n          (sdpa_matmul_quantizer): QuantizerStub()\n          (sdpa_softmax_quantizer): QuantizerStub()\n          (sdpa_dropout_quantizer): QuantizerStub()\n          (sdpa_output_quantizer): QuantizerStub()\n        )\n        (mlp): QuantizedLlamaMLP(\n          (gate_proj): QuantizedLinear(\n            in_features=2048, out_features=8192, bias=False\n            (input_quantizer): LinearQuantizer(num_bits=16, symmetric=False, granularity=PerTensor())\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (up_proj): QuantizedLinear(\n            in_features=2048, out_features=8192, bias=False\n            (input_quantizer): LinearQuantizer(num_bits=16, symmetric=False, granularity=PerTensor())\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (down_proj): QuantizedLinear(\n            in_features=8192, out_features=2048, bias=False\n            (input_quantizer): LinearQuantizer(num_bits=16, symmetric=False, granularity=PerTensor())\n            (weight_quantizer): LinearQuantizer(num_bits=8, symmetric=True, granularity=PerChannel(channel=0))\n            (bias_quantizer): None\n            (output_quantizer): QuantizerStub()\n          )\n          (act_fn): QuantizedSilu(\n            (input_quantizer): QuantizerStub()\n            (output_quantizer): QuantizerStub()\n          )\n          (input_quantizer): QuantizerStub()\n          (gate_act_quantizer): QuantizerStub()\n          (gated_up_proj_output_quantizer): QuantizerStub()\n        )\n        (input_layernorm): QuantizedLlamaRMSNorm(\n          (2048,), eps=1e-05\n          (input_quantizer): QuantizerStub()\n          (output_quantizer): QuantizerStub()\n          (weight_quantizer): QuantizerStub()\n        )\n        (post_attention_layernorm): QuantizedLlamaRMSNorm(\n          (2048,), eps=1e-05\n          (input_quantizer): QuantizerStub()\n          (output_quantizer): QuantizerStub()\n          (weight_quantizer): QuantizerStub()\n        )\n        (input_quantizer): QuantizerStub()\n        (attn_res_act_quantizer): QuantizerStub()\n        (mlp_res_act_quantizer): QuantizerStub()\n      )\n    )\n    (norm): QuantizedLlamaRMSNorm(\n      (2048,), eps=1e-05\n      (input_quantizer): QuantizerStub()\n      (output_quantizer): QuantizerStub()\n      (weight_quantizer): QuantizerStub()\n    )\n    (rotary_emb): QuantizedLlamaRotaryEmbedding()\n  )\n  (lm_head): QuantizedLinear(\n    in_features=2048, out_features=128256, bias=False\n    (input_quantizer): QuantizerStub()\n    (weight_quantizer): QuantizerStub()\n    (bias_quantizer): None\n    (output_quantizer): QuantizerStub()\n  )\n)</pre> In\u00a0[11]: Copied! <pre>model.eval()\n\nprint(\"Calibrating quantizers over training set...\")\n\nwith torch.no_grad(), ff.estimate_ranges(model, ff.range_setting.running_minmax):\n    for i, x in enumerate(tqdm(train_loader)):\n        x = prepare_batch(x, device)\n        model(**x)\n\nprint(\"Calibrated!\")\n</pre> model.eval()  print(\"Calibrating quantizers over training set...\")  with torch.no_grad(), ff.estimate_ranges(model, ff.range_setting.running_minmax):     for i, x in enumerate(tqdm(train_loader)):         x = prepare_batch(x, device)         model(**x)  print(\"Calibrated!\") <pre>Calibrating quantizers over training set...\n</pre> <pre>Calibrated!\n</pre> In\u00a0[12]: Copied! <pre>wa_quant_perplexity = evaluate_model(model, valid_loader, device=device, max_num_batches=None)\n\nprint(\"Perplexity over wikitext-validation:\")\nprint(f\" - Original model:       {orig_perplexity:.4f}  ({model_dtype}) \")\nprint(f\" - W-Quantized model:    {w_quant_perplexity:.4f}  (W{w_bits})\")\nprint(f\" - W+A Quantized model:  {wa_quant_perplexity:.4f}  (W{w_bits}A{a_bits})\")\n</pre> wa_quant_perplexity = evaluate_model(model, valid_loader, device=device, max_num_batches=None)  print(\"Perplexity over wikitext-validation:\") print(f\" - Original model:       {orig_perplexity:.4f}  ({model_dtype}) \") print(f\" - W-Quantized model:    {w_quant_perplexity:.4f}  (W{w_bits})\") print(f\" - W+A Quantized model:  {wa_quant_perplexity:.4f}  (W{w_bits}A{a_bits})\") <pre>Perplexity over wikitext-validation:\n - Original model:       14.0874  (torch.float16) \n - W-Quantized model:    14.1063  (W8)\n - W+A Quantized model:  17.5027  (W8A16)\n</pre> <p>In this tutorial, we demonstrated how to use FastForward to apply weight-only and weight-activation quantization to a large language model. We also evaluated the performance differences compared to the original model.</p> <p>FastForward, currently, provides a semi-automatic process for converting a model into a quantized one. However, if your model includes custom PyTorch modules, some manual work is still required to create a quantized version of those modules.</p> <p>For more information on how to quantize a model from scratch, check out the tutorial:Getting Started: Quantizing a LLM from scratch.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#quick-start-quantization-of-llama","title":"Quick Start: Quantization of Llama\u00b6","text":"<p>In this tutorial we will show how to quantize a large language model (Llama-v3) using FastForward.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-1-install-dependencies","title":"Step 1: Install Dependencies\u00b6","text":"<p>First, make sure you have all the necessary dependencies installed. You can do this by running the following command:</p> <pre><code>pip install transformers==4.46.3 sentencepiece==0.2.0 ipywidgets==8.1.5 datasets==3.1.0\n</code></pre> <p>For instructions on installing <code>fastforward</code>, please refer to the project's documentation and/or readme.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-2-load-the-model-tokenizer-and-datasets","title":"Step 2: Load the Model, Tokenizer, and Datasets\u00b6","text":"<p>Next, we'll load the model, tokenizer, and datasets using the HuggingFace's <code>transformers</code> and <code>datasets</code> libraries.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#base-model-evaluation","title":"Base Model Evaluation\u00b6","text":""},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-3-establish-an-inference-loop","title":"Step 3: Establish an Inference Loop\u00b6","text":"<p>First, we'll create an inference loop to assess the performance of the base model. This loop will be our foundation, allowing us to later compare the results with those from the quantized versions of the model.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#quantized-model","title":"Quantized Model\u00b6","text":"<p>Now that we have the original full-precision model and the tokenized dataset, we can start quantizing the model using FastForward.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-4-convert-to-a-quantization-ready-model","title":"Step 4: Convert to a Quantization-Ready Model\u00b6","text":"<p>First, we need to convert our model into a quantization-ready one. This type of model, called a <code>QuantizedModule</code>, allows us to fully or partially quantize the model easily. These modules work like standard <code>PyTorch</code> modules but have extra features for seamless interaction with <code>FastForward</code> APIs.</p> <p>Currently, converting a model into a quantized module is semi-automatic and requires a custom implementation of all the PyTorch modules involved. If you want to create a custom QuantizedModule, check out the tutorial on manually quantizing custom modules. However, for this tutorial, we will use pre-provided modules to quantize the Llama model.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#weight-only-quantization","title":"Weight-only Quantization\u00b6","text":"<p>A <code>QuantizedModule</code> contains <code>QuantizerStub</code> instances at specific locations. Each quantizer stub is a placeholder module that can be easily replaced with any FastForward quantizer.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-5-replace-stubs-with-actual-quantizers","title":"Step 5: Replace Stubs with Actual Quantizers\u00b6","text":"<p>We can start quantizing the model by replacing the stubs with actual quantizers. To do this, we use the <code>find_quantizers</code> function to select certain stubs and initialize them as <code>LinearQuantizer</code> objects.</p> <p>In this example, we will limit our quantization to all the weights in the self-attention and MLP modules within the Llama decoder layers. +</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#background-fastforwardmpath","title":"Background: <code>fastforward.mpath</code>\u00b6","text":"<p>The <code>find_quantizers</code> function is a tool for filtering and selecting specific quantizers within the model, using the capabilities provided by <code>fastforward.mpath</code>.</p> <p>By passing queries to <code>find_quantizer</code>, we can navigate the model and select quantizers similarly to how we select files in a Unix-like file system. Using strings and wildcards, we can match modules and quantizers just like matching folders and files from the terminal.</p> <p>Additionally, mpath offers advanced functionalities to select modules and quantizers based on special rules. In this example, we selected only the quantizers with the tag parameter/weight because we aim to perform weights-only quantization.</p> <p>Each <code>find_quantizer</code> call returns a QuantizerCollection object containing the selected quantizers, which behaves similarly to a Python set. In this case, we merged two collections simply using the <code>|</code> operator.</p> <p>For more information about mpath and its full range of functionalities, we recommend reading the MPath tutorial.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#calibrate-weight-quantized-model","title":"Calibrate Weight-Quantized Model\u00b6","text":"<p>Before performing inference, we need to initialize the quantizer parameters using calibration data.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-6-estimate-quantization-ranges","title":"Step 6: Estimate Quantization Ranges\u00b6","text":"<p>FastForward provides a method for estimating quantization ranges by running the model's forward pass. This is done using the <code>fastforward.estimate_ranges</code> context manager.</p> <p>For weight-only quantization, passing dummy data is sufficient since no activation quantizers are involved. In this example, we use a running min-max estimator, which sets the quantizer ranges to the minimum and maximum values observed in the tensors during the forward pass.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-7-evaluate-the-weight-quantized-model","title":"Step 7: Evaluate the weight-quantized model\u00b6","text":"<p>Now, we can perform inference and evaluate the model's performance using the same procedure applied to the original model.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#weight-activation-quantization","title":"Weight-Activation Quantization\u00b6","text":"<p>In addition to the weight quantizers, we will now initialize some of the input quantizers for the linear layers; enabling weight-activation quantization.</p> <p>Generally, quantizing all activations can significantly degrade accuracy. Therefore, in this example, we will limit our quantization to the inputs of the linear layers within the model.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-8-initialize-input-quantizers","title":"Step 8: Initialize Input Quantizers\u00b6","text":"<p>We are initializing input quantizers for the linear layers to enable weight-activation quantization.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-8-calibrating-activation-quantizers","title":"Step 8: Calibrating Activation Quantizers\u00b6","text":"<p>Activation quantizers are significantly more sensitive to calibration. Unlike weight quantizers, the exact range of data passing through activation quantizers cannot be determined in advance. Therefore, we need to estimate the activation ranges using a calibration set.</p> <p>In this example, we use the Wikitext training set for calibration. Evaluation is conducted on a separate validation set.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#step-9-evaluate-weight-activation-quantized-model","title":"Step 9: Evaluate Weight-Activation Quantized Model\u00b6","text":"<p>Now, we can evaluate our weight-activation quantized model and compare its performance to the other models.</p>"},{"location":"examples/quick_start/quick_start_quantize_llms.nb/#wrap-up","title":"Wrap up\u00b6","text":""},{"location":"reference/summary/","title":"Summary","text":"<ul> <li>fastforward<ul> <li>common</li> <li>dispatcher</li> <li>exceptions</li> <li>flags</li> <li>forward_override</li> <li>mpath<ul> <li>fragments</li> <li>selector</li> </ul> </li> <li>nn<ul> <li>activations</li> <li>container</li> <li>conv</li> <li>dynamic_linear_quantizer</li> <li>embedding</li> <li>functional</li> <li>linear</li> <li>linear_quantizer</li> <li>normalization</li> <li>quantized_module</li> <li>quantizer</li> </ul> </li> <li>overrides</li> <li>quantization<ul> <li>affine</li> <li>dynamic</li> <li>function</li> <li>granularity</li> <li>local_error</li> <li>output_mse</li> <li>quant_init</li> <li>random</li> <li>ste</li> <li>strict_quantization</li> <li>tiled_tensor</li> </ul> </li> <li>quantized_tensor</li> <li>range_setting<ul> <li>common</li> <li>min_error</li> <li>minmax</li> </ul> </li> <li>type_common</li> </ul> </li> </ul>"},{"location":"reference/fastforward/","title":"fastforward","text":""},{"location":"reference/fastforward/#fastforward.PerChannel","title":"<code>fastforward.PerChannel = granularity.PerChannel</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/#fastforward.PerTensor","title":"<code>fastforward.PerTensor = granularity.PerTensor</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/#fastforward.PerTile","title":"<code>fastforward.PerTile = granularity.PerTile</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/#fastforward.__version__","title":"<code>fastforward.__version__ = version</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/common/","title":"common","text":""},{"location":"reference/fastforward/common/#fastforward.common.S","title":"<code>fastforward.common.S = TypeVar('S')</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/common/#fastforward.common.T","title":"<code>fastforward.common.T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/common/#fastforward.common.ensure_tensor","title":"<code>fastforward.common.ensure_tensor(maybe_tensor, device=None)</code>","text":"<p>Convert <code>maybe_tensor</code> to a tensor if it is not already a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>maybe_tensor</code> <code>Tensor | float | Sequence[float]</code> <p><code>Object</code> to convert to a tensor</p> required <code>device</code> <code>Optional[device]</code> <p>Device to create the tensor on if <code>maybe_tensor</code> is not a tensor, otherwise device is ignored.</p> <code>None</code>"},{"location":"reference/fastforward/common/#fastforward.common.maybe_tensor_apply","title":"<code>fastforward.common.maybe_tensor_apply(maybe_tensor, fn)</code>","text":"<pre><code>maybe_tensor_apply(maybe_tensor: torch.Tensor, fn: Callable[[torch.Tensor], S]) -&gt; S\n</code></pre><pre><code>maybe_tensor_apply(maybe_tensor: T, fn: Callable[[torch.Tensor], Any]) -&gt; T\n</code></pre> <p>Apply function to tensor.</p> <p>Apply <code>fn</code> to <code>maybe_tensor</code> if <code>maybe_tensor</code> is an instance of <code>torch.Tensor</code>. Otherwise return `maybe_tensor.</p> <p>Parameters:</p> Name Type Description Default <code>maybe_tensor</code> <code>T</code> <p>Any object</p> required <code>fn</code> <code>Callable[[Tensor], S]</code> <p>Function that is applied to <code>maybe_tensor</code> if it is a tensor</p> required"},{"location":"reference/fastforward/dispatcher/","title":"dispatcher","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.KernelType","title":"<code>fastforward.dispatcher.KernelType: typing.TypeAlias = Callable[P, torch.Tensor]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.P","title":"<code>fastforward.dispatcher.P = ParamSpec('P')</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherItem","title":"<code>fastforward.dispatcher.DispatcherItem(predicate, fn, priority=DispatcherPriority.DEFAULT)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[P]</code></p> <p>Data class representing an item in the dispatcher.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>Predicate[P]</code> <p>A predicate that determines when the kernel should be used.</p> required <code>fn</code> <code>KernelType[P]</code> <p>The kernel function to be dispatched.</p> required <code>priority</code> <code>DispatcherPriority</code> <p>The priority of the dispatcher item.</p> <code>DEFAULT</code>"},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherItem.fn","title":"<code>fn: KernelType[P]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherItem.predicate","title":"<code>predicate: Predicate[P]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherItem.priority","title":"<code>priority: DispatcherPriority = DispatcherPriority.DEFAULT</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherPriority","title":"<code>fastforward.dispatcher.DispatcherPriority</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Dispatch priority for registered implementations.</p> <p>Operator implementations predicates are evaluated in the order DEFAULT, FALLBACK, NOT_IMPLEMENTED_FALLBACK.</p>"},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherPriority.DEFAULT","title":"<code>DEFAULT = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherPriority.FALLBACK","title":"<code>FALLBACK = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherPriority.NOT_IMPLEMENTED_FALLBACK","title":"<code>NOT_IMPLEMENTED_FALLBACK = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherRegistrationHook","title":"<code>fastforward.dispatcher.DispatcherRegistrationHook(op_name, dispatcher_item)</code>","text":"<p>Context manager for registering and unregistering dispatcher items.</p> <p>Parameters:</p> Name Type Description Default <code>op_name</code> <code>str</code> <p>The name of the operation.</p> required <code>dispatcher_item</code> <code>DispatcherItem[Any]</code> <p>The dispatcher item to register.</p> required"},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherRegistrationHook.__enter__","title":"<code>__enter__()</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.DispatcherRegistrationHook.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.Predicate","title":"<code>fastforward.dispatcher.Predicate(fn)</code>","text":"<p>               Bases: <code>_Predicate[P]</code></p> <p>Wrapper for a predicate function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[P, bool]</code> <p>A callable that takes arbitrary arguments and returns a boolean.</p> required"},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.Predicate.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Call wrapped predicate and return result.</p>"},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.Predicate.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.dispatch","title":"<code>fastforward.dispatcher.dispatch(op_name, *args, **kwargs)</code>","text":"<p>Returns the latest registered kernel whose predicate evaluates to True.</p> <p>Parameters:</p> Name Type Description Default <code>op_name</code> <code>str</code> <p>The name of the operation.</p> required <code>*args</code> <code>args</code> <p>Positional arguments to pass to the predicate.</p> <code>()</code> <code>**kwargs</code> <code>kwargs</code> <p>Keyword arguments to pass to the predicate.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[KernelType[P]]</code> <p>The kernel function if a matching predicate is found, otherwise None.</p>"},{"location":"reference/fastforward/dispatcher/#fastforward.dispatcher.register","title":"<code>fastforward.dispatcher.register(op_name, predicate, kernel=None, priority=DispatcherPriority.DEFAULT)</code>","text":"<pre><code>register(op_name: str, predicate: Optional[Predicate[P]], kernel: KernelType[P], priority: DispatcherPriority = DispatcherPriority.DEFAULT) -&gt; DispatcherRegistrationHook\n</code></pre><pre><code>register(op_name: str, predicate: Optional[Predicate[P]], kernel: None = None, priority: DispatcherPriority = DispatcherPriority.DEFAULT) -&gt; Callable[[KernelType[P]], KernelType[P]]\n</code></pre> <p>Register a new implementation for an operator.</p> <p>This which will be used when the predicate evaluates to True. Might raise an exception if there is already a kernel with the same priority.</p> <p>register can be used standalone:</p> <p>register(op_name, predicate, kernel)</p> <p>as a standalone context manager:</p> <p>@register(op_name, predicate) def my_kernel(...):    ...</p> <p>or using a with statement:</p> <pre><code>&gt; with register(op_name, predicate, kernel):\n&gt;     ...\n</code></pre> <p>In the latter case, the registered implementation is removed at the end of the with block.</p>"},{"location":"reference/fastforward/exceptions/","title":"exceptions","text":""},{"location":"reference/fastforward/exceptions/#fastforward.exceptions.QuantizationError","title":"<code>fastforward.exceptions.QuantizationError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>General quantization error.</p>"},{"location":"reference/fastforward/flags/","title":"flags","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.compiled_quant_funcs","title":"<code>fastforward.flags.compiled_quant_funcs = _compiled_quant_funcs.context</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.export_mode","title":"<code>fastforward.flags.export_mode = _export_mode.context</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.get_compiled_quant_funcs","title":"<code>fastforward.flags.get_compiled_quant_funcs = _compiled_quant_funcs.getter</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.get_export_mode","title":"<code>fastforward.flags.get_export_mode = _export_mode.getter</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.get_strict_quantization","title":"<code>fastforward.flags.get_strict_quantization = _strict_quant.getter</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.set_compiled_quant_funcs","title":"<code>fastforward.flags.set_compiled_quant_funcs = _compiled_quant_funcs.setter</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.set_export_mode","title":"<code>fastforward.flags.set_export_mode = _export_mode.setter</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.set_strict_quantization","title":"<code>fastforward.flags.set_strict_quantization = _strict_quant.setter</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.strict_quantization","title":"<code>fastforward.flags.strict_quantization = _strict_quant.context</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/flags/#fastforward.flags.context","title":"<code>fastforward.flags.context(flag, value)</code>","text":"<p>Decorator to execute a function in a given context.</p> <p>Here context refers to a value of a flag. Using this decorator, the flag value is set to <code>value</code> before the decorated function is invoked and reset to it's state before invocations after the function completes.</p> <p>Parameters:</p> Name Type Description Default <code>flag</code> <code>_FlagContext</code> <p>The flag to set</p> required <code>value</code> <code>bool</code> <p>The value to set the value to</p> required"},{"location":"reference/fastforward/forward_override/","title":"forward_override","text":""},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.T","title":"<code>fastforward.forward_override.T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.OverrideFn","title":"<code>fastforward.forward_override.OverrideFn</code>","text":"<p>               Bases: <code>Protocol[T]</code></p> <p>Protocol for override functions.</p> <p>Override functions can be registered to override a quantizer. These acts as dynamic decorators that can be applied an removed at runtime.</p>"},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.OverrideFn.__call__","title":"<code>__call__(__context, __overridden_fn, __args, __kwargs)</code>","text":"<p>Generic call signature.</p> <p>Parameters:</p> Name Type Description Default <code>__context</code> <code>Any</code> <p>The quantizer that is overriden by this override function</p> required <code>__overriden_fn</code> <p>The function that is overriden, this may the forward method of a quantizer, or a reference to another override. This should be called instead of the forward method on __context in order for an override to cooperate with other overrides.</p> required <code>__args</code> <code>tuple[Any, ...]</code> <p>The arguments passed to the quantizer.</p> required <code>__kwargs</code> <code>dict[str, Any]</code> <p>The keyword arguments passed to the quantizer.</p> required"},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.OverrideHandle","title":"<code>fastforward.forward_override.OverrideHandle(override_map)</code>","text":"<p>Handle object which that can be used to remove a function override.</p> <p>Parameters:</p> Name Type Description Default <code>override_map</code> <code>MutableMapping[int, OverrideFn[Any]]</code> <p>Mapping that stores overrides indexed by override id.</p> required"},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.OverrideHandle.global_handles","title":"<code>global_handles: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.OverrideHandle.handle_id","title":"<code>handle_id: int = OverrideHandle.global_handles</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.OverrideHandle.override_map","title":"<code>override_map = weakref.ref(override_map)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.OverrideHandle.__enter__","title":"<code>__enter__()</code>","text":""},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.OverrideHandle.__exit__","title":"<code>__exit__(type, value, traceback)</code>","text":""},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.OverrideHandle.remove","title":"<code>remove()</code>","text":"<p>Remove override associated with this handle.</p> <p>Returns:</p> Type Description <code>OverrideFn[Any] | None</code> <p>(Callable) the override function associated with this handle</p>"},{"location":"reference/fastforward/forward_override/#fastforward.forward_override.apply_overrides","title":"<code>fastforward.forward_override.apply_overrides(context, overridden_fn, override_map)</code>","text":"<p>Apply overrides in <code>override_map</code> to <code>overridden_fn</code>.</p> <p>This returns a callable that, when applicable, calls functions overrides. These act similarly to decorators.</p> <p>The overrides are applied in descending order of the key in <code>override_map</code></p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Any</code> <p>An object related to overridden fn that may store contextual data that is passed into the function overrides.</p> required <code>overridden_fn</code> <code>Callable[..., T]</code> <p>The function to override.</p> required <code>override_map</code> <code>Mapping[int, OverrideFn[T]]</code> required <p>Returns:</p> Type Description <code>_WrappedOverriddenFn[T] | Callable[..., T]</code> <p>(Callable[..., T]) wrapped overridden_fn, or overridden_fn if override_map</p> <code>_WrappedOverriddenFn[T] | Callable[..., T]</code> <p>is empty.</p>"},{"location":"reference/fastforward/overrides/","title":"overrides","text":""},{"location":"reference/fastforward/overrides/#fastforward.overrides.DisableQuantizationOverride","title":"<code>fastforward.overrides.DisableQuantizationOverride()</code>","text":"<p>Override to disable quantization.</p> <p>Attach <code>DisableQuantizationOverride</code> instance as quantizer override to disable quantization. Quantization is enabled/disabled using the <code>enable_quantization</code> and <code>disable_quantization</code> methods for all quantizers the instance is attached to.</p> <p>Quantizers can be 'bulk' attached to using the <code>attach_to</code> and <code>detach</code> methods. Note that <code>detach</code> will only detach from quantizers which where attached to using the <code>attach_to</code> methods, i.e.,  if an instance of <code>DisableQuantizationOverride</code> is registered as override to a quantizers using different means, it must be detached separately.</p>"},{"location":"reference/fastforward/overrides/#fastforward.overrides.DisableQuantizationOverride.quantization_enabled","title":"<code>quantization_enabled: bool</code>  <code>property</code>","text":"<p>True if quantization is enabled, False otherwise.</p>"},{"location":"reference/fastforward/overrides/#fastforward.overrides.DisableQuantizationOverride.__call__","title":"<code>__call__(_context, callback, args, kwargs)</code>","text":""},{"location":"reference/fastforward/overrides/#fastforward.overrides.DisableQuantizationOverride.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/overrides/#fastforward.overrides.DisableQuantizationOverride.attach_to","title":"<code>attach_to(quantizers)</code>","text":"<p>Attach this override to one or more quantizers.</p> <p>Parameters:</p> Name Type Description Default <code>quantizers</code> <code>Quantizer | QuantizerCollection | Iterable[Quantizer]</code> <p>Either a single quantizer, a <code>QuantizerCollection</code> obtained using <code>ff.find_quantizers</code> or an iterable of <code>Quantizer</code>s. Attach this override to each.</p> required"},{"location":"reference/fastforward/overrides/#fastforward.overrides.DisableQuantizationOverride.detach","title":"<code>detach()</code>","text":"<p>Detach this override.</p> <p>Detach from all quantizers it was attached to using the <code>attach_to</code> method.</p>"},{"location":"reference/fastforward/overrides/#fastforward.overrides.DisableQuantizationOverride.disable_quantization","title":"<code>disable_quantization()</code>","text":"<p>Disable quantization.</p> <p>See the docstring of <code>enable_quantization</code> for more information.</p>"},{"location":"reference/fastforward/overrides/#fastforward.overrides.DisableQuantizationOverride.enable_quantization","title":"<code>enable_quantization(enabled=True)</code>","text":"<p>Enable quantization.</p> <p>More specifically, this instance will not disable quantization for any quantizers it is attached to. Other instance, or other methods may still disable quantization.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>True if quantization must be enabled, False if it must be disabled.</p> <code>True</code>"},{"location":"reference/fastforward/overrides/#fastforward.overrides.disable_quantization","title":"<code>fastforward.overrides.disable_quantization(model)</code>","text":"<p>Disable quantization for all quantizers in <code>model</code> within context.</p> <p>The global <code>strict_quantization</code> flag is also set to <code>False</code> during the context.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model for which all quantizers are disabled.</p> required"},{"location":"reference/fastforward/quantized_tensor/","title":"quantized_tensor","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor","title":"<code>fastforward.quantized_tensor.QuantizedTensor(data, quantization_function)</code>","text":"<p>               Bases: <code>Tensor</code></p> <p>Tensor that holds quantized data that can be dequantized.</p> <p>In general, a quantized tensor has an integer representation and associated parameters. Operations can either dequantize to obtain a 'real-valued' representation of the data or act on the integer data directly.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Raw quantized data, e.g., the integer repsentation obtained from quantization_function.</p> required <code>quantization_function</code> <code>BoundQuantizationFunction</code> <p>The quantization function used to produce data.</p> required"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.is_quantized","title":"<code>is_quantized: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Always returns False for a <code>QuantizedTensor</code> and should not be used to identify QuantizedTensors.</p> <p>This property evaluates to False for QuantizedTensors as it is otherwise identified as a PyTorch native Quantized tensor. The recommended approach to test for QuantizedTensors is using isinstance.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.quant_func","title":"<code>quant_func: type[BaseQuantizationFunction]</code>  <code>property</code>","text":"<p>Return the associated quantization function.</p> <p>Returns:</p> Name Type Description <code>BaseQuantizationFunction</code> <code>type[BaseQuantizationFunction]</code> <p>The <code>QuantizationFunction</code> implementation used to quantize self.</p> Note <p>this only returns the implementation and no arguments are bound to it. To obtain a bound quantization function use:</p> <p>quantized_tensor.quant_func.bind(**quantized_tensor.quant_args())</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.raw_data","title":"<code>raw_data: torch.Tensor</code>  <code>property</code>","text":"<p>Return raw data representation.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Torch.Tensor: the raw_data as a normal tensor.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__iadd__","title":"<code>__iadd__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__iand__","title":"<code>__iand__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__ifloordiv__","title":"<code>__ifloordiv__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__ilshift__","title":"<code>__ilshift__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__imatmul__","title":"<code>__imatmul__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__imod__","title":"<code>__imod__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__imul__","title":"<code>__imul__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__ior__","title":"<code>__ior__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__ipow__","title":"<code>__ipow__(other, modulo=None)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__irshift__","title":"<code>__irshift__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__isub__","title":"<code>__isub__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__itruediv__","title":"<code>__itruediv__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__ixor__","title":"<code>__ixor__(other)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__new__","title":"<code>__new__(data, *args, **kwargs)</code>","text":"<p>Create a new quantized tensor.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__reduce_ex__","title":"<code>__reduce_ex__(proto)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__repr__","title":"<code>__repr__(**kwargs)</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.__torch_function__","title":"<code>__torch_function__(func, types, args=(), kwargs=None)</code>  <code>classmethod</code>","text":""},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.clone","title":"<code>clone()</code>","text":"<p>Clone tensor.</p> <p>This makes a copy of the tensor and associated quantization parameter tensors</p> <p>Returns:</p> Name Type Description <code>QuantizedTensor</code> <code>QuantizedTensor</code> <p>Quantized tensor that holds the same data but copied.</p> Note <p>The non-tensor quantization parameters are not copied.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.contiguous","title":"<code>contiguous(memory_format=torch.contiguous_format)</code>","text":"<p>Return a tensor with contiguous data, this includes quantization parameters.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.cpu","title":"<code>cpu()</code>","text":"<p>Move <code>QuantizedTensor</code> and associated parameters to CPU.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.cuda","title":"<code>cuda(device=None, non_blocking=False)</code>","text":"<p>Move <code>QuantizedTensor</code> and associated parameters to GPU.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.dequantize","title":"<code>dequantize()</code>","text":"<p>Dequantize and return real-valued torch.Tensor.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.detach","title":"<code>detach()</code>","text":"<p>Detach tensor.</p> <p>This returns a ternsor that alliases this tensor, but is detached from the autograd graph.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.int_repr","title":"<code>int_repr()</code>","text":"<p>Return the integer (or quantized) data representation underlying this quantized tensor.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.quant_args","title":"<code>quant_args()</code>","text":"<p>Return quantization arguments.</p> <p>Returns:</p> Name Type Description <code>QuantArgs</code> <code>QuantArgs</code> <p>Arguments used to quantize self.</p> Note <p><code>QuantArgs</code> can be mutated without it having an effect on self, but all reference types on <code>QuantArgs</code> are shared and mutation will have side effects.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.QuantizedTensor.to","title":"<code>to(*args, **kwargs)</code>","text":"<pre><code>to(dtype: torch.dtype, non_blocking: bool = False, copy: bool = False) -&gt; Tensor\n</code></pre><pre><code>to(device: Optional[torch.device | int | str] = None, dtype: None = None, non_blocking: bool = False, copy: bool = False) -&gt; _TensorT\n</code></pre><pre><code>to(device: Optional[torch.device | int | str] = None, dtype: Optional[torch.dtype] = None, non_blocking: bool = False, copy: bool = False) -&gt; torch.Tensor\n</code></pre><pre><code>to(other: torch.Tensor, non_blocking: bool = False, copy: bool = False) -&gt; Tensor\n</code></pre> <p>Perform tensor dtype and/or device conversions.</p> <p>When <code>QuantizedTensor</code> is moved to a different device, the associated quantization parameter tensors are moved to the same device. This results in an extra copy of the quantization parameters.</p> <p>See PyTorch's documentation for arguments.</p>"},{"location":"reference/fastforward/quantized_tensor/#fastforward.quantized_tensor.apply_and_reattach","title":"<code>fastforward.quantized_tensor.apply_and_reattach(func, quantized=None)</code>","text":"<pre><code>apply_and_reattach(func: Callable[[torch.Tensor], torch.Tensor], quantized: None = None) -&gt; Callable[[QuantizedTensor], QuantizedTensor]\n</code></pre><pre><code>apply_and_reattach(func: Callable[[torch.Tensor], torch.Tensor], quantized: QuantizedTensor) -&gt; QuantizedTensor\n</code></pre> <p>Apply func to quantized as if it is a normal tensor.</p> <p>Rewrap the result as quantized tensor using the same quantization metadata. If <code>quantized</code> is not provided, a callable that accepts a quantized_tensor and applies func is returned. This way, apply_and_reattach can also be used as a decorator.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[Tensor], Tensor]</code> <p>Function that takes a tensor and produces a tensor</p> required <code>quantized</code> <code>Optional[QuantizedTensor]</code> <p>The tensor to apply func to</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[QuantizedTensor], QuantizedTensor] | QuantizedTensor</code> <p><code>QuantizedTensor</code> Or Callable: If quantized is provided, return the result of applying func to quantized.raw_data as a quantized tensor. If not, return a function that takes a function and applies func when called.</p>"},{"location":"reference/fastforward/type_common/","title":"type_common","text":""},{"location":"reference/fastforward/type_common/#fastforward.type_common.IntOr2Tuple","title":"<code>fastforward.type_common.IntOr2Tuple: TypeAlias = ScalarOr2Tuple[int]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/type_common/#fastforward.type_common.IntOrTuple","title":"<code>fastforward.type_common.IntOrTuple: TypeAlias = ScalarOrTuple[int]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/type_common/#fastforward.type_common.ScalarOr2Tuple","title":"<code>fastforward.type_common.ScalarOr2Tuple: TypeAlias = T | tuple[T, T]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/type_common/#fastforward.type_common.ScalarOrTuple","title":"<code>fastforward.type_common.ScalarOrTuple: TypeAlias = T | tuple[T, ...]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/type_common/#fastforward.type_common.SizeT","title":"<code>fastforward.type_common.SizeT = torch.Size | tuple[int, ...]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/type_common/#fastforward.type_common.T","title":"<code>fastforward.type_common.T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/type_common/#fastforward.type_common.Tuple3","title":"<code>fastforward.type_common.Tuple3: TypeAlias = tuple[T, T, T]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/","title":"mpath","text":"<p>MPath is a utility for finding and filtering submodules of a module. For example, it helps in finding all submodules of the same type or all modules whose name relative to the root module satisfies a certain condition. from typing import Any, NewType, Optional, cast</p> <p>At it's core MPath is used through query strings, however, more complicated queries can be build programmatically.</p> <p>MPath is open to extensions. Examples of these are <code>fragments.RegexPathSelectorFragment</code> and <code>fragments.ClassSelectorFragment</code>.</p> <p>Example:</p> <pre><code>    module = MyModule()\n    mpath.search(\"**/decoder/[cls:torch.nn.Linear]\", module)\n</code></pre> <p>This example will find all linear modules that are part of submodules called decoder, i.e., the attribute name of the module in the parent module. The result of <code>search()</code> is an <code>MPathCollection</code> which is a collection of search results and metadata. This collection behaves like a set, i.e., union, intersection and other set operation are supported. Moreover, it can be used to perform batch updates to the orignal model. See the documentation of <code>MPathCollection</code> for more details.</p>"},{"location":"reference/fastforward/mpath/#fastforward.mpath.Fragment","title":"<code>fastforward.mpath.Fragment = selector.Fragment</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/#fastforward.mpath.MPathQueryContextualExtension","title":"<code>fastforward.mpath.MPathQueryContextualExtension = _parser.MpathQueryContextualExtension</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/#fastforward.mpath.MPathQueryExtension","title":"<code>fastforward.mpath.MPathQueryExtension = _parser.MpathQueryExtension</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/#fastforward.mpath.Selector","title":"<code>fastforward.mpath.Selector = selector.Selector</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/#fastforward.mpath.mpath_query_extension","title":"<code>fastforward.mpath.mpath_query_extension = _parser.mpath_query_extension</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/#fastforward.mpath.register_mpath_query_extension","title":"<code>fastforward.mpath.register_mpath_query_extension = _parser.register_mpath_query_extension</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/#fastforward.mpath.root","title":"<code>fastforward.mpath.root = cast(selector.Selector, query('**'))</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/#fastforward.mpath.aliases","title":"<code>fastforward.mpath.aliases(*, context=None, **kwargs)</code>","text":"<p>Create aliases that can be used in <code>mpath.query</code> and <code>mpath.search</code>. Aliases are a dictionary that maps identifier strings to subqueries. This function is a convenience function for creating that mapping. When aliases are passed to <code>mpath.query</code> or <code>mpath.search</code> they can be referenced in the query using <code>&amp;&lt;alias identifier&gt;</code>.</p> <p>The aliases passed to this function are processed in order, this means that an alias can be defined in terms of other aliases, e.g.:</p> <pre><code>aliases(my_alias=\"/first/second\", other_alias=\"&amp;my_alias/third\")\n</code></pre> <p>The resulting dictionary can be passed to <code>mpath.query</code> or <code>mpath.search</code> as the aliases argument</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[dict[str, Any]]</code> <p>Context dictionary in which to evaluate the queries. Same as <code>mpath.query</code></p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, BaseSelector]</code> <p>Dictionary that can be used as alias in <code>mpath.query</code> and <code>mpath.search</code></p>"},{"location":"reference/fastforward/mpath/#fastforward.mpath.caller_context","title":"<code>fastforward.mpath.caller_context()</code>","text":""},{"location":"reference/fastforward/mpath/#fastforward.mpath.local_context","title":"<code>fastforward.mpath.local_context()</code>","text":""},{"location":"reference/fastforward/mpath/#fastforward.mpath.query","title":"<code>fastforward.mpath.query(query_str, *, context=None, aliases=None)</code>","text":"<p>Construct a query object for MPath from a query string. A query string consists of multiple fragment strings separated by a '/' fragment strings may consists of the following:</p> <ul> <li><code>*</code>: wildcard, will match any single fragment</li> <li><code>**</code>: wildcard, will match zero or more fragments</li> <li><code>[_a-zA-Z0-9]+</code>: path selector, will match a module whose name equals the   fragment string</li> <li><code>[&lt;extension_name&gt;: &lt;extension_input&gt;]</code>: use the extension identified by   extension_name (see register_mpath_query_extension for more details)</li> <li><code>~&lt;other_fragment&gt;</code>: match if other_fragment evaluates to False and vice versa.</li> </ul> <p>For example, consider the following module:     Module(         (attention): Attention(             (Q): Linear()             (K): Linear()             (V): Linear()         )         (output): Linear()     )</p> <p>The query string <code>attention/Q</code> will match the linear layer called Q in attention. <code>**/attention/[cls: torch.nn.Linear]</code> will match all the linear layers (Q, K, V) in the attention module. <code>**/~[cls:torch.nn.Linear]</code> will match the attention layer.</p> <p>Parameters:</p> Name Type Description Default <code>query_str</code> <code>str</code> <p>The query string to be parsed</p> required <code>context</code> <code>Optional[_QueryContext]</code> <p>Context available to extension. If no context is passed, all locals and globals from the call site are included automatically.</p> <code>None</code> <code>aliases</code> <code>Optional[dict[str, BaseSelector]]</code> <p>Mapping from string to <code>Selector</code>s. These can be used in <code>query_str</code> as aliases using <code>&amp;&lt;alias&gt;</code> syntax. Note that the alias name must be a valid python identifier for this to work.</p> <code>None</code> Note <p>fragment extension have access to the local scope at the call location of query. This is, for example, used by the class based matcher. This means that the location at which <code>query()</code> is called matters. If you want to call query outside of the correct context, you can obtain the required context using <code>mpath.local_context()</code> and pass the result of that to <code>query(query_str, context=context)</code></p>"},{"location":"reference/fastforward/mpath/fragments/","title":"fragments","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.logger","title":"<code>fastforward.mpath.fragments.logger = logging.Logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.ClassFragment","title":"<code>fastforward.mpath.fragments.ClassFragment(fragment_class)</code>","text":"<p>               Bases: <code>Fragment</code></p> <p>Match a fragment if the module is an instance of fragment_class</p>"},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.ClassFragment.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.ClassFragment.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.ClassFragment.from_raw_string_with_context","title":"<code>from_raw_string_with_context(raw_str, context)</code>  <code>classmethod</code>","text":"<p>Find an object in context, by name, that matches <code>raw_str</code>. <code>raw_str</code> may be qualified path (i.e., period separated string) that is not directly available in <code>context</code>, but is indirectly through attribute lookups.</p> <p>For example, if <code>raw_str = \"my_module.MyClass.my_attribute\", we first look up</code>my_module<code>in</code>context<code>. Then we perform an attribute lookup for</code>MyClass<code>on</code>my_module<code>and lastly perform an attribute lookup of</code>my_attribute<code>on</code>MyClass`.</p>"},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.ClassFragment.match","title":"<code>match(fragment_name, module)</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.DisjointFragment","title":"<code>fastforward.mpath.fragments.DisjointFragment(*fragments)</code>","text":"<p>               Bases: <code>Fragment</code></p> <p>Match a fragment if one of fragments evaluates to True</p>"},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.DisjointFragment.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.DisjointFragment.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.DisjointFragment.match","title":"<code>match(fragment_name, module)</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.DisjointFragment.must_advance","title":"<code>must_advance()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.InvertedFragment","title":"<code>fastforward.mpath.fragments.InvertedFragment(fragment)</code>","text":"<p>               Bases: <code>Fragment</code></p> <p>Match a fragment if fragment evaluates to False</p>"},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.InvertedFragment.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.InvertedFragment.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.InvertedFragment.match","title":"<code>match(fragment_name, module)</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.InvertedFragment.must_advance","title":"<code>must_advance()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.JointFragment","title":"<code>fastforward.mpath.fragments.JointFragment(*fragments)</code>","text":"<p>               Bases: <code>Fragment</code></p> <p>Match a fragment if all fragments evaluate to True</p>"},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.JointFragment.fragments","title":"<code>fragments: tuple[Fragment, ...]</code>  <code>property</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.JointFragment.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.JointFragment.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.JointFragment.match","title":"<code>match(fragment_name, module)</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.JointFragment.must_advance","title":"<code>must_advance()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.PathFragment","title":"<code>fastforward.mpath.fragments.PathFragment(fragment_str)</code>","text":"<p>               Bases: <code>Fragment</code></p> <p>Match the fragment name (i.e., module name in parent module) exactly.</p>"},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.PathFragment.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.PathFragment.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.PathFragment.match","title":"<code>match(fragment_name, module)</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.PredicateFragment","title":"<code>fastforward.mpath.fragments.PredicateFragment(predicate)</code>","text":"<p>               Bases: <code>Fragment</code></p> <p>Match a fragment if predicate evaluates to True.</p>"},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.PredicateFragment.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.PredicateFragment.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.PredicateFragment.match","title":"<code>match(fragment_name, module)</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.RegexPathFragment","title":"<code>fastforward.mpath.fragments.RegexPathFragment(fragment_pattern)</code>","text":"<p>               Bases: <code>Fragment</code></p> <p>Match a regex against the fragment name (i.e., module name in parent module). The regex must match the full name, otherwise it evaluates</p>"},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.RegexPathFragment.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.RegexPathFragment.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.RegexPathFragment.from_raw_string","title":"<code>from_raw_string(raw_str)</code>  <code>classmethod</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.RegexPathFragment.match","title":"<code>match(fragment_name, module)</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.WildcardFragment","title":"<code>fastforward.mpath.fragments.WildcardFragment(match_multiple=True)</code>","text":"<p>               Bases: <code>Fragment</code></p> <p>Match any fragment exactly once if match_multiple=True and match any fragment zero or more times otherwise.</p>"},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.WildcardFragment.match_multiple","title":"<code>match_multiple = match_multiple</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.WildcardFragment.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.WildcardFragment.match","title":"<code>match(fragment_name, module)</code>","text":""},{"location":"reference/fastforward/mpath/fragments/#fastforward.mpath.fragments.WildcardFragment.must_advance","title":"<code>must_advance()</code>","text":""},{"location":"reference/fastforward/mpath/selector/","title":"selector","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.FragmentList","title":"<code>fastforward.mpath.selector.FragmentList: TypeAlias = 'tuple[Fragment | FragmentList, ...]'</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.logger","title":"<code>fastforward.mpath.selector.logger = logging.Logger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector","title":"<code>fastforward.mpath.selector.BaseSelector(next)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Selector is the representation of MPath query.</p> <p>A selector consists of a sequence of SelectorFragments that all need to match for a module to be included in the results set.</p> <p>Selectors can be appended through using various methods, but we also support concattenating two selectors using 'selector_a / selector_b'. The order of fragments is kept unchanged. I.e., in this example the new selector contains first all the the fragments from selector_a and then all the fragments from selector_b. Hence, this syntax can be used to prepend a (Selector)Fragment to another.</p> <p>When the Selector has length one (i.e., it contains only a single fragment), the '&amp;' operator can be used to produce Selector with a single joint fragment. In all other cases, using these operators results in an exception.</p> <p>Selectors of arbitrary length can be joined using the '|' operator to produce a selector that will match when any any of the selectors match.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.next","title":"<code>next: BaseSelector | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__and__","title":"<code>__and__(rhs)</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__getitem__","title":"<code>__getitem__(key)</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__invert__","title":"<code>__invert__()</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__iter__","title":"<code>__iter__()</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__len__","title":"<code>__len__()</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__or__","title":"<code>__or__(rhs)</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__rtruediv__","title":"<code>__rtruediv__(lhs)</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.__truediv__","title":"<code>__truediv__(rhs)</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.extends","title":"<code>extends(selector)</code>","text":"<p>Extend the current selector with another selector.</p> <p>This method is used to concatenate two selectors. The order of fragments is kept unchanged, i.e., the new selector contains first all the fragments from the current selector and then all the fragments from the given selector.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>BaseSelector</code> <p>The selector to extend with.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new selector that is the concatenation of the current selector and the given selector.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.fragment_list","title":"<code>fragment_list()</code>  <code>abstractmethod</code>","text":"<p>Return a list of fragments that make up this selector and all subsequent selectors.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.fragments","title":"<code>fragments()</code>  <code>abstractmethod</code>","text":"<p>Return a list of fragments that make up this selector.</p> <p>Returns:</p> Type Description <code>tuple[Fragment, ...]</code> <p>A list of fragments that make up this selector.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.has_multi_wildcard_root","title":"<code>has_multi_wildcard_root()</code>  <code>abstractmethod</code>","text":"<p>Return True if the root fragment is a multi-wildcard fragment and False otherwise.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.matches_and_continuations","title":"<code>matches_and_continuations(name, module)</code>  <code>abstractmethod</code>","text":"<p>Returns a list of selector matches and a list of selector continuations.</p> <p>Matches represent a complete matching, i.e., there is no selector tail and the current fragment matched name/module.</p> <p>Continuations represent partial matches, i.e., the current fragment matched, and the tail of the selector should be considered for further matching.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.remove_multi_wildcard_root","title":"<code>remove_multi_wildcard_root()</code>  <code>abstractmethod</code>","text":"<p>Remove the multi-wildcard root from the selector.</p> <p>Returns:</p> Type Description <code>BaseSelector | None</code> <p>The selector with the multi-wildcard root removed, or None if the selector is made up of only a multi-wildcard fragment</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.BaseSelector.simplify","title":"<code>simplify()</code>","text":"<p>Simplify the selector by removing any redundant or unnecessary fragments.</p> <p>This method is used to optimize the selector for better performance.</p> <p>Returns:</p> Type Description <code>BaseSelector</code> <p>A simplified version of the selector.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Fragment","title":"<code>fastforward.mpath.selector.Fragment</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A selector fragment is a matcher for a single element in a module path.</p> <p>Here a module path is the sequence of module names and modules through which one would access a specific module from a root.</p> <p>For example, consider the following module</p> <pre><code>RootModule(\n    (first): SubModule(\n        (second): Linear()\n        (third): Conv2d()\n    )\n)\n</code></pre> <p>A module path to the linear module 'second' is: <code>(\"first\", SubModule), (\"second\", Linear)</code> where <code>SubModule</code> and <code>Linear</code> refer to the specific instances in this module. A selector fragment matches one of the elements (or fragments) on a module path.</p> <p>A Fragment matches a single element. If it matches, the children of the module that matched will be considered for matching the next fragment, or if there is no next fragment, the module is included in the result set.</p> <p>An implementation of Fragment must implement the <code>match</code> function and optionally the <code>must_advance</code> function.</p> <ul> <li>The match function accepts a fragment_name and a module. The fragment   name is the attribute name of the current module in the module path.   The module is the actual model. Using this information, match decides if   the module matches the Fragment.</li> <li>the must_advance function allows for 'wildcard-like' expansion. If must_advance   returns False, the current module is also included in the expansion set. If it   returns True, only the children of the current module are included.</li> </ul> <p>Example of fragment implementations can be found in <code>mpath.fragments</code>.</p> <p>Fragments can be used to create a query programmatically. This may be useful when you want to include more context in the Fragment that would not be possible through the standard query string. Two (or more) Fragments can form a Selector using the '/' operator (which mirrors the use of '/' in query strings). Moreover, higher level joint and disjoint fragments can be created using the '&amp;' and '|' operators.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Fragment.__invert__","title":"<code>__invert__()</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Fragment.match","title":"<code>match(fragment_name, module)</code>  <code>abstractmethod</code>","text":"<p>Matches a single fragment of a path on name or module.</p> <p>Parameters:</p> Name Type Description Default <code>fragment_name</code> <code>str</code> <p>The name of the path, corresponds to the attribute name on the 'parent' object.</p> required <code>module</code> <code>Module</code> <p>The module that corresponds to the fragment name.</p> required <p>Returns: Boolean indicating whether the fragment matches the current position</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Fragment.must_advance","title":"<code>must_advance()</code>","text":"<p>Return whether the current fragment can be repeated.</p> <p>Returns a boolean that indicates if the current module can be considered for the next fragment. This allows implement wildcard like FragmentSelectors.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current module is not included in the expansion set</p> <code>bool</code> <p>and False if it can be included.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.MPathQueryError","title":"<code>fastforward.mpath.selector.MPathQueryError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in the MPath query.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.MultiSelector","title":"<code>fastforward.mpath.selector.MultiSelector(next, selectors)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseSelector</code></p> <p>Represents a multi-fragment selector in an MPath query.</p> <p>Attributes:</p> Name Type Description <code>selectors</code> <code>tuple[BaseSelector, ...]</code> <p>The selectors to match.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.MultiSelector.selectors","title":"<code>selectors: tuple[BaseSelector, ...]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.MultiSelector.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the multi-selector.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the multi-selector.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.MultiSelector.fragment_list","title":"<code>fragment_list()</code>","text":"<p>Return a nested list of fragments in the multi-selector.</p> <p>Returns:</p> Name Type Description <code>FragmentList</code> <code>FragmentList</code> <p>A nested list of fragments.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.MultiSelector.fragments","title":"<code>fragments()</code>","text":"<p>Return a list of fragments in the multi-selector.</p> <p>Returns:</p> Type Description <code>tuple[Fragment, ...]</code> <p>list[Fragment]: A list of fragments.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.MultiSelector.has_multi_wildcard_root","title":"<code>has_multi_wildcard_root()</code>","text":"<p>Check if all selectors have a multi-wildcard root.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all selectors have a multi-wildcard root, False otherwise.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.MultiSelector.matches_and_continuations","title":"<code>matches_and_continuations(name, module)</code>","text":"<p>Return a list of selector matches and continuations.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the module.</p> required <code>module</code> <code>Module</code> <p>The module to match.</p> required <p>Returns:</p> Type Description <code>list[_SelectorMatch]</code> <p>tuple[list[SelectorMatch], list[SelectorContinuation]]: A tuple containing</p> <code>list[_SelectorContinuation]</code> <p>a list of matches and a list of continuations.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.MultiSelector.remove_multi_wildcard_root","title":"<code>remove_multi_wildcard_root()</code>","text":"<p>Remove the multi-wildcard root from the multi-selector.</p> <p>Returns:</p> Type Description <code>BaseSelector | None</code> <p>BaseSelector | None: The multi-selector without the multi-wildcard root, or None if not applicable.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector","title":"<code>fastforward.mpath.selector.Selector(next, fragment)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseSelector</code></p> <p>Represents a single fragment selector in an MPath query.</p> <p>Attributes:</p> Name Type Description <code>fragment</code> <code>Fragment</code> <p>The fragment to match.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector.fragment","title":"<code>fragment: Fragment</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector.__and__","title":"<code>__and__(rhs)</code>","text":"<p>Combine this selector with another using the '&amp;' operator.</p> <p>Parameters:</p> Name Type Description Default <code>rhs</code> <code>object</code> <p>The other selector.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The combined selector.</p> <p>Raises:</p> Type Description <code>MPathQueryError</code> <p>If either selector contains more than one fragment.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector.__invert__","title":"<code>__invert__()</code>","text":"<p>Invert the selector.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The inverted selector.</p> <p>Raises:</p> Type Description <code>MPathQueryError</code> <p>If the selector contains more than one fragment.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the selector.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the selector.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector.fragment_list","title":"<code>fragment_list()</code>","text":"<p>Return a nested list of fragments in the selector.</p> <p>Returns:</p> Name Type Description <code>FragmentList</code> <code>FragmentList</code> <p>A nested list of fragments.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector.fragments","title":"<code>fragments()</code>","text":"<p>Return a list of fragments in the selector.</p> <p>Returns:</p> Type Description <code>tuple[Fragment, ...]</code> <p>list[Fragment]: A list of fragments.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector.has_multi_wildcard_root","title":"<code>has_multi_wildcard_root()</code>","text":"<p>Check if the selector has a multi-wildcard root.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the selector has a multi-wildcard root, False otherwise.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector.matches_and_continuations","title":"<code>matches_and_continuations(name, module)</code>","text":"<p>Return a list of selector matches and continuations.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the module.</p> required <code>module</code> <code>Module</code> <p>The module to match.</p> required <p>Returns:</p> Type Description <code>list[_SelectorMatch]</code> <p>tuple[list[SelectorMatch], list[SelectorContinuation]]: A tuple containing</p> <code>list[_SelectorContinuation]</code> <p>a list of matches and a list of continuations.</p>"},{"location":"reference/fastforward/mpath/selector/#fastforward.mpath.selector.Selector.remove_multi_wildcard_root","title":"<code>remove_multi_wildcard_root()</code>","text":"<p>Remove the multi-wildcard root from the selector.</p> <p>Returns:</p> Type Description <code>BaseSelector | None</code> <p>BaseSelector | None: The selector without the multi-wildcard root, or None if not applicable.</p>"},{"location":"reference/fastforward/nn/","title":"nn","text":""},{"location":"reference/fastforward/nn/activations/","title":"activations","text":""},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedActivation","title":"<code>fastforward.nn.activations.QuantizedActivation</code>","text":"<p>               Bases: <code>QuantizedModule</code></p> <p>Base class for quantized activations.</p>"},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedActivation.__init_quantization__","title":"<code>__init_quantization__()</code>","text":""},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedRelu","title":"<code>fastforward.nn.activations.QuantizedRelu</code>","text":"<p>               Bases: <code>ReLU</code>, <code>QuantizedActivation</code></p> <p>Applies quantized Relu.</p>"},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedRelu--quantizers","title":"Quantizers","text":"<ol> <li>input_quantizer: input activation before relu is applied.</li> <li>output_quantizer: output activation after relu is applied.</li> </ol>"},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedRelu.__init_quantization__","title":"<code>__init_quantization__()</code>","text":""},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedRelu.forward","title":"<code>forward(input)</code>","text":""},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedSilu","title":"<code>fastforward.nn.activations.QuantizedSilu</code>","text":"<p>               Bases: <code>SiLU</code>, <code>QuantizedActivation</code></p> <p>Applies quantized Silu.</p>"},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedSilu--quantizers","title":"Quantizers","text":"<ol> <li>input_quantizer: input activation before silu is applied.</li> <li>output_quantizer: output activation after silu is applied.</li> </ol>"},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedSilu.__init_quantization__","title":"<code>__init_quantization__()</code>","text":""},{"location":"reference/fastforward/nn/activations/#fastforward.nn.activations.QuantizedSilu.forward","title":"<code>forward(input)</code>","text":""},{"location":"reference/fastforward/nn/container/","title":"container","text":""},{"location":"reference/fastforward/nn/container/#fastforward.nn.container.QuantizedModuleDict","title":"<code>fastforward.nn.container.QuantizedModuleDict</code>","text":"<p>               Bases: <code>QuantizedModule</code>, <code>ModuleDict</code></p> <p>Quantized implementation of torch.nn.ModuleDict.</p>"},{"location":"reference/fastforward/nn/container/#fastforward.nn.container.QuantizedModuleList","title":"<code>fastforward.nn.container.QuantizedModuleList</code>","text":"<p>               Bases: <code>QuantizedModule</code>, <code>ModuleList</code></p> <p>Quantized implementation of torch.nn.ModuleList.</p>"},{"location":"reference/fastforward/nn/container/#fastforward.nn.container.QuantizedParameterDict","title":"<code>fastforward.nn.container.QuantizedParameterDict</code>","text":"<p>               Bases: <code>QuantizedModule</code>, <code>ParameterDict</code></p> <p>Quantized implementation of torch.nn.ParameterDict.</p>"},{"location":"reference/fastforward/nn/container/#fastforward.nn.container.QuantizedParameterList","title":"<code>fastforward.nn.container.QuantizedParameterList</code>","text":"<p>               Bases: <code>QuantizedModule</code>, <code>ParameterList</code></p> <p>Quantized implementation of torch.nn.ParameterList.</p>"},{"location":"reference/fastforward/nn/container/#fastforward.nn.container.QuantizedSequential","title":"<code>fastforward.nn.container.QuantizedSequential</code>","text":"<p>               Bases: <code>QuantizedModule</code>, <code>Sequential</code></p> <p>Quantized implementation of torch.nn.Sequential.</p>"},{"location":"reference/fastforward/nn/conv/","title":"conv","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv1d","title":"<code>fastforward.nn.conv.QuantizedConv1d</code>","text":"<p>               Bases: <code>QuantizedModule</code>, <code>Conv1d</code></p> <p>Quantized implementation of Conv1d.</p>"},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv1d.bias_quantizer","title":"<code>bias_quantizer: Quantizer | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv1d.input_quantizer","title":"<code>input_quantizer: Quantizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv1d.output_quantizer","title":"<code>output_quantizer: Quantizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv1d.weight_quantizer","title":"<code>weight_quantizer: Quantizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv1d.__init_quantization__","title":"<code>__init_quantization__()</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv1d.forward","title":"<code>forward(input)</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv2d","title":"<code>fastforward.nn.conv.QuantizedConv2d</code>","text":"<p>               Bases: <code>QuantizedModule</code>, <code>Conv2d</code></p> <p>Quantized implementation of Conv2d.</p>"},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv2d.bias_quantizer","title":"<code>bias_quantizer: Quantizer | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv2d.input_quantizer","title":"<code>input_quantizer: Quantizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv2d.output_quantizer","title":"<code>output_quantizer: Quantizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv2d.weight_quantizer","title":"<code>weight_quantizer: Quantizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv2d.__init_quantization__","title":"<code>__init_quantization__()</code>","text":""},{"location":"reference/fastforward/nn/conv/#fastforward.nn.conv.QuantizedConv2d.forward","title":"<code>forward(input)</code>","text":""},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/","title":"dynamic_linear_quantizer","text":""},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/#fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer","title":"<code>fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer(num_bits, granularity=granularity.PerTensor())</code>","text":"<p>               Bases: <code>Quantizer</code></p> <p>Dynamic Linear quantizer.</p> <p>Support multiple quantization granularities. A granularity defines which parts of the input tensor are quantized using the same quantization parameter. E.g., per-tensor, per-channel, or per-block quantization. The granularity details are implemented in Granularity subclasses and are passed into LinearQuantizer.</p> <p>Parameters:</p> Name Type Description Default <code>num_bits</code> <code>int</code> <p>Bitwidh of quantization output granularity</p> required <code>granularity</code> <code>Granularity</code> <p>Granularity object that specifies the quantization granularity</p> <code>PerTensor()</code>"},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/#fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer.granularity","title":"<code>granularity = granularity</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/#fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer.num_bits","title":"<code>num_bits = num_bits</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/#fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer.per_channel","title":"<code>per_channel: bool</code>  <code>property</code>","text":"<p>Boolean indicating whether quantizer uses PerChannel quantization.</p>"},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/#fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer.per_tensor","title":"<code>per_tensor: bool</code>  <code>property</code>","text":"<p>Boolean indicating whether quantizer uses PerTensor quantization.</p>"},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/#fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer.symmetric","title":"<code>symmetric: bool</code>  <code>property</code>","text":"<p>True if symmetric quantization, False otherwise.</p> <p>Part of fastforward.range_setting.SupportsRangeBasedOperator Protocol</p>"},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/#fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer.bound_quantization_function","title":"<code>bound_quantization_function(tile_size)</code>","text":"<p>Creates a <code>BoundQuantizationFunction</code> using parameters on this module.</p>"},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/#fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Provide extra repr information on num_bits and granularities.</p>"},{"location":"reference/fastforward/nn/dynamic_linear_quantizer/#fastforward.nn.dynamic_linear_quantizer.DynamicLinearQuantizer.quantize","title":"<code>quantize(data)</code>","text":"<p>Quantizer data using dynamic linear quantizer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Tensor to quantize</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Quantized data</p>"},{"location":"reference/fastforward/nn/embedding/","title":"embedding","text":""},{"location":"reference/fastforward/nn/embedding/#fastforward.nn.embedding.QuantizedEmbedding","title":"<code>fastforward.nn.embedding.QuantizedEmbedding</code>","text":"<p>               Bases: <code>Embedding</code>, <code>QuantizedModule</code></p> <p>Quantized implementation of torch.nn.Embedding.</p>"},{"location":"reference/fastforward/nn/embedding/#fastforward.nn.embedding.QuantizedEmbedding.__init_quantization__","title":"<code>__init_quantization__()</code>","text":""},{"location":"reference/fastforward/nn/embedding/#fastforward.nn.embedding.QuantizedEmbedding.forward","title":"<code>forward(input)</code>","text":""},{"location":"reference/fastforward/nn/linear/","title":"linear","text":""},{"location":"reference/fastforward/nn/linear/#fastforward.nn.linear.QuantizedLinear","title":"<code>fastforward.nn.linear.QuantizedLinear</code>","text":"<p>               Bases: <code>QuantizedModule</code>, <code>Linear</code></p> <p>Quantized implementation of torch.nn.Linear.</p>"},{"location":"reference/fastforward/nn/linear/#fastforward.nn.linear.QuantizedLinear.bias_quantizer","title":"<code>bias_quantizer: Quantizer | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/linear/#fastforward.nn.linear.QuantizedLinear.input_quantizer","title":"<code>input_quantizer: Quantizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/linear/#fastforward.nn.linear.QuantizedLinear.output_quantizer","title":"<code>output_quantizer: Quantizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/linear/#fastforward.nn.linear.QuantizedLinear.weight_quantizer","title":"<code>weight_quantizer: Quantizer</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/linear/#fastforward.nn.linear.QuantizedLinear.__init_quantization__","title":"<code>__init_quantization__()</code>","text":""},{"location":"reference/fastforward/nn/linear/#fastforward.nn.linear.QuantizedLinear.forward","title":"<code>forward(input)</code>","text":""},{"location":"reference/fastforward/nn/linear_quantizer/","title":"linear_quantizer","text":""},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer","title":"<code>fastforward.nn.linear_quantizer.LinearQuantizer(num_bits, symmetric=True, allow_one_sided=True, granularity=granularity.PerTensor(), device='cpu')</code>","text":"<p>               Bases: <code>Quantizer</code></p> <p>Linear quantizer.</p> <p>Support multiple quantization granularities. A granularity defines which part of the input tensor are quantized using the same quantization parameter. E.g., per-tensor, per-channel, or per-block quantization. The granularity details are implemented in Granularity subclasses and are passed into LinearQuantizer.</p> <p>Parameters:</p> Name Type Description Default <code>num_bits</code> <code>int</code> <p>Bitwidh of quantization output granularity</p> required <code>symmetric</code> <code>bool</code> <p>Use symmetric quantization if True, asymmetric otherwise. (Default: True)</p> <code>True</code> <code>allow_one_sided</code> <code>bool</code> <p>If symmetric and allow_one_sided, the quantizer may fall back to a one-sided (or unsigned) quantizer if all lower quantizations thresholds are above zero. (Default: True)</p> <code>True</code> <code>granularity</code> <code>Granularity</code> <p>Granularity object that specifies the quantization granularity</p> <code>PerTensor()</code>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.allow_one_sided","title":"<code>allow_one_sided = allow_one_sided</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.granularity","title":"<code>granularity = granularity</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.has_uninitialized_params","title":"<code>has_uninitialized_params: bool</code>  <code>property</code>","text":"<p>Check if there any quantization parameters that are unitialized.</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.integer_maximum","title":"<code>integer_maximum: float</code>  <code>property</code>","text":"<p>The maximum integer value that quantized data takes, based on bitwidth.</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.integer_minimum","title":"<code>integer_minimum: float</code>  <code>property</code>","text":"<p>The minimum integer value that quantized data takes, based on bitwidth.</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.num_bits","title":"<code>num_bits = num_bits</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.offset","title":"<code>offset: torch.Tensor | torch.nn.Parameter | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.per_channel","title":"<code>per_channel: bool</code>  <code>property</code>","text":"<p>Boolean indicating whether quantizer uses PerChannel quantization.</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.per_tensor","title":"<code>per_tensor: bool</code>  <code>property</code>","text":"<p>Boolean indicating whether quantizer uses PerTensor quantization.</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.quantization_range","title":"<code>quantization_range: tuple[torch.Tensor | float | None, torch.Tensor | float | None]</code>  <code>property</code> <code>writable</code>","text":"<p>Getter for quantization range.</p> <p>Returns:</p> Type Description <code>tuple[Tensor | float | None, Tensor | float | None]</code> <p>tuple[torch.Tensor, torch.Tensor]: <code>Tuple</code> of tensor representing the minimum and maximum thresholds of the quantization range, respectively.</p> <p>Part of fastforward.range_setting.RangeSettable Protocol</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.scale","title":"<code>scale = torch.nn.UninitializedParameter(device=device)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.symmetric","title":"<code>symmetric: bool</code>  <code>property</code>","text":"<p>True if symmetric quantization, False otherwise.</p> <p>Part of fastforward.range_setting.SupportsRangeBasedOperator Protocol</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.bound_quantization_function","title":"<code>bound_quantization_function(tile_size)</code>","text":"<p>Creates a <code>BoundQuantizationFunction</code> using parameters on this module.</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Provide extra repr information on num_bits, symmetric flag and granularities.</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.operator_for_range","title":"<code>operator_for_range(min_range, max_range, data_shape)</code>","text":"<p>Part of fastforward.range_setting.SupportsRangeBasedOperator Protocol.</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.quantize","title":"<code>quantize(data)</code>","text":"<p>Quantize data using linear quantizer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Tensor to quantize</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Quantized data</p>"},{"location":"reference/fastforward/nn/linear_quantizer/#fastforward.nn.linear_quantizer.LinearQuantizer.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Reset parameters to scale=1, offset=0.</p>"},{"location":"reference/fastforward/nn/normalization/","title":"normalization","text":""},{"location":"reference/fastforward/nn/normalization/#fastforward.nn.normalization.QuantizedLayerNorm","title":"<code>fastforward.nn.normalization.QuantizedLayerNorm</code>","text":"<p>               Bases: <code>LayerNorm</code>, <code>QuantizedModule</code></p> <p>Quantized implementation of torch.nn.LayerNorm.</p>"},{"location":"reference/fastforward/nn/normalization/#fastforward.nn.normalization.QuantizedLayerNorm.bias","title":"<code>bias: torch.Tensor | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/normalization/#fastforward.nn.normalization.QuantizedLayerNorm.weight","title":"<code>weight: torch.Tensor | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/normalization/#fastforward.nn.normalization.QuantizedLayerNorm.__init_quantization__","title":"<code>__init_quantization__()</code>","text":""},{"location":"reference/fastforward/nn/normalization/#fastforward.nn.normalization.QuantizedLayerNorm.forward","title":"<code>forward(input)</code>","text":""},{"location":"reference/fastforward/nn/quantized_module/","title":"quantized_module","text":""},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.ModuleConversionDict","title":"<code>fastforward.nn.quantized_module.ModuleConversionDict: TypeAlias = dict[ModuleType, Union[QuantizedModuleType, 'SkipQuantization']]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.ModuleType","title":"<code>fastforward.nn.quantized_module.ModuleType: TypeAlias = type[torch.nn.Module]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModuleType","title":"<code>fastforward.nn.quantized_module.QuantizedModuleType: TypeAlias = type['QuantizedModule']</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.SKIP_QUANTIZATION","title":"<code>fastforward.nn.quantized_module.SKIP_QUANTIZATION = SkipQuantization()</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.logger","title":"<code>fastforward.nn.quantized_module.logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule","title":"<code>fastforward.nn.quantized_module.QuantizedModule</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for quantized neural network models/modules.</p>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule--extending-existing-non-quantized-modules","title":"Extending existing (non-quantized) modules","text":"<p>In many cases, a quantized module will be a specialization of a non-quantized module. <code>QuantizedModule</code> follows an extension principle, i.e., QuantizedModule extends the existing classes by implementing all the quantization related initialization logic in the <code>__init_quantization__()</code> method. I.e., no quantization logic should be implemented in <code>__init__()</code></p> <p>When a quantized module is initialized, first the <code>__init__()</code> of the super classes is called. Once these conclude, <code>__init_quantization__()</code> is executed. Alternatively, one can create an instance of the super class. Update the <code>__class__</code> attribute to the quantized class and call <code>__init_quantization__()</code>. By adhering to this separation of initialization, we can easily convert submodules of a non-quantized neural network to their quantized counterparts.</p> <p>Quantized classes are automatically detected through the subclass hierarchy. For example, a class that subclasses <code>QuantizedModule</code> and <code>torch.nn.Linear</code> is considered a quantized implementation of <code>torch.nn.Linear</code> and will be used in <code>quantized_module</code>. To opt-out from this automatic discovery mechanism, pass <code>include_in_module_map=False</code> as class argument:</p> <pre><code>class MyQuantizedModule(torch.nn.QuantizedModule, include_in_module_map=False):\n    ...\n</code></pre> Note <p>Future versions of this library will include support to automatically generate quantized counterparts. However, until that feature lands, quantized counterparts need to be implemented manually. Quantized implementations for a subset of modules available in PyTorch can be found in the <code>fastforward.nn</code> module.</p>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule--quantizedstubs","title":"<code>QuantizedStub</code>s","text":"<p>A QuantizedModule does not define specific quantizers during initialization. Instead, one or more <code>QuantizerStub</code>s are defined. These may be replaced by explicit quantizers at a later time. This makes <code>QuantizedModule</code>s general over particular quantizers.</p>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule.__init_quantization__","title":"<code>__init_quantization__()</code>","text":"<p>Quantization specific initializer.</p> <p>This method is automatically called when a <code>QuantizedModule</code> is initialized directly, after <code>__init__()</code>.</p>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule.__init_subclass__","title":"<code>__init_subclass__(include_in_module_map=True)</code>","text":"<p>Record <code>QuantizedModule</code> subclasses in module map for model conversion.</p> <p>Parameters:</p> Name Type Description Default <code>include_in_module_map</code> <code>bool</code> <p>If True, the newly created <code>QuantizedModule</code> is recorded in the global module conversion map. This means that the module will be automatically discovered and used for module replacements in <code>ff.quantize_model</code>. If False, this registration is skipped. This can be useful for one-off modules or modules for which you want to provide the mapping explicitly.</p> <code>True</code>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule.__setattr__","title":"<code>__setattr__(name, value)</code>","text":""},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule.named_quantizers","title":"<code>named_quantizers(prefix='', recurse=True, remove_duplicate=True, skip_stubs=True)</code>","text":"<p><code>Iterator</code> over quantizers and their names.</p> <p>Return an iterator over <code>QuantizedModule</code>s in the network, yielding both the name of the quantizer as well ass the quantizer itself. Yields only direct children if <code>recurse</code> is False.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Str that prefixed to the name of the module.</p> <code>''</code> <code>recurse</code> <code>bool</code> <p>Only yield direct children if True, yield all quantizers in submodules otherwise.</p> <code>True</code> <code>remove_duplicate</code> <code>bool</code> <p>Flag indicating whether duplicated should be yielded once or multiple times.</p> <code>True</code> <code>skip_stubs</code> <code>bool</code> <p>Do not yield instances of <code>QuantizerStub</code> if True.</p> <code>True</code> <p>Yields:</p> Type Description <code>(str, QuantizerModule)</code> <p><code>Tuple</code> of name and quantizer</p>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule.quantize_children","title":"<code>quantize_children(extra_conversion=None, skip_quantized_modules=False)</code>","text":"<p>Quantize children.</p> <p>Converts the children modules to their <code>QuantizedModule</code> counterparts defined in <code>fastforward.nn.quantized_module.quantized_module_map</code> and <code>extra_conversion</code>. This may be used as part of <code>__init_quantization__</code> to perform recursive quantization initialization.</p> <p>Parameters:</p> Name Type Description Default <code>extra_conversion</code> <code>ModuleConversionDict | None</code> <p>A dict that maps <code>torch.nn.Module</code> to <code>QuantizedModule</code> subclasses. For any conversion, this dict is first checked. If there is no match, the general mapping as given by <code>quantized_module_map</code> is used.</p> <code>None</code> Warning <p>If this method is used on a module that has submodules for which no mapping is available in both <code>extra_conversion</code> and <code>quantized_module_map</code>, an error is thrown and the conversions fails.</p>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule.quantizers","title":"<code>quantizers(recurse=True, skip_stubs=True)</code>","text":"<p>Iteartor over quantizers.</p> <p>Return an iterator over <code>QuantizedModule</code>s in the network. Yields only direct children if <code>recurse</code> is False.</p> <p>Parameters:</p> Name Type Description Default <code>recurse</code> <code>bool</code> <p>Only yield direct children if True, yield all quantizers in submodules otherwise.</p> <code>True</code> <code>skip_stubs</code> <code>bool</code> <p>Do not yield instances of <code>QuantizerStub</code> if True.</p> <code>True</code>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.QuantizedModule.register_quantizer","title":"<code>register_quantizer(name, quantizer, *, _register_module=True)</code>","text":"<p>Register new quantizer on module.</p> <p>Register a quantizer with under <code>name</code> on this module. This method is similar to register_module, but specific to quantizers. If an attribute is set to a quantizer on a QuantizerModule, this method is called automatically.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the quantizer. It will be exposed under this name as an attribute on the QuantizedModule</p> required <code>quantizer</code> <code>Optional[Quantizer]</code> <p>The quantizer to register.</p> required"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.SkipQuantization","title":"<code>fastforward.nn.quantized_module.SkipQuantization</code>","text":"<p>Marker class to signify that a module must not be quantized.</p>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.SkipQuantization.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.named_quantizers","title":"<code>fastforward.nn.quantized_module.named_quantizers(module, prefix='', recurse=True, remove_duplicate=True, skip_stubs=True)</code>","text":"<p><code>Iterator</code> over quantizers with names.</p> <p>Return an iterator over <code>QuantizedModule</code>s in module, yielding both the name of the quantizer as well ass the quantizer itself. Yields only direct children if <code>recurse</code> is False.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Str that prefixed to the name of the module.</p> <code>''</code> <code>recurse</code> <code>bool</code> <p>Only yield direct children if True, yield all quantizers in submodules otherwise.</p> <code>True</code> <code>remove_duplicate</code> <code>bool</code> <p>Flag indicating whether duplicated should be yielded once or multiple times.</p> <code>True</code> <code>skip_stubs</code> <code>bool</code> <p>Do not yield instances of <code>QuantizerStub</code> if True.</p> <code>True</code> <p>Yields:</p> Type Description <code>(str, QuantizerModule)</code> <p><code>Tuple</code> of name and quantizer</p>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.quantize_model","title":"<code>fastforward.nn.quantized_module.quantize_model(model, recursive=True, extra_conversion=None, skip_quantized_modules=False)</code>","text":"<p>Convert modules and submodules to quantized counterparts.</p> <p>Converts a <code>torch.nn.Module</code> and it's children to their <code>QuantizedModule</code> counterparts defined in <code>fastforward.nn.quantized_module.quantized_module_map</code> and <code>extra_conversion</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to quantize</p> required <code>recursive</code> <code>bool</code> <p>If True, recursively quantize all submodules</p> <code>True</code> <code>extra_conversion</code> <code>Optional[ModuleConversionDict]</code> <p>A mapping from <code>torch.nn.Module</code> subclasses to <code>QuantizedModule</code> subclasses that extends the default mapping returned by <code>quantized_module_map</code>. All (sub)modules in model will be replaced according to this mapping. A module can be marked as <code>do not replace</code> by adding an entry that maps to <code>fastforward.nn.quantized_module.SKIP_QUANTIZATION</code>.</p> <code>None</code> <code>skip_quantized_modules</code> <code>bool</code> <p>If True, do not attempt to replace already quantized modules. If False, quantized modules are treated as any other module and may be replaced if an entry is available in the conversion map or an error is thrown otherwise.</p> <code>False</code>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.quantized_module_map","title":"<code>fastforward.nn.quantized_module.quantized_module_map()</code>","text":"<p>Returns a dictionary that maps <code>torch.nn.Module</code>s to their <code>ff.nn.QuantizedModule</code> counterparts.</p> Warning <p>This finds all known subclasses of <code>ff.nn.QuantizedModule</code>, hence, in order for a particular <code>QuantizedModule</code> subclass to be included it needs to be created.</p>"},{"location":"reference/fastforward/nn/quantized_module/#fastforward.nn.quantized_module.surrogate_quantized_modules","title":"<code>fastforward.nn.quantized_module.surrogate_quantized_modules(model)</code>","text":"<p>Create surrogate quantization modules for prototyping.</p> <p>Construct a <code>ModuleConversionDict</code> that contains surrogate quantized modules of all submodules in <code>model</code> for which no quantized counterpart exists. In this context, a surrogate is a <code>QuantizedModule</code> without any changes to it's forward pass. For example, a surrogate for <code>MyModule</code> looks like:</p> <pre><code>class QuantizedMyModule(QuantizedModule, MyModule, include_in_module_map=False):\n    pass\n</code></pre> <p>These surrogates act as quantized implementations, but perform no real quantization. This is useful to temporarily create placeholder quantized implementations or for quantized implementations of container Modules, i.e., modules which only call submodules.</p> <p>When the result of this function is used as <code>extra_conversion</code> in <code>quantize_model</code> the quantization conversion process will always succeed, however, the resulting model may not be correctly quantized. For example, given our <code>MyModule</code> example, <code>MyModule.forward</code> may perform an operation that should be quantized. Since the forward method is untouched, the resulting model may not be correctly quantizable.</p> <p>Note that when using this function, the created classes are not automatically discovered as with normal <code>QuantizedModule</code> creation. Hence, future calls of <code>quantize_model</code> will not incorrectly discover classes that where created through this function.</p> Warning <p>Caution is advised when using this function. Inspect the result of <code>quantize_model</code> and validate if the created model matches your expectations. All quantized modules introduces by using this function have a type name that ends in 'Surrogate'.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p><code>Model</code> for which to create placeholder <code>QuantizedModule</code> implementations</p> required <p>Returns:</p> Type Description <code>ModuleConversionDict</code> <p>a <code>ModuleConversionDict</code> that can be used as <code>extra_conversion</code> in <code>quantize_model</code></p>"},{"location":"reference/fastforward/nn/quantizer/","title":"quantizer","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.default_tags","title":"<code>fastforward.nn.quantizer.default_tags = SimpleNamespace(parameter_quantizer=_parameter_quantizer, activation_quantizer=_activation_quantizer, weight_quantizer=_weight_quantizer, bias_quantizer=_bias_quantizer, input_quantizer=_input_quantizer, output_quantizer=_output_quantizer)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Quantizer","title":"<code>fastforward.nn.quantizer.Quantizer()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for Quantizers.</p> <p>Initialize the Quantizer.</p> <p>This sets up the quantizer overrides as an ordered dictionary and initializes the quantizer metadata to None.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Quantizer.__call__","title":"<code>__call__: Callable[..., torch.Tensor]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Quantizer.quant_metadata","title":"<code>quant_metadata: QuantizerMetadata | None = None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Quantizer.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Extra representation of the quantizer.</p> <p>This includes information about the registered overrides.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The extra representation string.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Quantizer.forward","title":"<code>forward(data)</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Quantizer.is_stub","title":"<code>is_stub()</code>","text":"<p>Returns: False, indicating this is not a stub.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Quantizer.quantize","title":"<code>quantize(data)</code>","text":"<p>Quantize the input data.</p> <p>This method should be overridden by subclasses to implement the actual quantization logic.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input tensor to be quantized.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The quantized tensor.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Quantizer.register_override","title":"<code>register_override(override_fn)</code>","text":"<p>Push a quantizer override on the module.</p> <p>This override will be called instead of <code>self.quantizer</code>. The quantizer and self.quantizer is passed into the overwrite and can be called. If multiple overrides are registered, calling the overwriten function from an override will trigger the 'next' override.</p> <p>Parameters:</p> Name Type Description Default <code>override_fn</code> <code>OverrideFn[Tensor]</code> <p>The function override for quantize.</p> required <p>Returns:</p> Type Description <code>OverrideHandle</code> <p>(override.OverrideHandle) a handle that can be used to remove the</p> <code>OverrideHandle</code> <p>pushed override.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata","title":"<code>fastforward.nn.quantizer.QuantizerMetadata(*tags, weight_quantizer=False, bias_quantizer=False, input_quantizer=False, output_quantizer=False, shape=None, **kwargs)</code>","text":"<p>Metadata class for quantizers, holding tags and additional attributes.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>str | Tag</code> <p>Tags to be added to the metadata.</p> <code>()</code> <code>weight_quantizer</code> <code>bool</code> <p>Whether to add the weight quantizer tag.</p> <code>False</code> <code>bias_quantizer</code> <code>bool</code> <p>Whether to add the bias quantizer tag.</p> <code>False</code> <code>input_quantizer</code> <code>bool</code> <p>Whether to add the input quantizer tag.</p> <code>False</code> <code>output_quantizer</code> <code>bool</code> <p>Whether to add the output quantizer tag.</p> <code>False</code> <code>shape</code> <code>Optional[tuple[int, ...] | Size]</code> <p>The shape attribute.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional attributes.</p> <code>{}</code>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.activation_quantizer","title":"<code>activation_quantizer = _TagAttribute(_activation_quantizer)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.bias_quantizer","title":"<code>bias_quantizer = _TagAttribute(_bias_quantizer)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.input_quantizer","title":"<code>input_quantizer = _TagAttribute(_input_quantizer)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.output_quantizer","title":"<code>output_quantizer = _TagAttribute(_output_quantizer)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.parameter_quantizer","title":"<code>parameter_quantizer = _TagAttribute(_parameter_quantizer)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.shape","title":"<code>shape: tuple[int, ...] | torch.Size | None</code>  <code>property</code>","text":"<p>Return the shape metadata, if set. Otherwise return None.</p> <p>Returns:</p> Type Description <code>tuple[int, ...] | Size | None</code> <p>tuple[int, ...] | torch.Size | None: The shape attribute.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.weight_quantizer","title":"<code>weight_quantizer = _TagAttribute(_weight_quantizer)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.__contains__","title":"<code>__contains__(tag)</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.__getattr__","title":"<code>__getattr__(key)</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.add_tag","title":"<code>add_tag(tag)</code>","text":"<p>Add a tag to the metadata.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>Tag | str</code> <p>The tag to be added.</p> required"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.is_extension","title":"<code>is_extension(other)</code>","text":"<p>Returns True if other is an extension of self, False otherwise.</p> <p>metadata B is an extension of metadata A if B's tag set is a superset of A's tag set and all attributes of A match with the corresponding attributes of B. That is, B may have more tags and/or attributes, but they can not conflict with those of A.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Self</code> <p>Metadata to check if it is an extension of self</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Boolean indicating if other is an extension of self</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerMetadata.to_stub","title":"<code>to_stub()</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerStub","title":"<code>fastforward.nn.quantizer.QuantizerStub(*tags, weight_quantizer=False, bias_quantizer=False, input_quantizer=False, output_quantizer=False, shape=None, __metadata=None, **kwargs)</code>","text":"<p>               Bases: <code>Quantizer</code></p> <p>Stub class for Quantizers.</p> <p>Used for Quantizer/Quantized network initialization.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>str | Tag</code> <p>Tags to be added to the metadata.</p> <code>()</code> <code>weight_quantizer</code> <code>bool</code> <p>Whether to add the weight quantizer tag.</p> <code>False</code> <code>bias_quantizer</code> <code>bool</code> <p>Whether to add the bias quantizer tag.</p> <code>False</code> <code>input_quantizer</code> <code>bool</code> <p>Whether to add the input quantizer tag.</p> <code>False</code> <code>output_quantizer</code> <code>bool</code> <p>Whether to add the output quantizer tag.</p> <code>False</code> <code>shape</code> <code>Optional[tuple[int, ...] | Size]</code> <p>The shape attribute.</p> <code>None</code> <code>__metadata</code> <code>Optional[QuantizerMetadata]</code> <p>If provided, use as metadata and ignore all other arguments.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional attributes.</p> <code>{}</code>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerStub.quant_metadata","title":"<code>quant_metadata: QuantizerMetadata</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerStub.is_stub","title":"<code>is_stub()</code>","text":"<p>Check if this quantizer is a stub.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True, indicating this is a stub.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.QuantizerStub.quantize","title":"<code>quantize(data)</code>","text":"<p>Stub quantize method that returns the input data unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The same input tensor, unchanged.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Tag","title":"<code>fastforward.nn.quantizer.Tag</code>","text":"<p>Tags are symbol-like objects used to communicate features of a quantizer.</p> <p>A tag can be hierarchical using '/' to separate the tag levels. A hierarchical tag can also be constructed using <code>tag / tag</code>, which produces a new tag.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Tag.__new__","title":"<code>__new__(symbol)</code>","text":"<p>Create a new Tag instance or return an existing one if it already exists.</p> <p>Parameters:</p> Name Type Description Default <code>symbol</code> <code>str | Self</code> <p>The symbol representing the tag.</p> required <p>Returns:</p> Name Type Description <code>Tag</code> <code>Tag</code> <p>The Tag instance.</p>"},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Tag.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Tag.__rtruediv__","title":"<code>__rtruediv__(lhs)</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Tag.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Tag.__truediv__","title":"<code>__truediv__(rhs)</code>","text":""},{"location":"reference/fastforward/nn/quantizer/#fastforward.nn.quantizer.Tag.hierarchy","title":"<code>hierarchy()</code>","text":"<p>Return all tags in the hierarchy.</p> <p>For example, if the tag is <code>first/second/third</code>, this function will yield a tag for <code>first</code>, <code>first/second</code>, and <code>first/second/third</code>.</p> <p>Returns:</p> Type Description <code>Iterator[Self]</code> <p>An iterator over the tags in the hierarchy.</p>"},{"location":"reference/fastforward/nn/functional/","title":"functional","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.add","title":"<code>fastforward.nn.functional.add(input, other, alpha=1.0, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.avg_pool1d","title":"<code>fastforward.nn.functional.avg_pool1d(input, kernel_size, stride, padding=0, ceil_mode=False, count_include_pad=True, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.avg_pool2d","title":"<code>fastforward.nn.functional.avg_pool2d(input, kernel_size, stride, padding=0, ceil_mode=False, count_include_pad=True, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.avg_pool3d","title":"<code>fastforward.nn.functional.avg_pool3d(input, kernel_size, stride, padding=0, ceil_mode=False, count_include_pad=True, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.bmm","title":"<code>fastforward.nn.functional.bmm(input, mat2, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.cat","title":"<code>fastforward.nn.functional.cat(tensors, dim=0, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.conv1d","title":"<code>fastforward.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.conv2d","title":"<code>fastforward.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.conv3d","title":"<code>fastforward.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.conv_transpose1d","title":"<code>fastforward.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, dilation=1, groups=1, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.conv_transpose2d","title":"<code>fastforward.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, dilation=1, groups=1, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.conv_transpose3d","title":"<code>fastforward.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, dilation=1, groups=1, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.cumsum","title":"<code>fastforward.nn.functional.cumsum(input, dim, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.div","title":"<code>fastforward.nn.functional.div(input, other, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.dropout","title":"<code>fastforward.nn.functional.dropout(input, p=0.5, training=True, inplace=False, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.embedding","title":"<code>fastforward.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.gelu","title":"<code>fastforward.nn.functional.gelu(input, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.index_add","title":"<code>fastforward.nn.functional.index_add(input, dim, index, source, alpha=1, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.layer_norm","title":"<code>fastforward.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.linear","title":"<code>fastforward.nn.functional.linear(input, weight, bias=None, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.matmul","title":"<code>fastforward.nn.functional.matmul(input, other, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.mm","title":"<code>fastforward.nn.functional.mm(input, mat2, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.mul","title":"<code>fastforward.nn.functional.mul(input, other, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.permute","title":"<code>fastforward.nn.functional.permute(input, dims, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.relu","title":"<code>fastforward.nn.functional.relu(input, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.scaled_dot_product_attention","title":"<code>fastforward.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.sigmoid","title":"<code>fastforward.nn.functional.sigmoid(input, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.silu","title":"<code>fastforward.nn.functional.silu(input, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.softmax","title":"<code>fastforward.nn.functional.softmax(input, dim, dtype=None, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/nn/functional/#fastforward.nn.functional.sum","title":"<code>fastforward.nn.functional.sum(input, dim=None, *, output_quantizer=None, strict_quantization=None)</code>","text":""},{"location":"reference/fastforward/quantization/","title":"quantization","text":""},{"location":"reference/fastforward/quantization/affine/","title":"affine","text":""},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.TiledAffineQuantizationFunction","title":"<code>fastforward.quantization.affine.TiledAffineQuantizationFunction()</code>","text":"<p>               Bases: <code>QuantizationAutogradFunction</code></p> <p>QuantizationFunction for affine quantization.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.TiledAffineQuantizationFunction.bind","title":"<code>bind(scale, tile_size, num_bits, output_dtype, offset=None)</code>  <code>classmethod</code>","text":"<p>Create a bound quantization function for affine quantization given the provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>Tensor</code> <p>Quantization scale</p> required <code>tile_size</code> <code>SizeT | Literal['data_shape']</code> <p>Tile size that determines quantizations grouping</p> required <code>num_bits</code> <code>int</code> <p>Number of bits for quantization</p> required <code>output_dtype</code> <code>dtype | None</code> <p>Dtype used for quantized representation</p> required <code>offset</code> <code>Optional[Tensor]</code> <p>Quantization offset</p> <code>None</code> <p>Returns:</p> Type Description <code>BoundQuantizationFunction</code> <p><code>BoundQuantizationFunction</code> that captures the quantization using the</p> <code>BoundQuantizationFunction</code> <p>provided arguments.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.TiledAffineQuantizationFunction.dequantize","title":"<code>dequantize(quant_data, scale, tile_size, num_bits, output_dtype, offset=None)</code>  <code>staticmethod</code>","text":"<p>Dequantize tensor using a tiled linear operator.</p> <p>Take integer representation of quantized data and return real-valued (float) quantized data.</p> <p>Parameters:</p> Name Type Description Default <code>quant_data</code> <code>Tensor</code> <p>Integer data to dequantize</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Quantized real-valued representation of quant_data</p> Note <p>Dequantization is used colloquially, i.e., it is not the inverse of the quantize operation, but takes a quantized integer representation and represent the quantized data as real-valued. The returned data still folows a quantization grid.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.TiledAffineQuantizationFunction.quant_dequant_backward","title":"<code>quant_dequant_backward(ctx, grad, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Combined quantize and dequantize backward implementation.</p> <p>Quantize tensor using a tiled linear operator, for performing the backward pass. The function returns the gradient values and graph settings.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.TiledAffineQuantizationFunction.quantize","title":"<code>quantize(ctx, data, scale, tile_size, num_bits, output_dtype, offset=None)</code>  <code>staticmethod</code>","text":"<p>Quantize tensor unsing tiled linear operator.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.integer_maximum","title":"<code>fastforward.quantization.affine.integer_maximum(num_bits)</code>","text":"<p>Return the maximum integer value given num_bits.</p> <p>Parameters:</p> Name Type Description Default <code>num_bits</code> <code>float</code> <p>Number of bits</p> required <p>Returns:     float: Maximum integer value supporting the quantization range</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.integer_minimum","title":"<code>fastforward.quantization.affine.integer_minimum(num_bits)</code>","text":"<p>Return the minimum integer value given num_bits.</p> <p>Parameters:</p> Name Type Description Default <code>num_bits</code> <code>float</code> <p>Number of bits</p> required <p>Returns:     float: Minimum integer value supporting the quantization range</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.parameters_for_range","title":"<code>fastforward.quantization.affine.parameters_for_range(min_range, max_range, num_bits, symmetric, allow_one_sided)</code>","text":"<p>Compute affine quantization parameters for a range.</p> <p>Given a range or ranges (if min_range and max_range are multidimensional), compute the scale and offset parameters that best represent that the given range(s)</p> <p>Parameters:</p> Name Type Description Default <code>min_range</code> <code>Tensor</code> <p><code>Tensor</code> representing the minimum range threshold</p> required <code>max_range</code> <code>Tensor</code> <p><code>Tensor</code> representing the maximum range threshold</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>tuple[torch.Tensor, torch.Tensor | None]: scale and offset values that best represent the given range. Offset may be None in the non-onesided symmetric case.</p> <p>Notes::     - The dimensionality of min_range and max_range must match.     - If symmetric == True, not every range can be represented exactly,         in that case, the scale and offset parameters are selected such         that the entire given range is within bounds, i.e., the range         used by <code>LinearQuantizer</code> may be wider and no assumptions on using         the given range exactly must be made.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.quantization_range","title":"<code>fastforward.quantization.affine.quantization_range(scale, offset, num_bits)</code>","text":"<p>Compute quantization range for a set of quantization parameters.</p> <p>If both scale and offset are tensors, their dimensions must match.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>Tensor | float</code> <p>Scale for quantization</p> required <code>offset</code> <code>Tensor | float | None</code> <p>Offset for quantization</p> required <code>num_bits</code> <code>float</code> <p>Number of bits used for quantization</p> required <p>Returns:</p> Type Description <code>tuple[Tensor | float, Tensor | float]</code> <p>tuple[torch.Tensor, torch.Tensor]: <code>Tuple</code> of tensor representing the minimum and maximum thresholds of the quantization range, respectively.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.quantize_by_tile","title":"<code>fastforward.quantization.affine.quantize_by_tile(input, scale, offset, tile_size, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The input variables are passed to a Function class, which immediately returns the quantized tensor.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.quantize_by_tile_function","title":"<code>fastforward.quantization.affine.quantize_by_tile_function(scale, offset, tile_size, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The input variables are bound to a Function class, which can then be invoked multiple times.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.quantize_per_block","title":"<code>fastforward.quantization.affine.quantize_per_block(input, scale, offset, channel_axis, block_axis, block_size, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The tile size is calculated based on the input channel axis, block axis and block size, so the quantization parameters are project per user defined block.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.quantize_per_channel","title":"<code>fastforward.quantization.affine.quantize_per_channel(input, scale, offset, axis, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The tile size is calculated based on the input axis, so the quantization parameters are project per channel.</p>"},{"location":"reference/fastforward/quantization/affine/#fastforward.quantization.affine.quantize_per_tensor","title":"<code>fastforward.quantization.affine.quantize_per_tensor(input, scale, offset=None, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The tile size is automatically assigned to the data shape, so the function applies the provided scale and offset on the entirety of the tensor.</p>"},{"location":"reference/fastforward/quantization/dynamic/","title":"dynamic","text":""},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.TiledDynamicAffineQuantizationFunction","title":"<code>fastforward.quantization.dynamic.TiledDynamicAffineQuantizationFunction()</code>","text":"<p>               Bases: <code>QuantizationAutogradFunction</code></p> <p>QuantizationFunction that implements dynamic affine quantization.</p>"},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.TiledDynamicAffineQuantizationFunction.bind","title":"<code>bind(tile_size, num_bits, output_dtype)</code>  <code>classmethod</code>","text":"<p>Construct a <code>BoundQuantizationFunction</code> for TiledDynamicAffineQuantizationFunction.</p>"},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.TiledDynamicAffineQuantizationFunction.dequantize","title":"<code>dequantize(quant_data, scale, offset, tile_size, num_bits)</code>  <code>staticmethod</code>","text":"<p>Dequantize quantized data.</p> <p>Dequantize tensor using a tiled linear operator. I.e., take integer representation of quantized data and return real-valued (float) quantized data.</p> <p>Parameters:</p> Name Type Description Default <code>quant_data</code> <code>Tensor</code> <p>Integer data to dequantize</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Quantized real-valued representation of quant_data</p> Note <p>Dequantization is used colloquially, i.e., it is not the inverse of the quantize operation, but takes a quantized integer representation and represent the quantized data as real-valued. The returned data still folows a quantization grid.</p>"},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.TiledDynamicAffineQuantizationFunction.quant_dequant_backward","title":"<code>quant_dequant_backward(ctx, grad, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Combined backward pass for dequantize(quantize(x)).</p> <p>Since the quantization parameters are selected such to no clipping occurs, the entire backward pass is equivalent to a STE backward.</p>"},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.TiledDynamicAffineQuantizationFunction.quantize","title":"<code>quantize(ctx, data, tile_size, num_bits, output_dtype)</code>  <code>staticmethod</code>","text":"<p>Quantize tensor unsing tiled linear operator.</p>"},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.quantize_by_tile","title":"<code>fastforward.quantization.dynamic.quantize_by_tile(input, tile_size, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The input variables are passed to a Function class, which immediately returns the quantized tensor.</p>"},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.quantize_by_tile_function","title":"<code>fastforward.quantization.dynamic.quantize_by_tile_function(tile_size, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The input variables are bound to a Function class, which can then be invoked multiple times.</p>"},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.quantize_per_block","title":"<code>fastforward.quantization.dynamic.quantize_per_block(input, channel_axis, block_axis, block_size, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The tile size is calculated based on the input channel axis, block axis and block size, so the quantization parameters are project per user defined block.</p>"},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.quantize_per_channel","title":"<code>fastforward.quantization.dynamic.quantize_per_channel(input, axis, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The tile size is calculated based on the input axis, so the quantization parameters are project per channel.</p>"},{"location":"reference/fastforward/quantization/dynamic/#fastforward.quantization.dynamic.quantize_per_tensor","title":"<code>fastforward.quantization.dynamic.quantize_per_tensor(input, num_bits=8, output_dtype=None)</code>","text":"<p>Quantize data  using a tiled linear operator.</p> <p>The tile size is automatically assigned to the data shape, so the function applies the provided scale and offset on the entirety of the tensor.</p>"},{"location":"reference/fastforward/quantization/function/","title":"function","text":"<p><code>QuantizationFunction</code> implement the logic for quantization and dequantization.</p> <p>In neural network quantization, the quantization operation is often decomposed into a <code>quantize</code> and <code>dequantize</code> operation. The first maps a real valued input to an integer, whereas the latter is a mapping from integers back to the reals. When combined (i.e., dequantize o quantize) we obtain an R -&gt; R quantization mapping.</p> <p>In FastForward, a quantized tensor associates integer data obtained from the <code>quantize</code> operation with the quantization parameters that specify the quantization function. As such, from a representation point of view, a quantized tensor represents the same real-valued data as a dequantized tensor. In other words, the <code>dequantize</code> operation on a quantized tensor is only a change in representation, not in value.</p> <p>The assumption in the previous paragraph has two consequences for autograd:</p> <pre><code>1. The creation of a `QuantizedTensor` is an implicit dequantize. Hence,\n   the backward pass mus take this into account.\n2. Dequantize is an identity operation through the lens of autograd.\n</code></pre> <p>This module offers abstractions to implement the quantize/dequantize operation in a way that follows the assumptions. There are two options:</p> <pre><code>1. `QuantizationFunction`: implement `quantize` and `dequantize` operations\n   seperately. The resulting quantization function will perform the\n   `quantize` function during the forward pass. During the backward pass\n   both the forward and backward for `dequantize` is performed to ensure the\n   correct gradient is computed. This may lead to some extra overhead when\n   the tensor was already dequantized during the forward pass. The benefit\n   of this option is that only `quantize` and `dequantize` have to be\n   implemented, the backward pass is implicit through standard autograd.\n\n2. `QuantizationAutogradFunction`: next to `quantize` and `dequantize`, a\n   `quant_dequant_backward` must be implemented. This latter implements the\n   backward pass for `quantize` and `dequantize` jointly. Note that, since\n   only `quantize` is guaranteed to have been called in the forward pass,\n   any tensor that must be saved for the backward pass, must be saved during\n   the `quantize` operation. The `quantize` and `quant_dequant_backward`\n   implementation are used to create a standard `torch.autograd.Function`\n   subclass. As such, next to the quantization arguments, they must accept\n   a context argument. The main benefit of this option is that, depending\n   on the implementation, it may have some performance benefits over the\n   implicit gradient of the other option.\n</code></pre> <p>For an example implementation, see <code>fastforward.quantization.affine</code>.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.T","title":"<code>fastforward.quantization.function.T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.Ts","title":"<code>fastforward.quantization.function.Ts = TypeVarTuple('Ts')</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BaseQuantizationFunction","title":"<code>fastforward.quantization.function.BaseQuantizationFunction()</code>","text":"<p>Base class for Quantization functions.</p> <p>Users should not directly subclass BaseQuantizationFunction but subclass QuantizationFunction or QuantizationAutogradFunction instead.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BaseQuantizationFunction.apply","title":"<code>apply(data, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Apply Quantization function to data with given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Data to quantize</p> required <code>args</code> <code>Any</code> <p>Arguments used in quantize/dequantize function.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments used in quantize/dequantize function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>QuantizedTensor</code> <code>QuantizedTensor</code> <p>Quantized data</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BaseQuantizationFunction.attach","title":"<code>attach(data, *args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>QuantizedTensor</code> that is associated with this <code>QuantizationFunction</code> and given data.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BaseQuantizationFunction.bind","title":"<code>bind(*args, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct a <code>BoundQuantizationFunction</code> for this QuantizationFuncion.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BaseQuantizationFunction.dequantize","title":"<code>dequantize(data, *args)</code>  <code>staticmethod</code>","text":"<p>Dequantize data using args.</p> <p>Data represents quantized data. The representation of this data (i.e., datatype) depends on the the implementation of <code>_quantize_forward</code>. All arguments that are provided the the quantize operations are provided to the <code>dequantize</code> operation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Raw quantized data</p> required <code>args</code> <code>Any</code> <p>All quantization arguments</p> <code>()</code>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction","title":"<code>fastforward.quantization.function.BoundQuantizationFunction(quant_func, *args)</code>","text":"<p>Container that associated a QuantizationFunction with arguments.</p> <p>Parameters:</p> Name Type Description Default <code>quant_func</code> <code>type[BaseQuantizationFunction]</code> <p>The quantization function to which arguments are bound</p> required <code>args</code> <code>Any</code> <p>Arguments that will be associated with quant_func</p> <code>()</code>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.args","title":"<code>args: tuple[Any, ...] = args</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.quant_func","title":"<code>quant_func: type[BaseQuantizationFunction] = quant_func</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.__call__","title":"<code>__call__(data)</code>","text":"<p>Quantize data and return a QuantizedTensor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Data to quantize</p> required <p>Returns:</p> Type Description <code>QuantizedTensor</code> <p>QuantizedTensor</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.__format__","title":"<code>__format__(format_spec)</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.arguments","title":"<code>arguments()</code>","text":"<p>Returns the arguments bound to self.quant_func.</p> <p>Returns:</p> Name Type Description <code>QuantArgs</code> <code>QuantArgs</code> <p>Arguments bound to this function.</p> Note <p><code>QuantArgs</code> can be mutated without it having an effect on self, but all reference types on <code>QuantArgs</code> are shared and mutation will have side effects.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.attach","title":"<code>attach(data, *dynamic_args)</code>","text":"<p>Create a quantized tensor that associates this <code>BoundQuantizationFunction</code> with data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The raw data for the quantized tensor</p> required"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.clone","title":"<code>clone()</code>","text":"<p>Create a new <code>BoundQuantizationFunction</code> in which all tensor arguments are cloned.</p> <p>Returns:</p> Name Type Description <code>BoundQuantizationFunction</code> <code>Self</code> <p>Bound function that is equivalent to self, but</p> <code>Self</code> <p>all tensor arguments are cloned.</p> Note <p>All non-Tensor arguments are left unchanged and uncopied. Hence, any argument that references an object is shared between the output and self.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.contiguous","title":"<code>contiguous()</code>","text":"<p>Convert any non-contiguous tensor parameter into a contiguous parameter.</p> <p>This may result in a memory copy and the creation of new BoundQuantizationFunction. If all tensor parameters are already contiguous, do nothing and return self.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.dequantize","title":"<code>dequantize(quantized)</code>","text":"<p>Helper function to apply dequantize implentation of the associated QuantizationFunction.</p> A <code>QuantizedTensor</code> and its dequantized floating point value <p>represent the same real-valued data. As such, from the autograd perspective, the Jocabion of the dequantize operation is an identity matrix. This does not mean that dequantize is not ignored during the backward pass, but its backward pass is executed as part of the 'quantize' backward pass. See the docstring in <code>fastforward.quantization.function</code> for more information.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.detach_arguments","title":"<code>detach_arguments()</code>","text":"<p>Create a new <code>BoundQuantizationFunction</code> in which all tensor arguments are detached.</p> <p>Returns:</p> Name Type Description <code>BoundQuantizationFunction</code> <code>Self</code> <p>Bound function that is equivalent to self, but</p> <code>Self</code> <p>all tensor arguments are detached.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.quantize","title":"<code>quantize(data)</code>","text":"<p>Quantize data and return a QuantizedTensor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Data to quantize</p> required <p>Returns:</p> Type Description <code>QuantizedTensor</code> <p>QuantizedTensor</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.rebind","title":"<code>rebind(**kwargs)</code>","text":"<p>Create a new <code>BoundQuantizationFunction</code> and overwrite the newly provided arguments.</p> <p>All unprovided arguments are left unchanged. An error is raised if an argument is provided that is unknown to the QuantizationFunction.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Any new quantization argument</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BoundQuantizationFunction</code> <code>Self</code> <p>The new bound function</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.rebind_and_attach","title":"<code>rebind_and_attach(quantized_tensor, **kwargs)</code>","text":"<p>Rebind quantization function and immediatly attach to an existing quantized tensor.</p> <p>This will raise an error if <code>quantized_tensor</code> is not already quantized by the same QuantizationFunction.</p> <p>Parameters:</p> Name Type Description Default <code>quantized_tensor</code> <code>QuantizedTensor</code> <p>Existing quantized tensor that is reassoacited with an updated QuantizationFunction.</p> required <code>kwargs</code> <code>Any</code> <p>Any new quantization argument</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>QuantizedTensor</code> <code>QuantizedTensor</code> <p>The newly created quantized tensor that is quantized according to the newly created BoundQuantizationFunction.</p> Note <p>This only changes the quantization parameters and not the quantized data. Moreover, the data of the output quantized tensor is shared with the input.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.BoundQuantizationFunction.to","title":"<code>to(device)</code>","text":"<p>Create a new <code>BoundQuantizationFunction</code> in which all tensor arguments are moved to <code>device</code>.</p> <p>Returns:</p> Type Description <code>Self</code> <p>quantization function: bound function that is equivalent to self, but all tensor arguments are moved to <code>device</code>.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantArgs","title":"<code>fastforward.quantization.function.QuantArgs(**kwargs)</code>","text":"<p>               Bases: <code>Mapping[str, Any]</code></p> <p>QuantArgs is a str to value mapping for quantization arguments.</p> <p>The mapping is mutable and will not affect the quantized tensor or BoundQuantizationFunction from which QuantArgs was obtained. However, values may be shared with <code>BoundQuantizationFunction</code>s or <code>QuantizedTensor</code>s, hence, changes values inplace may have an effect outside of the QuantArgs instance.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantArgs.__format__","title":"<code>__format__(format_spec)</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantArgs.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantArgs.__getitem__","title":"<code>__getitem__(name)</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantArgs.__iter__","title":"<code>__iter__()</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantArgs.__len__","title":"<code>__len__()</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantArgs.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantArgs.__setattr__","title":"<code>__setattr__(name, value)</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantizationAutogradFunction","title":"<code>fastforward.quantization.function.QuantizationAutogradFunction()</code>","text":"<p>               Bases: <code>BaseQuantizationFunction</code></p> <p>QuantizationFunction with an explicit autograd implementation.</p> <p>The autograd 'step' is implemented for the quantize and dequantize function jointly. A torch autograd function is created from the quantize and quant_dequant_backward functions. Dequantize is only used if the resulting tesnor is explcitly dequantized.</p> Dequantize must accept the exact same arguments as quantize, except <p>ctx. This is not enforced, but will violating this contract will result in undefined behaviour.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantizationAutogradFunction.__init_subclass__","title":"<code>__init_subclass__(*args, **kwargs)</code>","text":""},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantizationAutogradFunction.dequantize","title":"<code>dequantize(data, *args)</code>  <code>staticmethod</code>","text":"<p>Dequantize data using given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Data to dequantize. The value of this tensor is equal to the output of quantize.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: dequantized data</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantizationAutogradFunction.quant_dequant_backward","title":"<code>quant_dequant_backward(ctx, grad)</code>  <code>staticmethod</code>","text":"<p>Implementation of the backward pass for both quantize and dequantize jointly.</p> <p>Accepts exactly one gradient arguments, and returns one gradient tensor or None for each argument to quantize.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantizationAutogradFunction.quantize","title":"<code>quantize(ctx, data, *args)</code>  <code>staticmethod</code>","text":"<p>Quantize data using given arguments.</p> <p>The output of this function represents the data of a quantized tensor. The argumnets provided to this function are associated with the <code>QuantizedTensor</code> seperarely.</p> <p>Returns:</p> Name Type Description <code>ctx</code> <code>Tensor</code> <p>Torch autograd context</p> <code>Tensor</code> <p>torch.Tensor: quantized data</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantizationFunction","title":"<code>fastforward.quantization.function.QuantizationFunction()</code>","text":"<p>               Bases: <code>BaseQuantizationFunction</code></p> <p>Implementation of BaseQuantizationFunction.</p> <p>The backward pass for quantize/dequantize is implicit.</p> The arguments of the quantize and dequantize implementation must be <p>the same. This is not enforced, but will violating this contract will result in undefined behaviour.</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantizationFunction.dequantize","title":"<code>dequantize(data, *args)</code>  <code>staticmethod</code>","text":"<p>Dequantize data using given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Data to dequantize. The value of this tensor is equal to the output of quantize.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: dequantized data</p>"},{"location":"reference/fastforward/quantization/function/#fastforward.quantization.function.QuantizationFunction.quantize","title":"<code>quantize(data, *args)</code>  <code>staticmethod</code>","text":"<p>Quantize data using given arguments.</p> <p>The output of this function represents the data of a quantized tensor. The arguments provided to this function are associated with the <code>QuantizedTensor</code> separately.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: quantized data</p>"},{"location":"reference/fastforward/quantization/granularity/","title":"granularity","text":""},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.Granularity","title":"<code>fastforward.quantization.granularity.Granularity</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Granularity represents how paraameters are shared during quantization.</p> <p>Granularities provide an abstraction used for element-wise operations with shared parameters. These are most prominently used in quantizers. For example, when using per-channel quantization, the same quantization parameters are used for each element in the input tensor that are in the same channel.</p> <p>Because FastForward performs quantization based on input tiles, granularities are just used to provide the user with a simple class-based approach of retrieving the appropriate tile sizes for performing per tensor and per channel quantization.</p> <p>The usage for the granularities subclass is as follows:</p> <pre><code>&gt; data = torch.randn(2, 4, 6)\n&gt; gr = some_granularity(...)\n&gt; tile_size = gr.tile_size(data.shape)\n</code></pre> <p>The found tile size can then be used as input for the <code>quantize_by_tile</code> and <code>quantize_by_tile_function</code> methods from <code>fastforward.quantization.affine</code> like so:</p> <pre><code>&gt; scale = 1.4\n&gt; offset = 0.\n&gt; num_bits = 3\n&gt; quantize_by_tile(scale, offset, tile_size, num_bits)\n</code></pre> <p>A more involved usage of the Granularity class can be found in the <code>LinearQuantizer</code> class in FastForward (<code>fastforward.nn.linear_quantizer</code>).</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.Granularity.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.Granularity.parameter_dimensionality","title":"<code>parameter_dimensionality(data_shape)</code>","text":"<p>The dimensionality of the quantizer parameters  to porcess the input data.</p> <p>This is computed by dividing the number of elements of the original data shape over the number of elements of the tile size. For example, for PerTensor quantization the tile size is the same as the data shape, applying parameters with a dimensionality of 1.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.Granularity.repr_args","title":"<code>repr_args()</code>","text":"<p>Return a dictionary of arguments for the repr method.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary of arguments.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.Granularity.tile_size","title":"<code>tile_size(data_shape)</code>  <code>abstractmethod</code>","text":"<p>Retrieve the tile size that will be applied over the input data.</p> <p>The tile functions (fastforward.quantization.affine) brodcast the dimensions with a size of 1 over the data shape. For example, given an input data shape of (2, 4, 6) and the requested granularity is per channel quantization over dimension zero, the tile size will be (1, 4, 6) setting data_shape[0] = 1.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerChannel","title":"<code>fastforward.quantization.granularity.PerChannel(channel_dim=0)</code>","text":"<p>               Bases: <code>Granularity</code></p> <p>Granularity class for per-channel quantization.</p> <p>Attributes:</p> Name Type Description <code>channel_dims</code> <p>The dimensions to apply per-channel quantization.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerChannel.channel_dims","title":"<code>channel_dims = channel_dim</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerChannel.repr_args","title":"<code>repr_args()</code>","text":"<p>Return a dictionary of arguments for the repr method.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerChannel.tile_size","title":"<code>tile_size(data_shape)</code>","text":"<p>Return the tile size for per-channel quantization.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape</code> <code>Size</code> <p>The shape of the input data.</p> required <p>Returns:</p> Type Description <code>Size</code> <p>The tile size for per-channel quantization.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerTensor","title":"<code>fastforward.quantization.granularity.PerTensor()</code>","text":"<p>               Bases: <code>Granularity</code></p> <p>Granularity class for per-tensor quantization.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerTensor.tile_size","title":"<code>tile_size(data_shape)</code>","text":"<p>Return the tile size for per-tensor quantization.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape</code> <code>Size</code> <p>The shape of the input data.</p> required <p>Returns:</p> Type Description <code>Literal['data_shape']</code> <p>The tile size, which is the same as the data shape.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerTile","title":"<code>fastforward.quantization.granularity.PerTile(tile_shape)</code>","text":"<p>               Bases: <code>Granularity</code></p> <p>Granularity class for per-tile quantization.</p> <p>Attributes:</p> Name Type Description <code>tile_shape</code> <p>The shape of the tile.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerTile.tile_shape","title":"<code>tile_shape = torch.Size(tile_shape)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerTile.repr_args","title":"<code>repr_args()</code>","text":"<p>Return a dictionary of arguments for the repr method.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.PerTile.tile_size","title":"<code>tile_size(data_shape)</code>","text":"<p>Return the tile size for per-tile quantization.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape</code> <code>Size</code> <p>The shape of the input data.</p> required <p>Returns:</p> Type Description <code>Size</code> <p>The tile size for per-tile quantization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the tile shape is not compatible with the data shape.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.is_per_channel","title":"<code>fastforward.quantization.granularity.is_per_channel(granularity)</code>","text":"<p>Check if the granularity is per-channel.</p> <p>Parameters:</p> Name Type Description Default <code>granularity</code> <code>Granularity</code> <p>The granularity instance.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the granularity is per-channel, False otherwise.</p>"},{"location":"reference/fastforward/quantization/granularity/#fastforward.quantization.granularity.is_per_tensor","title":"<code>fastforward.quantization.granularity.is_per_tensor(granularity)</code>","text":"<p>Check if the granularity is per-tensor.</p> <p>Parameters:</p> Name Type Description Default <code>granularity</code> <code>Granularity</code> <p>The granularity instance.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the granularity is per-tensor, False otherwise.</p>"},{"location":"reference/fastforward/quantization/local_error/","title":"local_error","text":""},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.ExecGenFactory","title":"<code>fastforward.quantization.local_error.ExecGenFactory: TypeAlias = Callable[[], Generator[None, None, None]]</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.LocalErrorMethod","title":"<code>fastforward.quantization.local_error.LocalErrorMethod</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for local error methods.</p> <p>A local error method minimizes errors for sub networks. Specifically, it aims to minimize the error between a 'default' or reference forward pass and an alternative forward pass. An example of this a floating point forward pass and a quantized forward pass of a subset of the layers in a neural network.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.LocalErrorMethod.alternative_context","title":"<code>alternative_context()</code>","text":"<p>Return the alternative execution context.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.LocalErrorMethod.cleanup","title":"<code>cleanup()</code>  <code>abstractmethod</code>","text":"<p>Clean up resources used by the local error method.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.LocalErrorMethod.conclude_partition","title":"<code>conclude_partition()</code>  <code>abstractmethod</code>","text":"<p>Conclude the evaluation and update of a partition of the network.</p> <p>This is called after all stages for a particular partition have been concluded.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.LocalErrorMethod.default_context","title":"<code>default_context()</code>","text":"<p>Return the default execution context.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.LocalErrorMethod.prepare","title":"<code>prepare(ctx)</code>  <code>abstractmethod</code>","text":"<p>Prepare local error method with the given context.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.LocalErrorMethod.propagate","title":"<code>propagate(__replay_value)</code>  <code>abstractmethod</code>","text":"<p>Propagate data from replay stage to default and alterantive stage.</p> <p>Given the value obtained in the replay stage, generate new activation value for default and alternative stage. Return None to 'propagate' the actual values from the default and alterantive stage.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.LocalErrorMethod.update","title":"<code>update(__default_value, __alternative_value)</code>  <code>abstractmethod</code>","text":"<p>Perform update step.</p> <p>Perform update step given the value obtain from the default context and the value obtained from the alternative context.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.Runner","title":"<code>fastforward.quantization.local_error.Runner(target, method)</code>","text":"<p>Runner class to manage execution stages and contexts.</p> <p>Initialize the runner.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Callable[..., Any]</code> <p>The target function to run.</p> required <code>method</code> <code>LocalErrorMethod</code> <p>The method to manage errors.</p> required"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.Runner.ALTERNATIVE_STAGE","title":"<code>ALTERNATIVE_STAGE = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.Runner.DEFAULT_STAGE","title":"<code>DEFAULT_STAGE = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.Runner.REPLAY_STAGE","title":"<code>REPLAY_STAGE = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.Runner.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Record input data by calling the runner.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.Runner.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the runner.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.Runner.communicate","title":"<code>communicate(value)</code>","text":"<p>Communicate a value within the runner's context.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The value to communicate.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The communicated value.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.Runner.record_input","title":"<code>record_input(*args, **kwargs)</code>","text":"<p>Record input data for the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments for the input data.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the input data.</p> <code>{}</code>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.Runner.start","title":"<code>start()</code>","text":"<p>Start the runner and manage execution contexts.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.RunnerContext","title":"<code>fastforward.quantization.local_error.RunnerContext</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for RunnerContext.</p> <p>Defines methods that are exposed to implementors of the LocalErrorMethod interface through a context object.</p>"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.RunnerContext.communicate","title":"<code>communicate(value)</code>","text":"<p>Communicate values to other stages.</p> <p>Communicate a value obtained during the execution of a partition in a stage to the runner. This enables 'communication' between different execution contexts.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Tensor</code> <p>The tensor value to communicate to the runner and share with other contexts.</p> required"},{"location":"reference/fastforward/quantization/local_error/#fastforward.quantization.local_error.execution_context","title":"<code>fastforward.quantization.local_error.execution_context(func)</code>","text":"<p>Decorator to create an ExecutorContext.</p> <p>Returns:</p> Type Description <code>Callable[[], _GeneratorExecContext]</code> <p>_GeneratorExecContext that wraps func</p>"},{"location":"reference/fastforward/quantization/output_mse/","title":"output_mse","text":""},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSE","title":"<code>fastforward.quantization.output_mse.OutputMSE(modules, optimizer_factory)</code>","text":"<p>               Bases: <code>LocalErrorMethod</code></p> <p>Local error method for output Mean Squared Error (MSE) minimization.</p>"},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSE.alternative_context","title":"<code>alternative_context()</code>","text":"<p>Provide an alternative context with passthrough mode enabled.</p> <p>Yields:</p> Type Description <code>None</code> <p>Generator[None, None, None]: A generator for the alternative context.</p>"},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSE.cleanup","title":"<code>cleanup()</code>","text":"<p>Clean up by removing all registered hooks.</p>"},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSE.conclude_partition","title":"<code>conclude_partition()</code>","text":"<p>Conclude the current partition by zeroing the gradients.</p>"},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSE.prepare","title":"<code>prepare(ctx)</code>","text":"<p>Prepare the context by registering hooks for quantizers.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>RunnerContext</code> <p>The runner context.</p> required"},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSE.propagate","title":"<code>propagate(replay_value)</code>","text":"<p>Propagate the replay value to the default and alternative context.</p> <p>Parameters:</p> Name Type Description Default <code>replay_value</code> <code>Tensor</code> <p>The replay value tensor.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor]: The detached and dequantized replay value.</p>"},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSE.update","title":"<code>update(quantized, unquantized)</code>","text":"<p>Update the model parameters based on the MSE between quantized and unquantized tensors.</p> <p>Parameters:</p> Name Type Description Default <code>quantized</code> <code>Tensor</code> <p>The quantized tensor.</p> required <code>unquantized</code> <code>Tensor</code> <p>The unquantized tensor.</p> required"},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSEOverride","title":"<code>fastforward.quantization.output_mse.OutputMSEOverride()</code>","text":"<p>Override class for output Mean Squared Error (MSE) calculation.</p> <p>Attributes:</p> Name Type Description <code>passthrough</code> <p>Flag to enable passthrough mode.</p>"},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSEOverride.passthrough","title":"<code>passthrough = False</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/quantization/output_mse/#fastforward.quantization.output_mse.OutputMSEOverride.__call__","title":"<code>__call__(quantizer, callback, args, kwargs)</code>","text":"<p>Call the override with the given quantizer, callback, and arguments.</p> <p>Parameters:</p> Name Type Description Default <code>quantizer</code> <code>Quantizer</code> <p>The quantizer instance.</p> required <code>callback</code> <code>Callable[[Tensor], Tensor]</code> <p>The callback function.</p> required <code>args</code> <code>tuple[Any, ...]</code> <p>Positional arguments for the callback.</p> required <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for the callback.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The result of the callback or the data tensor if passthrough is enabled.</p>"},{"location":"reference/fastforward/quantization/quant_init/","title":"quant_init","text":""},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizationConfig","title":"<code>fastforward.quantization.quant_init.QuantizationConfig()</code>","text":"<p>Manages quantization configuration rules and initializes quantizers in a model.</p>"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizationConfig.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizationConfig.add_rule","title":"<code>add_rule(query, factory, **kwargs)</code>","text":"<p>Add a quantizer rule to the collection.</p> <p>A rule consists of an mpath query and a quantizer factory. When a query matches a quantizer (stub) and has highest precedence, the factory is called to initialize a <code>Quantizer</code> (see the docstring of <code>precedence_score</code>).</p> <p><code>quantizer_factory</code> may either be a <code>Quantizer</code> subclass or a callable that accepts a <code>str</code> and <code>Quantizer</code>. In the latter case, the provided string is the full name relative to the root module and the quantizer is the quantizer that is currently referenced at this location. This may either be a <code>QuantizerStub</code> or an existing (and initialized) quantizer.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>BaseSelector | str</code> <p>Mpath.Selector or str that represent the filter query. Please see the documentation of <code>fastforward.mpath.query</code> and <code>fastforward.mpath.search</code> for more details.</p> required <code>quantizer_factory</code> <p><code>Quantizer</code> or callable to initialize each quantizer in the collection. This factory is called for each element in the collection. If a <code>Quantizer</code> class is passed, all <code>kwargs</code> will be forwarded to the initializer. In the case of a callable, all <code>kwargs</code> are ignored.</p> required"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizationConfig.initialize","title":"<code>initialize(model, *, safe=True, overwrite_policy='error')</code>","text":"<p>Initialize quantizers in model following the rules in this config.</p> <p>For each quantizer (stub) that matches at least one location, the last added rule is applied. The factory function of this rule is executed to obtain an initialized quantizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Root module to initialize quantizers in</p> required <code>overwrite_policy</code> <code>_OverwriteOptions</code> <p>Either <code>\"skip\"</code>, <code>\"overwrite\"</code>, or <code>\"error\"</code>. The <code>overwrite_policy</code> indicates what to do when the element in the collection is already an initialized quantizer. In case of <code>\"skip\"</code> the initialization for that specific quantizer is skipped and nothing happens, for <code>\"overwrite\"</code> a new quantizer is created and the existing quantizer is overwritten. Lastly, in case of <code>\"error\"</code> an error is raised when it is attempted to re-initialize an already initialized quantizer.</p> <code>'error'</code> <code>safe</code> <code>bool</code> <p>Boolean that indicates what to do when the module was already replaced in the root module between the creation of this collection and the call to initialize. When safe is <code>True</code>, an error is raised. Otherwise, a (re-)initialization of the quantizer is attempted.</p> <code>True</code>"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizationConfig.precedence_score","title":"<code>precedence_score(rule, result)</code>","text":"<p>Score precedence given a result.</p> <p>Returns an integer value that is used to choose between two matching rules. The rule with the highest score is selected. Ties are broken by order, i.e., select the rule that was added last. The precedence score is computer on a per result basis, this means that the precedence can differ per module.</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>_QuantConfigRule</code> <p>The rule that is scored</p> required <code>result</code> <code>FilterResult</code> <p>The query/filter request for a given module obtained using rule</p> required <p>Returns:</p> Type Description <code>int</code> <p>integer indicating the rule/result precedence</p>"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizerCollection","title":"<code>fastforward.quantization.quant_init.QuantizerCollection(root, results=None)</code>","text":"<p>               Bases: <code>MPathCollection</code></p> <p>MPathCollection that only stores Quantizer subclasses.</p> <p>On creation, all non quantizer results are filtered out from the results iterable. In contrast, append will throw an error when a non quantizer result is provided.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>Module</code> <p>The root module that was used to produce the search results</p> required <code>results</code> <code>Optional[Iterable[FilterResult]]</code> <p>Optional list of search results, may be extended using append</p> <code>None</code>"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizerCollection.append","title":"<code>append(item)</code>","text":"<p>Add <code>FilterResult</code> to the collection.</p> <p>Raises an error if the filter result contains a module that is not a Quantizer.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>FilterResult</code> <p>Filter result to add to the collection</p> required"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizerCollection.initialize","title":"<code>initialize(quantizer_factory, *, overwrite_policy='error', safe=True, **kwargs)</code>","text":"<p>Initialize all quantizers in the collection using <code>quantizer_factory</code>.</p> <p><code>quantizer_factory</code> may either be a <code>Quantizer</code> subclass or a callable that accepts a <code>str</code> and <code>Quantizer</code>. In the latter case, the provided string is the full name relative to the root module and the quantizer is the quantizer that is currently referenced at this location. This may either be a <code>QuantizerStub</code> or an existing quantizer.</p> <p>Parameters:</p> Name Type Description Default <code>quantizer_factory</code> <code>type[Quantizer] | Callable[[str, Module], Quantizer]</code> <p><code>Quantizer</code> or callable to initialize each quantizer in the collection. This factory is called for each element in the collection. If a <code>Quantizer</code> class is passed, all <code>kwargs</code> will be forwarded to the initializer. In the case of a callable, all <code>kwargs</code> are ignored.</p> required <code>overwrite_policy</code> <code>_OverwriteOptions</code> <p>Either <code>\"skip\"</code>, <code>\"overwrite\"</code>, or <code>\"error\"</code>. The <code>overwrite_policy</code> indicates what to do when the element in the collection is already an initialized quantizer. In case of <code>\"skip\"</code> the initialization for that specific quantizer is skipped and nothing happens, for <code>\"overwrite\"</code> a new quantizer is created and the existing quantizer is overwritten. Lastly, in case of <code>\"error\"</code> an error is raised when it is attempted to re-initialize an already initialized quantizer.</p> <code>'error'</code> <code>safe</code> <code>bool</code> <p>Boolean that indicates what to do when the module was already replaced in the root module between the creation of this collection and the call to initialize. When safe is <code>True</code>, an error is raised. Otherwise, a (re-)initialization of the quantizer is attempted.</p> <code>True</code>"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizerTagSelectorFragment","title":"<code>fastforward.quantization.quant_init.QuantizerTagSelectorFragment(*tags)</code>","text":"<p>               Bases: <code>Fragment</code></p> <p>Fragment that matches quantizer tags.</p> <p>Can be used in query string using <code>[quantizer:&lt;tag&gt;(,&lt;tag&gt;)+]</code> or \"[qtag:(,)+]"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizerTagSelectorFragment.__hash__","title":"<code>__hash__()</code>","text":""},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizerTagSelectorFragment.__str__","title":"<code>__str__()</code>","text":""},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizerTagSelectorFragment.from_raw_string","title":"<code>from_raw_string(raw_str)</code>  <code>classmethod</code>","text":"<p>Create a <code>QuantizerTagSelectorFragment</code> from a string.</p> <p>Tags are expected to be comma separated. Any whitespace between tags is ignored.</p>"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.QuantizerTagSelectorFragment.match","title":"<code>match(fragment_name, module)</code>","text":"<p>Return True if module's tag metadata includes <code>self._tags</code>, False otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>fragment_name</code> <code>str</code> <p>Unused</p> required <code>module</code> <code>Module</code> <p>The module that corresponds to the fragment name.</p> required <p>Returns: Boolean indicating whether the fragment matches the tag set.</p>"},{"location":"reference/fastforward/quantization/quant_init/#fastforward.quantization.quant_init.find_quantizers","title":"<code>fastforward.quantization.quant_init.find_quantizers(root_module, query, *, aliases=None)</code>","text":"<p>Find all quantizers in root_module that match query.</p> <p>Parameters:</p> Name Type Description Default <code>root_module</code> <code>Module</code> <p>The module to search for quantizers.</p> required <code>query</code> <code>str | BaseSelector</code> <p>Mpath.Selector or str that represent the filter query. Please see the documentation of <code>fastforward.mpath.query</code> and <code>fastforward.mpath.search</code> for more details.</p> required"},{"location":"reference/fastforward/quantization/random/","title":"random","text":""},{"location":"reference/fastforward/quantization/random/#fastforward.quantization.random.random_quantized","title":"<code>fastforward.quantization.random.random_quantized(shape, scale=0.1, offset=None, num_bits=3, requires_grad=False, granularity=granularities.PerTensor(), device=None, storage_dtype=torch.float32)</code>","text":"<p>Generate a random quantized tensor.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>The shape of the tensor.</p> required <code>scale</code> <code>float | Tensor</code> <p>The scale factor for quantization.</p> <code>0.1</code> <code>offset</code> <code>Optional[int | Tensor]</code> <p>The offset for quantization.</p> <code>None</code> <code>num_bits</code> <code>int</code> <p>The number of bits for quantization.</p> <code>3</code> <code>requires_grad</code> <code>bool</code> <p>If True, the tensor requires gradient.</p> <code>False</code> <code>granularity</code> <code>Granularity</code> <p>The granularity of quantization.</p> <code>PerTensor()</code> <code>device</code> <code>Optional[device | str]</code> <p>The device for the tensor.</p> <code>None</code> <code>storage_dtype</code> <code>dtype</code> <p>The storage data type for the tensor.</p> <code>float32</code> <p>Returns:</p> Name Type Description <code>QuantizedTensor</code> <code>QuantizedTensor</code> <p>The generated random quantized tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the scale and offset tensors do not have the same number of elements.</p>"},{"location":"reference/fastforward/quantization/ste/","title":"ste","text":""},{"location":"reference/fastforward/quantization/ste/#fastforward.quantization.ste.round_ste","title":"<code>fastforward.quantization.ste.round_ste = ste(torch.round)</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/quantization/ste/#fastforward.quantization.ste.STEAutogradFunc","title":"<code>fastforward.quantization.ste.STEAutogradFunc</code>","text":"<p>               Bases: <code>Function</code></p> <p>A custom autograd function for Straight-Through Estimator (STE).</p> <p>This function allows the forward pass to be non-differentiable while providing a gradient for the backward pass.</p> <p>Attributes:</p> Name Type Description <code>apply</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>The function to apply STE.</p>"},{"location":"reference/fastforward/quantization/ste/#fastforward.quantization.ste.STEAutogradFunc.apply","title":"<code>apply: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/quantization/ste/#fastforward.quantization.ste.STEAutogradFunc.backward","title":"<code>backward(ctx, output_grad)</code>  <code>staticmethod</code>","text":"<p>Backward pass for the STE function.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Any</code> <p>Context object.</p> required <code>output_grad</code> <code>Tensor</code> <p>Gradient of the output.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, None]</code> <p>Gradient of the input and None for the output.</p>"},{"location":"reference/fastforward/quantization/ste/#fastforward.quantization.ste.STEAutogradFunc.forward","title":"<code>forward(ctx, input, output)</code>  <code>staticmethod</code>","text":"<p>Forward pass for the STE function.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Any</code> <p>Context object to store information for backward computation.</p> required <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>output</code> <code>Tensor</code> <p>The output tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor.</p>"},{"location":"reference/fastforward/quantization/ste/#fastforward.quantization.ste.STEWrappedFunction","title":"<code>fastforward.quantization.ste.STEWrappedFunction</code>","text":"<p>               Bases: <code>Protocol[_P]</code></p> <p>Protocol for a wrapped function using STE.</p> <p>This protocol defines the call signature for functions wrapped with STE.</p>"},{"location":"reference/fastforward/quantization/ste/#fastforward.quantization.ste.STEWrappedFunction.__call__","title":"<code>__call__(__input, *args, **kwargs)</code>","text":"<p>Call the wrapped function.</p> <p>Parameters:</p> Name Type Description Default <code>__input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>*args</code> <code>args</code> <p>Additional positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor.</p>"},{"location":"reference/fastforward/quantization/ste/#fastforward.quantization.ste.ste","title":"<code>fastforward.quantization.ste.ste(func)</code>","text":"<p>Decorator to apply STE to a function.</p> <p>This decorator wraps a function to use STE, ensuring the input and output shapes match.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[Concatenate[Tensor, _P], Tensor]</code> <p>The function to wrap.</p> required <p>Returns:</p> Type Description <code>STEWrappedFunction[_P]</code> <p>STEWrappedFunction[P]: The wrapped function with STE applied.</p>"},{"location":"reference/fastforward/quantization/strict_quantization/","title":"strict_quantization","text":""},{"location":"reference/fastforward/quantization/strict_quantization/#fastforward.quantization.strict_quantization.strict_quantization","title":"<code>fastforward.quantization.strict_quantization.strict_quantization = ff.flags.strict_quantization</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/quantization/strict_quantization/#fastforward.quantization.strict_quantization.ModuleStrictQuantHandle","title":"<code>fastforward.quantization.strict_quantization.ModuleStrictQuantHandle(enable_strict_quantization)</code>","text":"<p>A handle to manage strict quantization settings for specific modules.</p> <p>This class attaches pre-forward and post-forward hooks to the provided modules to enable or disable strict quantization during their execution.</p> <p>Initialize the ModuleStrictQuantHandle.</p> <p>Parameters:</p> Name Type Description Default <code>enable_strict_quantization</code> <code>bool</code> <p>Flag to enable or disable strict quantization.</p> required"},{"location":"reference/fastforward/quantization/strict_quantization/#fastforward.quantization.strict_quantization.ModuleStrictQuantHandle.__enter__","title":"<code>__enter__()</code>","text":""},{"location":"reference/fastforward/quantization/strict_quantization/#fastforward.quantization.strict_quantization.ModuleStrictQuantHandle.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Exit the context manager and remove all hooks.</p>"},{"location":"reference/fastforward/quantization/strict_quantization/#fastforward.quantization.strict_quantization.ModuleStrictQuantHandle.attach","title":"<code>attach(module)</code>","text":"<p>Attach the pre-forward and post-forward hooks to a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to attach hooks to.</p> required"},{"location":"reference/fastforward/quantization/strict_quantization/#fastforward.quantization.strict_quantization.ModuleStrictQuantHandle.remove","title":"<code>remove()</code>","text":"<p>Remove all attached hooks.</p>"},{"location":"reference/fastforward/quantization/strict_quantization/#fastforward.quantization.strict_quantization.strict_quantization_for_module","title":"<code>fastforward.quantization.strict_quantization.strict_quantization_for_module(strict=True, *modules)</code>","text":"<p>Enable or disable strict quantization on a per module basis.</p> <p>This function attaches pre-forward and post-forward hooks to the provided modules to set the global strict quantization setting before executing the model and reset it to its original state after the module concludes. It can be used directly or as a context manager.</p> <p>Parameters:</p> Name Type Description Default <code>strict</code> <code>bool</code> <p>The strict quantization state to use for modules.</p> <code>True</code> <code>modules</code> <code>Module</code> <p>The modules to apply the strict quantization state to.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>ModuleStrictQuantHandle</code> <code>ModuleStrictQuantHandle</code> <p>A handle to manage the strict quantization settings.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; network = Network()\n&gt;&gt;&gt; with strict_quantization_for_module(False, network.layer3, network.layer5):\n&gt;&gt;&gt;     network(torch.randn(3, 3))\n</code></pre> <pre><code>&gt;&gt;&gt; network = Network()\n&gt;&gt;&gt; handle = strict_quantization_for_module(False, network.layer1, network.layer2, network.layer3)\n&gt;&gt;&gt; network(torch.randn(3, 3))\n&gt;&gt;&gt; handle.remove()\n</code></pre>"},{"location":"reference/fastforward/quantization/tiled_tensor/","title":"tiled_tensor","text":""},{"location":"reference/fastforward/quantization/tiled_tensor/#fastforward.quantization.tiled_tensor.T","title":"<code>fastforward.quantization.tiled_tensor.T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/quantization/tiled_tensor/#fastforward.quantization.tiled_tensor.check_tile_compatibility","title":"<code>fastforward.quantization.tiled_tensor.check_tile_compatibility(input_size, tile_size)</code>","text":"<p>Helper function to assess whether a given tile size can be used with a given data shape.</p> <p>The function will raise an error if not, otherwise return None.</p>"},{"location":"reference/fastforward/quantization/tiled_tensor/#fastforward.quantization.tiled_tensor.rows_to_tiles","title":"<code>fastforward.quantization.tiled_tensor.rows_to_tiles(tiled_data, data_size, tile_size)</code>","text":"<p>Reshape and permute tiled_data.</p> <p>Reshape and permute tiled_data to a tensor of data_size tiled by tile_size where each row in tiled_data corresponds to a single tile.</p>"},{"location":"reference/fastforward/quantization/tiled_tensor/#fastforward.quantization.tiled_tensor.rows_to_tiles--args","title":"Args","text":"<pre><code>tiled_data: Data to reshape and permute\ndata_size: Size of the output\ntile_size: Tile size to use for row collection\n</code></pre>"},{"location":"reference/fastforward/quantization/tiled_tensor/#fastforward.quantization.tiled_tensor.rows_to_tiles--returns","title":"Returns","text":"<pre><code>Tensor: The reshaped tensor.\n</code></pre>"},{"location":"reference/fastforward/quantization/tiled_tensor/#fastforward.quantization.tiled_tensor.rows_to_tiles--raises","title":"Raises","text":"<pre><code>ValuEerror: Tiled_data's size does not correspond to data_size and tile_size.\n</code></pre>"},{"location":"reference/fastforward/quantization/tiled_tensor/#fastforward.quantization.tiled_tensor.tiles_to_rows","title":"<code>fastforward.quantization.tiled_tensor.tiles_to_rows(data, tile_size)</code>","text":"<p>Reshape and permute data.</p> <p>Reshape and permute data to a tensor in which the elements per tile are layed out per row, following tile_size.</p>"},{"location":"reference/fastforward/quantization/tiled_tensor/#fastforward.quantization.tiled_tensor.tiles_to_rows--args","title":"Args","text":"<pre><code>data: Data to reshape and permute\ntile_size: Tile size to use for row collection\n</code></pre>"},{"location":"reference/fastforward/quantization/tiled_tensor/#fastforward.quantization.tiled_tensor.tiles_to_rows--returns","title":"Returns","text":"<pre><code>Tensor: The reshaped tensor.\n</code></pre>"},{"location":"reference/fastforward/range_setting/","title":"range_setting","text":""},{"location":"reference/fastforward/range_setting/common/","title":"common","text":"<p>Collection of types, protocols and base implementations for range setting methods.</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.RangeEstimator","title":"<code>fastforward.range_setting.common.RangeEstimator</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[_T, _Module]</code></p> <p>Abstract base class for range estimator methods.</p> <p>Subclasses of this base class implement preparation and cleanup methods for range estimation that setup a module for range estimation before passing data through the model and clean up any range estimation related settings afterwards.</p> <p>Since many range estimation methods operate 'locally' on leaf modules for which seperate prepare and cleanup steps are appropriate, <code>split_module</code> may take a particular module and yield child modules for which <code>prepare</code> and <code>cleanup</code> is called seperately.</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.RangeEstimator.cleanup","title":"<code>cleanup(module, metadata)</code>  <code>abstractmethod</code>","text":"<p>Clean up any range estimation specific settings from module.</p> <p>After the conclusion of this method, it is assumed that module is in the same state as before <code>prepare</code> was called, except for some quantization specific parameters that where estimated during the range estimation step.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>_Module</code> <p>The module to clean up</p> required <code>metadata</code> <code>_T</code> <p>Any metadata that was returned from the prepare method for <code>module</code></p> required"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.RangeEstimator.prepare","title":"<code>prepare(module)</code>  <code>abstractmethod</code>","text":"<p>Prepare module for a particular range estimation method.</p> <p>Any metadata that is returned from this method is fed back to cleanup after range estimation.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>_Module</code> <p>Module to prepare for range estimation</p> required <p>Returns:</p> Type Description <code>_T</code> <p>None or any Metadata that may be used during cleanup.</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.RangeEstimator.split_module","title":"<code>split_module(module)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Split the module into one or more submodules.</p> <p>For each module yielded from this function, prepare and cleanup are called seperately. If only a single prepare and cleanup should be performed from module, only yield module once.</p> <p>Yields:</p> Type Description <code>_Module</code> <p>Submodules for module (or module itself) for which a separate prepare/cleanup step is performed.</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.RangeSettable","title":"<code>fastforward.range_setting.common.RangeSettable</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for range settable quantization modules.</p> <p>The quantization parameters for modules that implement this interface can be specified through a quantization range. This is a generalization over particular quantizer parameterization and is used by range setting methods.</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.RangeSettable.granularity","title":"<code>granularity: granularity.Granularity</code>  <code>property</code>","text":"<p>The quantization granularity for a quantizer.</p> <p>The granularity specifies which part of the input tensor is quantized using which quantization parameters. Examples of granularities are per-tensor and per-channel.</p> Note <p>Granularities will soon be replaced by tile specificiations.</p> <p>Returns:</p> Type Description <code>Granularity</code> <p>quantization granulirity</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.RangeSettable.quantization_range","title":"<code>quantization_range: tuple[Optional[torch.Tensor], Optional[torch.Tensor]]</code>  <code>property</code> <code>writable</code>","text":"<p>Quantization range for a quantizer specified through a minimum and maximum threshold.</p> <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Minimum threshold for quantization range as N-d tensor or None of no range is specified.</p> <code>Optional[Tensor]</code> <p>Maximum threshold for quantization range as N-d tensor or None of no range is specified.</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.SimpleEstimatorStep","title":"<code>fastforward.range_setting.common.SimpleEstimatorStep(*args, disable_quantization=False, **kwargs)</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[_QuantizerType]</code></p> <p>Base class for simple range estimator step.</p> <p>SimpleEstimatorStep provides a forward pass in which estimate_step is called. During the first execution of the forward pass, setup_estimator is called.</p> <p>Parameters:</p> Name Type Description Default <code>disable_quantization</code> <code>bool</code> <p>If True, during range estimation, the ranges are estimated based on a non-quantized forward pass, i.e., the output of all quantizers will not be quantized. If False, the quantizers will produce quantized tensors which are propagated through the network.</p> <code>False</code>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.SimpleEstimatorStep.estimate_step","title":"<code>estimate_step(quantizer, data)</code>  <code>abstractmethod</code>","text":"<p>Given quantizer and data, update the quantization parameters of <code>quantizer</code> based on data.</p> <p>Parameters:</p> Name Type Description Default <code>quantizer</code> <code>_QuantizerType</code> <p><code>Quantizer</code> module for which parameters should be updated</p> required <code>data</code> <code>Tensor</code> <p>Data that update should be based on</p> required"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.SimpleEstimatorStep.forward","title":"<code>forward(quantizer, callback, args, kwargs)</code>","text":""},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.SimpleEstimatorStep.setup_estimator","title":"<code>setup_estimator(data)</code>","text":"<p>Perform setup for the estimator.</p> <p>The first data batch is passed. this method is only called once during the first estimator step.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The first batch passed to the estimator.</p> required"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.SupportsRangeBasedOperator","title":"<code>fastforward.range_setting.common.SupportsRangeBasedOperator</code>","text":"<p>               Bases: <code>RangeSettable</code>, <code>Protocol</code></p> <p>Interface for quantizers that can create a quantization operator for a range.</p> <p>Interface for Quantizers that can specify a quantization function given a min/max thresholded quantization range. This is used by range setting methods that require access to multiple parameterized quantization function during estimation time.</p> <p>This interface extends the <code>RangeSettable</code> interface.</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.SupportsRangeBasedOperator.symmetric","title":"<code>symmetric: bool</code>  <code>property</code>","text":"<p>Return boolean indicating if quantizer is symmetric.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Boolean indicating if quantization operator is symmetric or not</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.SupportsRangeBasedOperator.operator_for_range","title":"<code>operator_for_range(__min, __max, __data_shape)</code>","text":"<p>Return quantization operator using the given range.</p> <p>Returns a callable that specifies a quantization operator for the given min/max thresholded quantization range and specific data_shape.</p> <p>It is assumed that the returned operator is independent of self and not altered during its life and users of this method may assume a unique reference to the returned operator.</p> <p>Parameters:</p> Name Type Description Default <code>__min</code> <code>Tensor</code> <p>N-d tensor for minimum threshold of quantization range(s).</p> required <code>__max</code> <code>Tensor</code> <p>N-d tensor for maximum threshold of quantization range(s).</p> required <code>__data_shape</code> <code>Size</code> <p>Shape of input to quantization operator.</p> required Note <p>Implementers of this protocol can use different names for the function arguments</p> <p>Returns:</p> Type Description <code>Callable[[Tensor], QuantizedTensor]</code> <p>Quantization operator for given quantization range and data shape.</p>"},{"location":"reference/fastforward/range_setting/common/#fastforward.range_setting.common.estimate_ranges","title":"<code>fastforward.range_setting.common.estimate_ranges(model_or_layers, estimator, *args, **kwargs)</code>","text":"<p>Context manager to setup <code>model_or_layers</code> for range estimation.</p> <p>Within the context, any data that is passed to <code>model_or_layers</code> will trigger a range estimation step. Each module will be setup by <code>estimator</code> and cleaned up at the conclusion of the context.</p> <p>If <code>estimator</code> is a <code>RangeEstimator</code> type (in contrast to an instance), it is initialized with any extra arguments passed to this function. If it is an instance, it is used as is.</p> Example <p>This function may be used as follows:</p> <pre><code>with estimate_ranges([first_module, second_module], SomeEstimatorClass):\n    for batch in data_loader:\n        first_module(data)\n        second_module(data)\n\n# Here all quantizers that executed in the previous context will have their\n# ranges initialized and are ready for further use.\n</code></pre>"},{"location":"reference/fastforward/range_setting/min_error/","title":"min_error","text":"<p>Minimum error range estimators.</p> <p>This module contains implementations for range estimators that perform a search to determine the (near) optimal quantization grid that minimizes a specified error.</p> <p>Attributes:</p> Name Type Description <code>min_error_grid</code> <p>Alias of <code>MinErrorGridRangeEstimator</code></p> <code>mse_grid</code> <p>Alias of <code>MinErrorGridRangeEstimator</code></p>"},{"location":"reference/fastforward/range_setting/min_error/#fastforward.range_setting.min_error.min_error_grid","title":"<code>fastforward.range_setting.min_error.min_error_grid = MinErrorGridRangeEstimator</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/min_error/#fastforward.range_setting.min_error.mse_grid","title":"<code>fastforward.range_setting.min_error.mse_grid = MinErrorGridRangeEstimator</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/min_error/#fastforward.range_setting.min_error.MinErrorGridRangeEstimator","title":"<code>fastforward.range_setting.min_error.MinErrorGridRangeEstimator(error_fn=mse_error, num_candidates=100, search_grid_generator=_default_search_grid, update_range_policy=None)</code>","text":"<p>               Bases: <code>RangeEstimator[OverrideHandle, Quantizer]</code></p> <p>Grid range estimator for error minimization.</p> <p>Range Estimator that searches for quantization range that minimizes <code>error_fn</code> between quantized and non-quantized value.</p> <p>A grid search as defined by <code>search_grid_generator</code> is performed to find the candidate that minimizes the given error.</p> <p><code>update_range_policy</code> specifies how often the the quantization grids should be updated. If no such policy is provided, the ranges are updated after every step.</p> <p>Parameters:</p> Name Type Description Default <code>error_fn</code> <code>_ErrorFn</code> <p>The error function <code>(quantized_data, non_quantized_data) -&gt; real-valued error</code> that is minimized</p> <code>mse_error</code> <code>num_canidates</code> <p>The size of the search grid</p> required <code>search_grid_generator</code> <code>_SearchGridGenerator</code> <p>Callable that defines search grid</p> <code>_default_search_grid</code> <code>update_range_policy</code> <code>Optional[Callable[[_MinAvgErrorGridEstimator, int], bool]]</code> <p>Callable that defines whether quantizers should be updated per step.</p> <code>None</code>"},{"location":"reference/fastforward/range_setting/min_error/#fastforward.range_setting.min_error.MinErrorGridRangeEstimator.cleanup","title":"<code>cleanup(module, metadata)</code>","text":"<p>Cleanup <code>module</code> after min error range estimation.</p>"},{"location":"reference/fastforward/range_setting/min_error/#fastforward.range_setting.min_error.MinErrorGridRangeEstimator.prepare","title":"<code>prepare(module)</code>","text":"<p>Prepare <code>module</code> for min error range estimation.</p>"},{"location":"reference/fastforward/range_setting/min_error/#fastforward.range_setting.min_error.MinErrorGridRangeEstimator.split_module","title":"<code>split_module(module)</code>  <code>classmethod</code>","text":"<p>Yields all quantizers in <code>module</code>.</p> <p>Each is set up for min error range estimation seperately.</p>"},{"location":"reference/fastforward/range_setting/min_error/#fastforward.range_setting.min_error.mse_error","title":"<code>fastforward.range_setting.min_error.mse_error(quantized_data, unquantized_data)</code>","text":"<p>Mean Squared Error error function for min_error_grid method.</p> <p>Parameters:</p> Name Type Description Default <code>quantized_data</code> <code>Tensor</code> <p>Data after quantization</p> required <code>unquantized_data</code> <code>Tensor</code> <p>Data before quantization</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>mean squared error between <code>quantized_data</code> and <code>unquantized_data</code></p>"},{"location":"reference/fastforward/range_setting/minmax/","title":"minmax","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.T","title":"<code>fastforward.range_setting.minmax.T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.running_minmax","title":"<code>fastforward.range_setting.minmax.running_minmax = RunningMinMaxRangeEstimator</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.smoothed_minmax","title":"<code>fastforward.range_setting.minmax.smoothed_minmax = SmoothedMinMaxRangeEstimator</code>  <code>module-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxEstimator","title":"<code>fastforward.range_setting.minmax.RunningMinMaxEstimator(quantizer, disable_quantization=False)</code>","text":"<p>               Bases: <code>SimpleEstimatorStep[RangeSettable]</code>, <code>Module</code></p> <p>Estimates the quantization range using the running minimum and maximum values.</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>Tensor | None</code> <p>The minimum values observed.</p> <code>max</code> <code>Tensor | None</code> <p>The maximum values observed.</p> <p>Initialize the RunningMinMaxEstimator.</p> <p>Parameters:</p> Name Type Description Default <code>quantizer</code> <code>RangeSettable</code> <p>The quantizer to estimate ranges for.</p> required <code>disable_quantization</code> <code>bool</code> <p>Flag to disable quantization.</p> <code>False</code>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxEstimator.max","title":"<code>max: torch.Tensor | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxEstimator.min","title":"<code>min: torch.Tensor | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxEstimator.estimate_step","title":"<code>estimate_step(quantizer, data)</code>","text":"<p>Perform a single estimation step.</p> <p>Parameters:</p> Name Type Description Default <code>quantizer</code> <code>RangeSettable</code> <p>The quantizer to estimate ranges for.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxEstimator.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Return a string representation of the estimator.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the estimator.</p>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxEstimator.initialize_parameters","title":"<code>initialize_parameters(quantizer, data)</code>","text":"<p>Initialize the min and max parameters.</p> <p>Parameters:</p> Name Type Description Default <code>quantizer</code> <code>RangeSettable</code> <p>The quantizer to estimate ranges for.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxRangeEstimator","title":"<code>fastforward.range_setting.minmax.RunningMinMaxRangeEstimator(disable_quantization=False)</code>","text":"<p>               Bases: <code>RangeEstimator[OverrideHandle, Quantizer]</code></p> <p>Running min-max range estimator.</p> <p>Estimates the quantization ranges based on the minimum and maximum over all data seen for each quantizer.</p> <p>Attributes:</p> Name Type Description <code>disable_quantization</code> <p>Flag to disable quantization.</p> <p>Initialize the RunningMinMaxRangeEstimator.</p> <p>Parameters:</p> Name Type Description Default <code>disable_quantization</code> <code>bool</code> <p>Flag to disable quantization.</p> <code>False</code>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxRangeEstimator.disable_quantization","title":"<code>disable_quantization = disable_quantization</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxRangeEstimator.cleanup","title":"<code>cleanup(module, metadata)</code>","text":"<p>Cleanup after range estimation has concluded.</p>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxRangeEstimator.prepare","title":"<code>prepare(module)</code>","text":"<p>Prepare the module for range estimation.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Quantizer</code> <p>The quantizer module.</p> required <p>Returns:</p> Name Type Description <code>OverrideHandle</code> <code>OverrideHandle</code> <p>The override handle for the module.</p>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.RunningMinMaxRangeEstimator.split_module","title":"<code>split_module(module)</code>  <code>classmethod</code>","text":"<p>Split module up into separate quantizers.</p>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxEstimator","title":"<code>fastforward.range_setting.minmax.SmoothedMinMaxEstimator(quantizer, gamma=1.0, disable_quantization=False)</code>","text":"<p>               Bases: <code>SimpleEstimatorStep[RangeSettable]</code>, <code>Module</code></p> <p>Estimates the quantization range using exponential smoothing of the minimum and maximum values.</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>Tensor | None</code> <p>The minimum values observed.</p> <code>max</code> <code>Tensor | None</code> <p>The maximum values observed.</p> <code>gamma</code> <p>The smoothing factor.</p> <p>Initialize the SmoothedMinMaxEstimator.</p> <p>Parameters:</p> Name Type Description Default <code>quantizer</code> <code>RangeSettable</code> <p>The quantizer to estimate ranges for.</p> required <code>gamma</code> <code>float</code> <p>The smoothing factor.</p> <code>1.0</code> <code>disable_quantization</code> <code>bool</code> <p>Flag to disable quantization.</p> <code>False</code>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxEstimator.gamma","title":"<code>gamma = gamma</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxEstimator.max","title":"<code>max: torch.Tensor | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxEstimator.min","title":"<code>min: torch.Tensor | None</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxEstimator.estimate_step","title":"<code>estimate_step(quantizer, data)</code>","text":"<p>Perform a single estimation step.</p> <p>Parameters:</p> Name Type Description Default <code>quantizer</code> <code>RangeSettable</code> <p>The quantizer to estimate ranges for.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxEstimator.extra_repr","title":"<code>extra_repr()</code>","text":"<p>Return a string representation of the estimator.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the estimator.</p>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxEstimator.initialize_parameters","title":"<code>initialize_parameters(quantizer, data)</code>","text":"<p>Initialize the min and max parameters.</p> <p>Parameters:</p> Name Type Description Default <code>quantizer</code> <code>RangeSettable</code> <p>The quantizer to estimate ranges for.</p> required <code>data</code> <code>Tensor</code> <p>The input data tensor.</p> required"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxRangeEstimator","title":"<code>fastforward.range_setting.minmax.SmoothedMinMaxRangeEstimator(gamma=1.0, disable_quantization=False)</code>","text":"<p>               Bases: <code>RangeEstimator[OverrideHandle, Quantizer]</code></p> <p>Exponential moving average range estimator.</p> <p>Estimates the ranges based on a exponential moving average over batches of the minimum and maximum for each quantizer.</p> <p>Attributes:</p> Name Type Description <code>gamma</code> <p>The smoothing factor.</p> <code>disable_quantization</code> <p>Flag to disable quantization.</p> <p>Initialize the SmoothedMinMaxRangeEstimator.</p> <p>Parameters:</p> Name Type Description Default <code>gamma</code> <code>float</code> <p>The smoothing factor.</p> <code>1.0</code> <code>disable_quantization</code> <code>bool</code> <p>Flag to disable quantization.</p> <code>False</code>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxRangeEstimator.disable_quantization","title":"<code>disable_quantization = disable_quantization</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxRangeEstimator.gamma","title":"<code>gamma = gamma</code>  <code>instance-attribute</code>","text":""},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxRangeEstimator.cleanup","title":"<code>cleanup(module, metadata)</code>","text":"<p>Clean up the module after range estimation.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Quantizer</code> <p>The quantizer module.</p> required <code>metadata</code> <code>OverrideHandle</code> <p>The override handle for the module.</p> required"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxRangeEstimator.prepare","title":"<code>prepare(module)</code>","text":"<p>Prepare the module for range estimation.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Quantizer</code> <p>The quantizer module.</p> required <p>Returns:</p> Name Type Description <code>OverrideHandle</code> <code>OverrideHandle</code> <p>The override handle for the module.</p>"},{"location":"reference/fastforward/range_setting/minmax/#fastforward.range_setting.minmax.SmoothedMinMaxRangeEstimator.split_module","title":"<code>split_module(module)</code>  <code>classmethod</code>","text":"<p>Split the module into individual quantizers.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to split.</p> required <p>Yields:</p> Type Description <code>Quantizer</code> <p>Iterator[Quantizer]: An iterator over the quantizers.</p>"}]}