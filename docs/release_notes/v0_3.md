# FastForward 0.3.0

## General

Bumped minimum PyTorch version to 2.3.0. Note that using another PyTorch
version may work in most cases, but when using integer kernels for
`quantessential` (our companion library of kernels for integer data) there is
strict dependency on PyTorch 2.3.0. This strict dependency is likely to be
removed in a future release.

## Code Organization

`fastforward` can now be imported as a package. E.g., for most use cases
`import fastforward as ff` will suffice. Most subpackages are automatically
imported in the `fastforward` namespace.

## Operators

Added operators to `fastforward.nn.functional`:

- sigmoid
- silu
- gelu
- scaled_dot_product_attention
- dropout
- permute
- cat
- [Please file an issue to request new operators](https://morpheus-gitlab.qualcomm.com/compression/fastforward/-/issues/new?issue[title]=Operator%20request%3A%20%3Coperator%3E%0A&issue[description]=Please%20provide%20the%20full%20name%20of%20the%20PyTorch%20function%20that%20you%20want%20to%20add.%20E.g.%2C%20%60torch.nn.functional.linear%60)

Each can be used directly and its implementation can be overwritten through the
dispatcher.

Fixed issue with operators that accept `(Tensor) | NonTensor)` unions where
the non tensor inputs where incorrectly categorized as not quantized which
resulted  in an error if `strict_quantization` is enabled.  [Reported as #153](https://morpheus-gitlab.qualcomm.com/compression/fastforward/-/issues/153)  

## Quantized Modules

Quantized modules can now opt-out from automatic discovery. See
[QuantizedModule documentation](../../reference/fastforward/nn/quantized_module) for more information.

## Flags

Flags are generalized. This has mostly no impact to end-users, but future flags
will all follow the same struct. For example, the `strict_quantization flag` is
still available as
`fastfoward.quantization.strict_quantization.strict_quantization`
 but it is defined in `fastforward.flags` and exposed on the main
module. In particular there are three functions
`fastforward.set_strict_quantization`, `fastforward.get_strict_quantization`,
and `fastforward.strict_quantization`. The first and last can be used
interchangeably and can act as a context manager.

    import fastforward as ff

    with ff.strict_quantization(False):
      ff.get_strict_quantization() # False
        with ff.set_strict_quantization(True):
          ff.get_strict_quantization() # True

## MPath (fastforward.mpath)

MPath is a utility for filtering modules in a PyTorch module hierarchy (often
referred to as model or network). It can be used to easily obtain a collection
of submodules based on a query. These queries can be build (and extended)
programmatically or can be specified as a query string. For example:

    >>> module = MyModule()
    >>> modules = mpath.search("**/decoder/[cls:torch.nn.Linear]", module)
    >>> print(modules)
    MPathCollection([
        <0: layer4.decoder.linear1: Linear>,
        <2: layer4.decoder.linear2: Linear>,
        <3: layer3.decoder.linear1: Linear>,
        <4: layer2.decoder.linear1: Linear>,
        <5: layer1.decoder.linear1: Linear>,
    ])

In the example above, modules is a indexible container that also supports set
operations. See [the mpath documentation](../../reference/fastforward/mpath/) for
more information

## fastforward.quantization.quant_init

`quant_init` replaces `fastforward.quantization.quant_setup` which is now
deprecated and will be removed in a future release.
`quant_init` leverages mpath to target specific quantizers that need to be
initialized. It also provides a convenient initialization method for any
quantizer in a model.
Please see [the tutorial on MPath and quantizer initialization](../../examples/mpath.nb/) for more details.

## Less strict dependency on Quantessential

Our kernel library `quantessential` can still be used, but `fastforward` does
not strictly depend on it anymore. If `quantessential` is not installed, a
fallback tiled quantization implementation is used that leverages
`torch.compile`. Other kernels, e.g., linear for integer data, will not be available, but these are only used
when activated explicitly in user code. This ensures that `fastforward` has a
less strict dependency on PyTorch 2.3.0 than before. Although, some (future)
functionality may not work on older versions.
