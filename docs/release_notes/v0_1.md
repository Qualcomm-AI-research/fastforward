# FastForward 0.1

FastForward 0.1 marks the first official release of FastForward. [FastForward can be installed
(within Qualcomm) today using `pip install fastforward`](../index.md#installation).

FastForward is a neural network quantization library build on top of PyTorch. FastForward
focusses on extensibility and flexibility. In other words, FastForward offers utilities to
work with quantized neural networks making experimentation and exploration more easy. Our design
goals include being explicit and interpretable and we explicitly do not aim to provide a one-click
solutions to neural network quantization. This release of FastForward is a usable preview release, 
i.e., it can be used to experiment with quantized neural networks today, but expect that some features
are incomplete or missing.

<details>
  <summary>Notes on preview release</summary>
  <div>
    <p>
      Since this marks a preview release of FastForward there is a number of features one
      might expect from a neural network quantization library that are not included (yet).
    </p>
    <p>
      Two noteworthy features that are not included yet are 'autoquant' and
      'export'. This means that the action of quantizing a neural networks
      requires some manual work. And currently, we do not have an export method
      that supports exporting for a particular target device.
    </p>

  </div>
</details>

We welcome any ideas, feedback or feature requests. Please reach out to our development team
on [fastforward-dev@qti.qualcomm.com](mailto:fastforward-dev@qti.qualcomm.com).

## Features
Below, we outline a selection of features that are included in this release.
Please see the [getting started](#getting-started) references at the end of
these release notes for more.

### Quantized Tensor
Quantization deals with quantized data. In FastForward, quantized data is
represented by
[QuantizedTensor](../reference/fastforward/quantized_tensor.md#fastforward.quantized_tensor.QuantizedTensor).
QuantizedTensor is a general tensor datatype that can represent different
quantization representations. Currently, FastForwards supports linear
quantization parameterized by a scale and offset out of the box. Other formats,
for example vector quantization, or min-max parameterized tensors can be added
through user code.

### Quantized Modules
Quantized modules define a quantized neural network. In FastForward, quantized
modules can either be defined directly or extend an existing non-quantized
module. [See the documentation for more
details](../reference/fastforward/nn/quantized_module.md#fastforward.nn.quantized_module.QuantizedModule).
To get you started, FastForward already includes a set of quantized modules
that extend existing PyTorch modules. In our [getting started
tutorial](../examples/quantizing_networks.nb.py), we dive more deeply
into the creation of quantized modules.

### Range Setting
In general, quantization functions are parameterized (e.g., by scale and
offset). Hence, the act of quantizing a neural network entails choosing
(initial) values for these parameters. This process is often referred to as
range setting. FastForward implements several range setting methods that can
be used with general quantization methods. In particular, min-max, min-mse, and
minimum output mse range setting methods are included in this release. These
also provide excellent examples on how to implement similar techniques using
FastForward.

#### Basic Orchestration (experimental)
Range setting methods often require specific orchestration of a network. For
example, first update the quantization parameters of the first layer based on
several batches, before moving on to the second layer. FastForward provides a
method for this that helps users in building methods that require particular
orchestation. In particular, [output mse
minimization](../reference/fastforward/quantization/output_mse.md#fastforward.quantization.output_mse.OutputMSE)
provides an example of this.

__NB__: the orchestration feature is still under development and should be considered experimental
in this release.


### Tile-based Quantization (WIP)
Quantization can be performed in many ways. One of the flavours that many
people experiment with is the granularity of quantization. In other words, for
quantization the input tensor is subdivided in smaller tensors that each get
processed using a different quantizer (i.e., different quantization grid.). We
call this process of subdividing the tensors into smaller parts tiling and
provide implementations for linear quantizers that works with arbitrary regular
tiles (i.e., a single tile that divides the tensors). As of this release, these
implementations are included. Full integrations will be part of FastForward
0.2.

### Integer Kernels
With this release, we also announce a separate library called quantessential.
This library contains implementations of common neural network operations that
work directly on the quantized representation. As a result, you may already
benefit from quantization during training, instead of paying the price for
simulated quantization. However, since the use of integer representations
during training requires changes to PyTorch, these kernels are only included
for interested users and not used as part of FastForward at this point.

#### Operator Dispatcher
FastForward includes a dispatching mechanism for quantized operators. This
allows users to register custom implementations for operators on a per-function
basis. For example, a user may supplement an existing operator with a hardware
exact emulation. In fact, the quantessential kernels can also be included in
FastForward through this mechanism. Please refer to the [API Reference](../reference/fastforward/dispatcher.md)
for more information. 


## Getting Started

To get started using FastForward please refer to the following:

* [Installation instructions](../index.md#installation)
* [Getting Started Tutorial](../examples/quantizing_networks.nb.py)
* [API References](../reference/summary.md)

## Use Cases
FastForward is currently most useful for quantization related research. In fact,
it is currently already used for quantization specific research. For
example, the experiments of [this paper](https://arxiv.org/abs/2402.15319) were
successfully re-produced using fast-forward. Moreover, an early version of
hardware exact kernels were evaluated in FastForward, and we have successfully
quantized OPT and Llama models showing that it matches results, obtained earlier,
through our internal quantization library.


## What's Next
In the coming releases of FastForward we plan to add features for more
automatic ingestion of network (i.e., features related to Automatic
Quantization) and support for exporting networks. Moreover, we aim to
streamline the API to interact with quantized neural networks and general
network manipulations and extend the set of included algorithms. 

We welcome all feature requests. Please get in touch with us through [fastforward-dev@qti.qualcomm.com](mailto:fastforward-dev@qti.qualcomm.com).
